{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eZoIovBb35iXqEOL09nMJuZMDN-ZrwXR","timestamp":1695644380907}],"gpuType":"T4","mount_file_id":"1eZoIovBb35iXqEOL09nMJuZMDN-ZrwXR","authorship_tag":"ABX9TyNFxsMVsQ523DW1ev89Giym"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["\n","#Finetuning U2net"],"metadata":{"id":"k7ROL8-GdIBn"}},{"cell_type":"markdown","source":["참고: https://github.com/danielgatis/rembg/issues/193#issuecomment-1055534289"],"metadata":{"id":"WX6O88UIgfB7"}},{"cell_type":"code","source":["import os\n","os.chdir('/home/aidev/hy/ft_u2net/U-2-Net-master')"],"metadata":{"id":"-b7_6DFkfsDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## u2net_train.py와 동일하게 진행"],"metadata":{"id":"qGfe8m24gBTm"}},{"cell_type":"markdown","source":["0. import"],"metadata":{"id":"HYskL794gIC9"}},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision\n","from torch.autograd import Variable\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","import torch.optim as optim\n","import torchvision.transforms as standard_transforms\n","\n","import numpy as np\n","import glob\n","import os\n","import gdown\n","\n","from data_loader import Rescale\n","from data_loader import RescaleT\n","from data_loader import RandomCrop\n","from data_loader import ToTensor\n","from data_loader import ToTensorLab\n","from data_loader import SalObjDataset\n","\n","from model import U2NET\n","from model import U2NETP\n"],"metadata":{"id":"yPqTEjPYgGU9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. define loss function"],"metadata":{"id":"rESS9iQGgOyi"}},{"cell_type":"code","source":["bce_loss = nn.BCELoss(size_average=True)\n","\n","def muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v):\n","\n","\tloss0 = bce_loss(d0,labels_v)\n","\tloss1 = bce_loss(d1,labels_v)\n","\tloss2 = bce_loss(d2,labels_v)\n","\tloss3 = bce_loss(d3,labels_v)\n","\tloss4 = bce_loss(d4,labels_v)\n","\tloss5 = bce_loss(d5,labels_v)\n","\tloss6 = bce_loss(d6,labels_v)\n","\n","\tloss = loss0 + loss1 + loss2 + loss3 + loss4 + loss5 + loss6\n","\tprint(\"l0: %3f, l1: %3f, l2: %3f, l3: %3f, l4: %3f, l5: %3f, l6: %3f\\n\"%(loss0.data.item(),loss1.data.item(),loss2.data.item(),loss3.data.item(),loss4.data.item(),loss5.data.item(),loss6.data.item()))\n","\n","\treturn loss0, loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GJOEk7vfgLX3","executionInfo":{"status":"ok","timestamp":1695640191132,"user_tz":-540,"elapsed":616,"user":{"displayName":"김혜연","userId":"14076391861970698514"}},"outputId":"5d272baf-33e8-487d-c4e8-3137216b2c2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"]}]},{"cell_type":"markdown","source":["(prepare training dataset(background-removed picture to mask)\n","배경=0, 물체=1)"],"metadata":{"id":"ZYBMWUrZoRMM"}},{"cell_type":"markdown","source":["2. set the directory of training dataset"],"metadata":{"id":"-gAEzGppgTfu"}},{"cell_type":"code","source":["model_name = 'u2net' #'u2netp'\n","\n","data_dir = os.path.join(os.getcwd(), 'finetune_data' + os.sep) #/content/drive/MyDrive/U-2-Net-master/finetune_data/\n","tra_image_dir = os.path.join(\"finetune_input\"+ os.sep) #이부분 수정\n","tra_label_dir = os.path.join(\"finetune_groundtruth\"+ os.sep) #이부분 수정\n","print('data_dir:',data_dir)\n","\n","image_ext = '.png'\n","label_ext = '.png'\n","\n","model_dir = os.path.join(os.getcwd(), 'saved_models', model_name + os.sep) #/content/drive/MyDrive/U-2-Net-master/saved_models/u2net/\n","print('model_dir',model_dir)\n","\n","epoch_num = 100000\n","batch_size_train = 12\n","batch_size_val = 1\n","train_num = 0\n","val_num = 0\n","\n","tra_img_name_list = glob.glob(data_dir + tra_image_dir + '*' + image_ext)\n","print(tra_img_name_list)\n","tra_lbl_name_list = glob.glob(data_dir + tra_label_dir + '*' + image_ext)\n","print(tra_lbl_name_list)\n","\n","print(\"---\")\n","print(\"train images: \", len(tra_img_name_list))\n","print(\"train labels: \", len(tra_lbl_name_list))\n","print(\"---\")\n","\n","train_num = len(tra_img_name_list)\n","\n","salobj_dataset = SalObjDataset(\n","    img_name_list=tra_img_name_list,\n","    lbl_name_list=tra_lbl_name_list,\n","    transform=transforms.Compose([\n","        RescaleT(320),\n","        RandomCrop(288),\n","        ToTensorLab(flag=0)]))\n","salobj_dataloader = DataLoader(salobj_dataset, batch_size=batch_size_train, shuffle=True, num_workers=1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VV5h3B6kgSih","executionInfo":{"status":"ok","timestamp":1695297262406,"user_tz":-540,"elapsed":437,"user":{"displayName":"김혜연","userId":"14076391861970698514"}},"outputId":"c95c949b-6e38-4250-a8a0-a021d2701b6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data_dir: /content/drive/MyDrive/U-2-Net-master/finetune_data/\n","model_dir /content/drive/MyDrive/U-2-Net-master/saved_models/u2net/\n","['/content/drive/MyDrive/U-2-Net-master/finetune_data/finetune_input/강아지 (1).png']\n","['/content/drive/MyDrive/U-2-Net-master/finetune_data/finetune_groundtruth/강아지 (1)_clipdrop-background-removal.png']\n","---\n","train images:  1\n","train labels:  1\n","---\n"]}]},{"cell_type":"markdown","source":["3. define model"],"metadata":{"id":"NcwC_EARh5tx"}},{"cell_type":"code","source":["#bring state_dict model\n","if(model_name=='u2net'):\n","    net = U2NET(3, 1)\n","elif(model_name=='u2netp'):\n","    net = U2NETP(3,1)\n","\n","#get u2net check point(pth file)\n","\n","os.makedirs('./saved_models/u2net', exist_ok=True)\n","\n","gdown.download('https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ',\n","    './saved_models/u2net/u2net.pth',\n","    quiet=False)\n","\n","net.load_state_dict(torch.load('saved_models/u2net/u2net.pth'))\n","\n","\"\"\"또는 하단 코드\n","checkpoint = torch.load(pathlib.Path(model_path))\n","net.load_state_dict(checkpoint)\n","net.load_state_dict(torch.load(model_path))\n","\"\"\"\n","\n","if torch.cuda.is_available():\n","    net.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"xakcG-71h8wS","executionInfo":{"status":"error","timestamp":1695205460744,"user_tz":-540,"elapsed":5623,"user":{"displayName":"김혜연","userId":"14076391861970698514"}},"outputId":"43c80001-b406-4b53-c4cb-010ac1b53d9d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading...\n","From: https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\n","To: /content/drive/MyDrive/U-2-Net-master/saved_models/u2net/u2net.pth\n"," 76%|███████▋  | 135M/176M [00:03<00:00, 42.6MB/s]"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-c30b6ee36fe3>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./saved_models/u2net'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m gdown.download('https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ',\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m'./saved_models/u2net/u2net.pth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     quiet=False)\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gdown/download.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0mt_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHUNK_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    938\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    877\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    880\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m     def _raw_read(\n","\u001b[0;32m/usr/lib/python3.10/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    464\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1272\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1273\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1275\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1129\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1130\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["4. define optimizer"],"metadata":{"id":"tulUBpLCjErW"}},{"cell_type":"code","source":["print(\"---define optimizer...\")\n","optimizer = optim.Adam(net.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z8qy8saTjD7m","executionInfo":{"status":"ok","timestamp":1695131920605,"user_tz":-540,"elapsed":6,"user":{"displayName":"김혜연","userId":"14076391861970698514"}},"outputId":"691d343d-60e4-46e5-db67-99b73e216986"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---define optimizer...\n"]}]},{"cell_type":"markdown","source":["5. training process"],"metadata":{"id":"mKfu2HL1kBz3"}},{"cell_type":"code","source":["print(\"---start training...\")\n","ite_num = 0\n","running_loss = 0.0\n","running_tar_loss = 0.0\n","ite_num4val = 0\n","save_frq = 2000 # save the model every 2000 iterations\n","\n","for epoch in range(0, epoch_num):\n","    net.train()\n","\n","    for i, data in enumerate(salobj_dataloader):\n","        ite_num = ite_num + 1\n","        ite_num4val = ite_num4val + 1\n","\n","        inputs, labels = data['image'], data['label']\n","\n","        inputs = inputs.type(torch.FloatTensor)\n","        labels = labels.type(torch.FloatTensor)\n","\n","        # wrap them in Variable\n","        if torch.cuda.is_available():\n","            inputs_v, labels_v = Variable(inputs.cuda(), requires_grad=False), Variable(labels.cuda(),\n","                                                                                        requires_grad=False)\n","        else:\n","            inputs_v, labels_v = Variable(inputs, requires_grad=False), Variable(labels, requires_grad=False)\n","\n","        # y zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        d0, d1, d2, d3, d4, d5, d6 = net(inputs_v)\n","        loss2, loss = muti_bce_loss_fusion(d0, d1, d2, d3, d4, d5, d6, labels_v)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # # print statistics\n","        running_loss += loss.data.item()\n","        running_tar_loss += loss2.data.item()\n","\n","        # del temporary outputs and loss\n","        del d0, d1, d2, d3, d4, d5, d6, loss2, loss\n","\n","        print(\"[epoch: %3d/%3d, batch: %5d/%5d, ite: %d] train loss: %3f, tar: %3f \" % (\n","        epoch + 1, epoch_num, (i + 1) * batch_size_train, train_num, ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n","\n","        if ite_num % save_frq == 0:\n","\n","            torch.save(net.state_dict(), model_dir + model_name+\"_bce_itr_%d_train_%3f_tar_%3f.pth\" % (ite_num, running_loss / ite_num4val, running_tar_loss / ite_num4val))\n","            running_loss = 0.0\n","            running_tar_loss = 0.0\n","            net.train()  # resume train\n","            ite_num4val = 0\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9PlMj7UkG91","outputId":"9218f12c-73c2-4471-dc0f-ca4002971af5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---start training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3737: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n","  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n","\n","[epoch: 15441/100000, batch:    12/    1, ite: 15441] train loss: 0.118676, tar: 0.000082 \n","l0: 0.000067, l1: 0.000066, l2: 0.001150, l3: 0.011318, l4: 0.019767, l5: 0.032365, l6: 0.045080\n","\n","[epoch: 15442/100000, batch:    12/    1, ite: 15442] train loss: 0.118670, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001156, l3: 0.011856, l4: 0.020443, l5: 0.032613, l6: 0.062764\n","\n","[epoch: 15443/100000, batch:    12/    1, ite: 15443] train loss: 0.118677, tar: 0.000082 \n","l0: 0.000071, l1: 0.000072, l2: 0.001169, l3: 0.011532, l4: 0.019845, l5: 0.031504, l6: 0.046596\n","\n","[epoch: 15444/100000, batch:    12/    1, ite: 15444] train loss: 0.118672, tar: 0.000082 \n","l0: 0.000071, l1: 0.000072, l2: 0.001451, l3: 0.011661, l4: 0.019785, l5: 0.033402, l6: 0.048626\n","\n","[epoch: 15445/100000, batch:    12/    1, ite: 15445] train loss: 0.118669, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001099, l3: 0.011423, l4: 0.019399, l5: 0.030083, l6: 0.046226\n","\n","[epoch: 15446/100000, batch:    12/    1, ite: 15446] train loss: 0.118662, tar: 0.000082 \n","l0: 0.000073, l1: 0.000073, l2: 0.001345, l3: 0.010899, l4: 0.020136, l5: 0.033753, l6: 0.066899\n","\n","[epoch: 15447/100000, batch:    12/    1, ite: 15447] train loss: 0.118672, tar: 0.000082 \n","l0: 0.000071, l1: 0.000076, l2: 0.001207, l3: 0.011364, l4: 0.019488, l5: 0.030254, l6: 0.049835\n","\n","[epoch: 15448/100000, batch:    12/    1, ite: 15448] train loss: 0.118668, tar: 0.000082 \n","l0: 0.000059, l1: 0.000063, l2: 0.001274, l3: 0.011828, l4: 0.020169, l5: 0.030812, l6: 0.055943\n","\n","[epoch: 15449/100000, batch:    12/    1, ite: 15449] train loss: 0.118669, tar: 0.000082 \n","l0: 0.000073, l1: 0.000073, l2: 0.001130, l3: 0.011074, l4: 0.020096, l5: 0.030489, l6: 0.046239\n","\n","[epoch: 15450/100000, batch:    12/    1, ite: 15450] train loss: 0.118662, tar: 0.000082 \n","l0: 0.000068, l1: 0.000068, l2: 0.001149, l3: 0.011159, l4: 0.020301, l5: 0.030017, l6: 0.049215\n","\n","[epoch: 15451/100000, batch:    12/    1, ite: 15451] train loss: 0.118658, tar: 0.000082 \n","l0: 0.000069, l1: 0.000068, l2: 0.001539, l3: 0.011293, l4: 0.020491, l5: 0.031300, l6: 0.053882\n","\n","[epoch: 15452/100000, batch:    12/    1, ite: 15452] train loss: 0.118658, tar: 0.000082 \n","l0: 0.000076, l1: 0.000076, l2: 0.001520, l3: 0.011336, l4: 0.020156, l5: 0.033062, l6: 0.063685\n","\n","[epoch: 15453/100000, batch:    12/    1, ite: 15453] train loss: 0.118665, tar: 0.000082 \n","l0: 0.000061, l1: 0.000062, l2: 0.001110, l3: 0.010952, l4: 0.019139, l5: 0.031488, l6: 0.042441\n","\n","[epoch: 15454/100000, batch:    12/    1, ite: 15454] train loss: 0.118656, tar: 0.000082 \n","l0: 0.000077, l1: 0.000078, l2: 0.001274, l3: 0.011463, l4: 0.020425, l5: 0.034267, l6: 0.062807\n","\n","[epoch: 15455/100000, batch:    12/    1, ite: 15455] train loss: 0.118664, tar: 0.000082 \n","l0: 0.000081, l1: 0.000077, l2: 0.001460, l3: 0.011169, l4: 0.019952, l5: 0.032085, l6: 0.052826\n","\n","[epoch: 15456/100000, batch:    12/    1, ite: 15456] train loss: 0.118664, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001476, l3: 0.011195, l4: 0.019816, l5: 0.029997, l6: 0.047770\n","\n","[epoch: 15457/100000, batch:    12/    1, ite: 15457] train loss: 0.118658, tar: 0.000082 \n","l0: 0.000068, l1: 0.000073, l2: 0.001502, l3: 0.011658, l4: 0.020531, l5: 0.032699, l6: 0.048377\n","\n","[epoch: 15458/100000, batch:    12/    1, ite: 15458] train loss: 0.118655, tar: 0.000082 \n","l0: 0.000069, l1: 0.000068, l2: 0.001178, l3: 0.011543, l4: 0.019937, l5: 0.031681, l6: 0.051957\n","\n","[epoch: 15459/100000, batch:    12/    1, ite: 15459] train loss: 0.118654, tar: 0.000082 \n","l0: 0.000068, l1: 0.000073, l2: 0.001393, l3: 0.010987, l4: 0.019813, l5: 0.030170, l6: 0.046731\n","\n","[epoch: 15460/100000, batch:    12/    1, ite: 15460] train loss: 0.118647, tar: 0.000082 \n","l0: 0.000075, l1: 0.000075, l2: 0.001211, l3: 0.011374, l4: 0.020433, l5: 0.032015, l6: 0.050713\n","\n","[epoch: 15461/100000, batch:    12/    1, ite: 15461] train loss: 0.118645, tar: 0.000082 \n","l0: 0.000063, l1: 0.000063, l2: 0.001131, l3: 0.011594, l4: 0.020065, l5: 0.031588, l6: 0.054167\n","\n","[epoch: 15462/100000, batch:    12/    1, ite: 15462] train loss: 0.118645, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001093, l3: 0.011600, l4: 0.019665, l5: 0.031878, l6: 0.051632\n","\n","[epoch: 15463/100000, batch:    12/    1, ite: 15463] train loss: 0.118644, tar: 0.000082 \n","l0: 0.000063, l1: 0.000063, l2: 0.001125, l3: 0.011760, l4: 0.020108, l5: 0.031675, l6: 0.052503\n","\n","[epoch: 15464/100000, batch:    12/    1, ite: 15464] train loss: 0.118643, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001159, l3: 0.011561, l4: 0.019505, l5: 0.032158, l6: 0.044224\n","\n","[epoch: 15465/100000, batch:    12/    1, ite: 15465] train loss: 0.118636, tar: 0.000082 \n","l0: 0.000071, l1: 0.000072, l2: 0.001320, l3: 0.011714, l4: 0.019750, l5: 0.029463, l6: 0.050976\n","\n","[epoch: 15466/100000, batch:    12/    1, ite: 15466] train loss: 0.118632, tar: 0.000082 \n","l0: 0.000069, l1: 0.000065, l2: 0.001274, l3: 0.011188, l4: 0.020199, l5: 0.033569, l6: 0.049726\n","\n","[epoch: 15467/100000, batch:    12/    1, ite: 15467] train loss: 0.118631, tar: 0.000082 \n","l0: 0.000067, l1: 0.000071, l2: 0.001448, l3: 0.011564, l4: 0.019203, l5: 0.030198, l6: 0.047018\n","\n","[epoch: 15468/100000, batch:    12/    1, ite: 15468] train loss: 0.118624, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001149, l3: 0.011395, l4: 0.019790, l5: 0.030441, l6: 0.054075\n","\n","[epoch: 15469/100000, batch:    12/    1, ite: 15469] train loss: 0.118623, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001496, l3: 0.011274, l4: 0.020423, l5: 0.031389, l6: 0.053765\n","\n","[epoch: 15470/100000, batch:    12/    1, ite: 15470] train loss: 0.118623, tar: 0.000082 \n","l0: 0.000068, l1: 0.000067, l2: 0.001131, l3: 0.011531, l4: 0.020011, l5: 0.032322, l6: 0.059739\n","\n","[epoch: 15471/100000, batch:    12/    1, ite: 15471] train loss: 0.118627, tar: 0.000082 \n","l0: 0.000072, l1: 0.000072, l2: 0.001434, l3: 0.011740, l4: 0.019983, l5: 0.030603, l6: 0.052214\n","\n","[epoch: 15472/100000, batch:    12/    1, ite: 15472] train loss: 0.118626, tar: 0.000082 \n","l0: 0.000079, l1: 0.000075, l2: 0.001582, l3: 0.011059, l4: 0.020166, l5: 0.034351, l6: 0.066234\n","\n","[epoch: 15473/100000, batch:    12/    1, ite: 15473] train loss: 0.118636, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001157, l3: 0.011311, l4: 0.020627, l5: 0.030401, l6: 0.048785\n","\n","[epoch: 15474/100000, batch:    12/    1, ite: 15474] train loss: 0.118632, tar: 0.000082 \n","l0: 0.000072, l1: 0.000076, l2: 0.001175, l3: 0.011580, l4: 0.020268, l5: 0.032550, l6: 0.054081\n","\n","[epoch: 15475/100000, batch:    12/    1, ite: 15475] train loss: 0.118632, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001140, l3: 0.011532, l4: 0.020396, l5: 0.033514, l6: 0.049322\n","\n","[epoch: 15476/100000, batch:    12/    1, ite: 15476] train loss: 0.118631, tar: 0.000082 \n","l0: 0.000072, l1: 0.000072, l2: 0.001330, l3: 0.011191, l4: 0.020018, l5: 0.029455, l6: 0.048525\n","\n","[epoch: 15477/100000, batch:    12/    1, ite: 15477] train loss: 0.118625, tar: 0.000082 \n","l0: 0.000067, l1: 0.000071, l2: 0.001438, l3: 0.011769, l4: 0.019637, l5: 0.032757, l6: 0.047101\n","\n","[epoch: 15478/100000, batch:    12/    1, ite: 15478] train loss: 0.118621, tar: 0.000082 \n","l0: 0.000072, l1: 0.000073, l2: 0.001515, l3: 0.011892, l4: 0.021048, l5: 0.032722, l6: 0.061477\n","\n","[epoch: 15479/100000, batch:    12/    1, ite: 15479] train loss: 0.118628, tar: 0.000082 \n","l0: 0.000064, l1: 0.000063, l2: 0.001143, l3: 0.011120, l4: 0.020497, l5: 0.031821, l6: 0.050531\n","\n","[epoch: 15480/100000, batch:    12/    1, ite: 15480] train loss: 0.118626, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001356, l3: 0.011461, l4: 0.019792, l5: 0.031444, l6: 0.061025\n","\n","[epoch: 15481/100000, batch:    12/    1, ite: 15481] train loss: 0.118630, tar: 0.000082 \n","l0: 0.000072, l1: 0.000072, l2: 0.001334, l3: 0.011202, l4: 0.019984, l5: 0.032344, l6: 0.053931\n","\n","[epoch: 15482/100000, batch:    12/    1, ite: 15482] train loss: 0.118631, tar: 0.000082 \n","l0: 0.000063, l1: 0.000063, l2: 0.001379, l3: 0.011566, l4: 0.019610, l5: 0.032483, l6: 0.064144\n","\n","[epoch: 15483/100000, batch:    12/    1, ite: 15483] train loss: 0.118638, tar: 0.000082 \n","l0: 0.000071, l1: 0.000072, l2: 0.001292, l3: 0.011454, l4: 0.019809, l5: 0.030912, l6: 0.052501\n","\n","[epoch: 15484/100000, batch:    12/    1, ite: 15484] train loss: 0.118636, tar: 0.000082 \n","l0: 0.000071, l1: 0.000071, l2: 0.001087, l3: 0.011456, l4: 0.019298, l5: 0.030376, l6: 0.046939\n","\n","[epoch: 15485/100000, batch:    12/    1, ite: 15485] train loss: 0.118630, tar: 0.000082 \n","l0: 0.000067, l1: 0.000067, l2: 0.001124, l3: 0.011182, l4: 0.020174, l5: 0.031177, l6: 0.060220\n","\n","[epoch: 15486/100000, batch:    12/    1, ite: 15486] train loss: 0.118633, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001482, l3: 0.011524, l4: 0.020019, l5: 0.031038, l6: 0.044505\n","\n","[epoch: 15487/100000, batch:    12/    1, ite: 15487] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000064, l1: 0.000063, l2: 0.001142, l3: 0.011399, l4: 0.020568, l5: 0.031328, l6: 0.055800\n","\n","[epoch: 15488/100000, batch:    12/    1, ite: 15488] train loss: 0.118628, tar: 0.000081 \n","l0: 0.000069, l1: 0.000069, l2: 0.001277, l3: 0.010838, l4: 0.019805, l5: 0.032946, l6: 0.065492\n","\n","[epoch: 15489/100000, batch:    12/    1, ite: 15489] train loss: 0.118636, tar: 0.000081 \n","l0: 0.000074, l1: 0.000074, l2: 0.001297, l3: 0.011480, l4: 0.019639, l5: 0.028860, l6: 0.050386\n","\n","[epoch: 15490/100000, batch:    12/    1, ite: 15490] train loss: 0.118631, tar: 0.000081 \n","l0: 0.000065, l1: 0.000064, l2: 0.001423, l3: 0.011275, l4: 0.020356, l5: 0.031277, l6: 0.051224\n","\n","[epoch: 15491/100000, batch:    12/    1, ite: 15491] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001482, l3: 0.011246, l4: 0.020141, l5: 0.032882, l6: 0.049262\n","\n","[epoch: 15492/100000, batch:    12/    1, ite: 15492] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000072, l1: 0.000076, l2: 0.001156, l3: 0.011819, l4: 0.020508, l5: 0.033783, l6: 0.062496\n","\n","[epoch: 15493/100000, batch:    12/    1, ite: 15493] train loss: 0.118635, tar: 0.000081 \n","l0: 0.000074, l1: 0.000074, l2: 0.001232, l3: 0.011561, l4: 0.019822, l5: 0.033165, l6: 0.065537\n","\n","[epoch: 15494/100000, batch:    12/    1, ite: 15494] train loss: 0.118643, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001097, l3: 0.011427, l4: 0.019976, l5: 0.032837, l6: 0.048206\n","\n","[epoch: 15495/100000, batch:    12/    1, ite: 15495] train loss: 0.118640, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001226, l3: 0.011695, l4: 0.019959, l5: 0.031492, l6: 0.052536\n","\n","[epoch: 15496/100000, batch:    12/    1, ite: 15496] train loss: 0.118639, tar: 0.000081 \n","l0: 0.000068, l1: 0.000069, l2: 0.001274, l3: 0.011529, l4: 0.020442, l5: 0.034280, l6: 0.064944\n","\n","[epoch: 15497/100000, batch:    12/    1, ite: 15497] train loss: 0.118648, tar: 0.000081 \n","l0: 0.000066, l1: 0.000066, l2: 0.001478, l3: 0.011017, l4: 0.019392, l5: 0.031451, l6: 0.049624\n","\n","[epoch: 15498/100000, batch:    12/    1, ite: 15498] train loss: 0.118644, tar: 0.000081 \n","l0: 0.000063, l1: 0.000064, l2: 0.001124, l3: 0.011369, l4: 0.020594, l5: 0.031322, l6: 0.050170\n","\n","[epoch: 15499/100000, batch:    12/    1, ite: 15499] train loss: 0.118642, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001145, l3: 0.011538, l4: 0.019512, l5: 0.032555, l6: 0.045109\n","\n","[epoch: 15500/100000, batch:    12/    1, ite: 15500] train loss: 0.118636, tar: 0.000081 \n","l0: 0.000071, l1: 0.000070, l2: 0.001144, l3: 0.011333, l4: 0.019390, l5: 0.031708, l6: 0.043179\n","\n","[epoch: 15501/100000, batch:    12/    1, ite: 15501] train loss: 0.118628, tar: 0.000081 \n","l0: 0.000064, l1: 0.000064, l2: 0.001132, l3: 0.011266, l4: 0.019766, l5: 0.029547, l6: 0.051407\n","\n","[epoch: 15502/100000, batch:    12/    1, ite: 15502] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001243, l3: 0.011593, l4: 0.019963, l5: 0.033724, l6: 0.048357\n","\n","[epoch: 15503/100000, batch:    12/    1, ite: 15503] train loss: 0.118622, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001138, l3: 0.011101, l4: 0.019712, l5: 0.031983, l6: 0.044811\n","\n","[epoch: 15504/100000, batch:    12/    1, ite: 15504] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000065, l1: 0.000065, l2: 0.001533, l3: 0.011116, l4: 0.020178, l5: 0.032712, l6: 0.067437\n","\n","[epoch: 15505/100000, batch:    12/    1, ite: 15505] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000064, l1: 0.000063, l2: 0.001101, l3: 0.011088, l4: 0.020237, l5: 0.032305, l6: 0.046806\n","\n","[epoch: 15506/100000, batch:    12/    1, ite: 15506] train loss: 0.118621, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001492, l3: 0.011677, l4: 0.019315, l5: 0.030698, l6: 0.054331\n","\n","[epoch: 15507/100000, batch:    12/    1, ite: 15507] train loss: 0.118620, tar: 0.000081 \n","l0: 0.000065, l1: 0.000065, l2: 0.001211, l3: 0.011444, l4: 0.020298, l5: 0.033569, l6: 0.062493\n","\n","[epoch: 15508/100000, batch:    12/    1, ite: 15508] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000065, l1: 0.000066, l2: 0.001465, l3: 0.011120, l4: 0.019702, l5: 0.032059, l6: 0.066567\n","\n","[epoch: 15509/100000, batch:    12/    1, ite: 15509] train loss: 0.118635, tar: 0.000081 \n","l0: 0.000069, l1: 0.000076, l2: 0.001276, l3: 0.011395, l4: 0.020562, l5: 0.031600, l6: 0.050842\n","\n","[epoch: 15510/100000, batch:    12/    1, ite: 15510] train loss: 0.118633, tar: 0.000081 \n","l0: 0.000076, l1: 0.000076, l2: 0.001468, l3: 0.010956, l4: 0.019770, l5: 0.030040, l6: 0.045125\n","\n","[epoch: 15511/100000, batch:    12/    1, ite: 15511] train loss: 0.118626, tar: 0.000081 \n","l0: 0.000055, l1: 0.000059, l2: 0.001295, l3: 0.011864, l4: 0.020639, l5: 0.033296, l6: 0.059537\n","\n","[epoch: 15512/100000, batch:    12/    1, ite: 15512] train loss: 0.118631, tar: 0.000081 \n","l0: 0.000060, l1: 0.000064, l2: 0.001289, l3: 0.011206, l4: 0.020266, l5: 0.033257, l6: 0.049306\n","\n","[epoch: 15513/100000, batch:    12/    1, ite: 15513] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000064, l1: 0.000065, l2: 0.001353, l3: 0.011272, l4: 0.020601, l5: 0.030979, l6: 0.053721\n","\n","[epoch: 15514/100000, batch:    12/    1, ite: 15514] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000061, l1: 0.000061, l2: 0.001404, l3: 0.011598, l4: 0.020495, l5: 0.034053, l6: 0.064121\n","\n","[epoch: 15515/100000, batch:    12/    1, ite: 15515] train loss: 0.118638, tar: 0.000081 \n","l0: 0.000073, l1: 0.000072, l2: 0.001420, l3: 0.011592, l4: 0.019933, l5: 0.031614, l6: 0.051160\n","\n","[epoch: 15516/100000, batch:    12/    1, ite: 15516] train loss: 0.118636, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001108, l3: 0.011640, l4: 0.019616, l5: 0.032784, l6: 0.046421\n","\n","[epoch: 15517/100000, batch:    12/    1, ite: 15517] train loss: 0.118631, tar: 0.000081 \n","l0: 0.000061, l1: 0.000062, l2: 0.001506, l3: 0.011089, l4: 0.019517, l5: 0.031563, l6: 0.048364\n","\n","[epoch: 15518/100000, batch:    12/    1, ite: 15518] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000063, l1: 0.000062, l2: 0.001137, l3: 0.011112, l4: 0.019971, l5: 0.032762, l6: 0.046080\n","\n","[epoch: 15519/100000, batch:    12/    1, ite: 15519] train loss: 0.118622, tar: 0.000081 \n","l0: 0.000068, l1: 0.000072, l2: 0.001456, l3: 0.011885, l4: 0.020368, l5: 0.032744, l6: 0.049768\n","\n","[epoch: 15520/100000, batch:    12/    1, ite: 15520] train loss: 0.118621, tar: 0.000081 \n","l0: 0.000062, l1: 0.000063, l2: 0.001477, l3: 0.011420, l4: 0.020122, l5: 0.031302, l6: 0.059361\n","\n","[epoch: 15521/100000, batch:    12/    1, ite: 15521] train loss: 0.118624, tar: 0.000081 \n","l0: 0.000069, l1: 0.000070, l2: 0.001594, l3: 0.011618, l4: 0.020401, l5: 0.034252, l6: 0.064021\n","\n","[epoch: 15522/100000, batch:    12/    1, ite: 15522] train loss: 0.118633, tar: 0.000081 \n","l0: 0.000072, l1: 0.000076, l2: 0.001419, l3: 0.011242, l4: 0.019926, l5: 0.029855, l6: 0.052099\n","\n","[epoch: 15523/100000, batch:    12/    1, ite: 15523] train loss: 0.118630, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001497, l3: 0.012024, l4: 0.020428, l5: 0.032487, l6: 0.056907\n","\n","[epoch: 15524/100000, batch:    12/    1, ite: 15524] train loss: 0.118633, tar: 0.000081 \n","l0: 0.000069, l1: 0.000068, l2: 0.001119, l3: 0.011638, l4: 0.019739, l5: 0.029670, l6: 0.052548\n","\n","[epoch: 15525/100000, batch:    12/    1, ite: 15525] train loss: 0.118631, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001318, l3: 0.011649, l4: 0.019927, l5: 0.031171, l6: 0.050817\n","\n","[epoch: 15526/100000, batch:    12/    1, ite: 15526] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000067, l1: 0.000066, l2: 0.001480, l3: 0.011127, l4: 0.020087, l5: 0.030114, l6: 0.048291\n","\n","[epoch: 15527/100000, batch:    12/    1, ite: 15527] train loss: 0.118624, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001090, l3: 0.011137, l4: 0.019368, l5: 0.032056, l6: 0.043435\n","\n","[epoch: 15528/100000, batch:    12/    1, ite: 15528] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000063, l1: 0.000064, l2: 0.001142, l3: 0.011653, l4: 0.020023, l5: 0.033815, l6: 0.048283\n","\n","[epoch: 15529/100000, batch:    12/    1, ite: 15529] train loss: 0.118614, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001446, l3: 0.011744, l4: 0.020357, l5: 0.033353, l6: 0.049362\n","\n","[epoch: 15530/100000, batch:    12/    1, ite: 15530] train loss: 0.118612, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001086, l3: 0.011629, l4: 0.020059, l5: 0.029742, l6: 0.050921\n","\n","[epoch: 15531/100000, batch:    12/    1, ite: 15531] train loss: 0.118609, tar: 0.000081 \n","l0: 0.000073, l1: 0.000076, l2: 0.001449, l3: 0.011268, l4: 0.020177, l5: 0.031751, l6: 0.052320\n","\n","[epoch: 15532/100000, batch:    12/    1, ite: 15532] train loss: 0.118608, tar: 0.000081 \n","l0: 0.000073, l1: 0.000073, l2: 0.001134, l3: 0.011940, l4: 0.020919, l5: 0.033350, l6: 0.062630\n","\n","[epoch: 15533/100000, batch:    12/    1, ite: 15533] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001143, l3: 0.011588, l4: 0.019987, l5: 0.031718, l6: 0.051917\n","\n","[epoch: 15534/100000, batch:    12/    1, ite: 15534] train loss: 0.118614, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001455, l3: 0.011282, l4: 0.020095, l5: 0.032435, l6: 0.049476\n","\n","[epoch: 15535/100000, batch:    12/    1, ite: 15535] train loss: 0.118612, tar: 0.000081 \n","l0: 0.000082, l1: 0.000077, l2: 0.001432, l3: 0.011235, l4: 0.020201, l5: 0.032985, l6: 0.048263\n","\n","[epoch: 15536/100000, batch:    12/    1, ite: 15536] train loss: 0.118609, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001108, l3: 0.011398, l4: 0.019457, l5: 0.032519, l6: 0.045309\n","\n","[epoch: 15537/100000, batch:    12/    1, ite: 15537] train loss: 0.118603, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001126, l3: 0.011167, l4: 0.019860, l5: 0.033578, l6: 0.048776\n","\n","[epoch: 15538/100000, batch:    12/    1, ite: 15538] train loss: 0.118601, tar: 0.000081 \n","l0: 0.000072, l1: 0.000073, l2: 0.001534, l3: 0.011516, l4: 0.020616, l5: 0.033853, l6: 0.063664\n","\n","[epoch: 15539/100000, batch:    12/    1, ite: 15539] train loss: 0.118609, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001275, l3: 0.011090, l4: 0.019683, l5: 0.031965, l6: 0.046018\n","\n","[epoch: 15540/100000, batch:    12/    1, ite: 15540] train loss: 0.118604, tar: 0.000081 \n","l0: 0.000071, l1: 0.000076, l2: 0.001240, l3: 0.011352, l4: 0.020505, l5: 0.033624, l6: 0.049320\n","\n","[epoch: 15541/100000, batch:    12/    1, ite: 15541] train loss: 0.118602, tar: 0.000081 \n","l0: 0.000062, l1: 0.000063, l2: 0.001175, l3: 0.011548, l4: 0.019602, l5: 0.030170, l6: 0.047922\n","\n","[epoch: 15542/100000, batch:    12/    1, ite: 15542] train loss: 0.118597, tar: 0.000081 \n","l0: 0.000072, l1: 0.000073, l2: 0.001376, l3: 0.011698, l4: 0.020821, l5: 0.032051, l6: 0.061652\n","\n","[epoch: 15543/100000, batch:    12/    1, ite: 15543] train loss: 0.118603, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001330, l3: 0.011158, l4: 0.020005, l5: 0.029627, l6: 0.048544\n","\n","[epoch: 15544/100000, batch:    12/    1, ite: 15544] train loss: 0.118598, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001095, l3: 0.011382, l4: 0.019469, l5: 0.029689, l6: 0.047494\n","\n","[epoch: 15545/100000, batch:    12/    1, ite: 15545] train loss: 0.118592, tar: 0.000081 \n","l0: 0.000080, l1: 0.000079, l2: 0.001451, l3: 0.011431, l4: 0.020493, l5: 0.033475, l6: 0.061221\n","\n","[epoch: 15546/100000, batch:    12/    1, ite: 15546] train loss: 0.118598, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001223, l3: 0.011516, l4: 0.019716, l5: 0.032802, l6: 0.064369\n","\n","[epoch: 15547/100000, batch:    12/    1, ite: 15547] train loss: 0.118605, tar: 0.000081 \n","l0: 0.000068, l1: 0.000069, l2: 0.001580, l3: 0.011533, l4: 0.020399, l5: 0.034704, l6: 0.065159\n","\n","[epoch: 15548/100000, batch:    12/    1, ite: 15548] train loss: 0.118615, tar: 0.000081 \n","l0: 0.000073, l1: 0.000076, l2: 0.001649, l3: 0.011280, l4: 0.020770, l5: 0.034632, l6: 0.066994\n","\n","[epoch: 15549/100000, batch:    12/    1, ite: 15549] train loss: 0.118626, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001239, l3: 0.011574, l4: 0.020054, l5: 0.034373, l6: 0.065234\n","\n","[epoch: 15550/100000, batch:    12/    1, ite: 15550] train loss: 0.118635, tar: 0.000081 \n","l0: 0.000076, l1: 0.000076, l2: 0.001434, l3: 0.011266, l4: 0.020823, l5: 0.031480, l6: 0.058505\n","\n","[epoch: 15551/100000, batch:    12/    1, ite: 15551] train loss: 0.118638, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001197, l3: 0.011946, l4: 0.020739, l5: 0.032641, l6: 0.063109\n","\n","[epoch: 15552/100000, batch:    12/    1, ite: 15552] train loss: 0.118645, tar: 0.000081 \n","l0: 0.000077, l1: 0.000073, l2: 0.001485, l3: 0.011289, l4: 0.020198, l5: 0.032771, l6: 0.049329\n","\n","[epoch: 15553/100000, batch:    12/    1, ite: 15553] train loss: 0.118643, tar: 0.000081 \n","l0: 0.000076, l1: 0.000072, l2: 0.001463, l3: 0.011146, l4: 0.019972, l5: 0.029519, l6: 0.051019\n","\n","[epoch: 15554/100000, batch:    12/    1, ite: 15554] train loss: 0.118639, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001283, l3: 0.011016, l4: 0.019400, l5: 0.031438, l6: 0.044112\n","\n","[epoch: 15555/100000, batch:    12/    1, ite: 15555] train loss: 0.118632, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001292, l3: 0.010932, l4: 0.019073, l5: 0.030694, l6: 0.049229\n","\n","[epoch: 15556/100000, batch:    12/    1, ite: 15556] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000065, l1: 0.000065, l2: 0.001258, l3: 0.011559, l4: 0.020540, l5: 0.034661, l6: 0.064555\n","\n","[epoch: 15557/100000, batch:    12/    1, ite: 15557] train loss: 0.118637, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001228, l3: 0.011170, l4: 0.019772, l5: 0.033357, l6: 0.063188\n","\n","[epoch: 15558/100000, batch:    12/    1, ite: 15558] train loss: 0.118643, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001202, l3: 0.011108, l4: 0.019061, l5: 0.030584, l6: 0.047756\n","\n","[epoch: 15559/100000, batch:    12/    1, ite: 15559] train loss: 0.118637, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001499, l3: 0.011625, l4: 0.019844, l5: 0.029674, l6: 0.051258\n","\n","[epoch: 15560/100000, batch:    12/    1, ite: 15560] train loss: 0.118634, tar: 0.000081 \n","l0: 0.000073, l1: 0.000072, l2: 0.001179, l3: 0.011511, l4: 0.020452, l5: 0.031477, l6: 0.050416\n","\n","[epoch: 15561/100000, batch:    12/    1, ite: 15561] train loss: 0.118632, tar: 0.000081 \n","l0: 0.000068, l1: 0.000077, l2: 0.001205, l3: 0.011382, l4: 0.020579, l5: 0.033516, l6: 0.048633\n","\n","[epoch: 15562/100000, batch:    12/    1, ite: 15562] train loss: 0.118630, tar: 0.000081 \n","l0: 0.000066, l1: 0.000067, l2: 0.001482, l3: 0.011578, l4: 0.019748, l5: 0.033189, l6: 0.047089\n","\n","[epoch: 15563/100000, batch:    12/    1, ite: 15563] train loss: 0.118627, tar: 0.000081 \n","l0: 0.000067, l1: 0.000068, l2: 0.001121, l3: 0.011692, l4: 0.020683, l5: 0.033375, l6: 0.059796\n","\n","[epoch: 15564/100000, batch:    12/    1, ite: 15564] train loss: 0.118632, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001462, l3: 0.011010, l4: 0.019683, l5: 0.030427, l6: 0.045771\n","\n","[epoch: 15565/100000, batch:    12/    1, ite: 15565] train loss: 0.118626, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001196, l3: 0.011251, l4: 0.019642, l5: 0.032146, l6: 0.044126\n","\n","[epoch: 15566/100000, batch:    12/    1, ite: 15566] train loss: 0.118619, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001545, l3: 0.011344, l4: 0.020495, l5: 0.034051, l6: 0.061051\n","\n","[epoch: 15567/100000, batch:    12/    1, ite: 15567] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001346, l3: 0.011300, l4: 0.020173, l5: 0.032924, l6: 0.051254\n","\n","[epoch: 15568/100000, batch:    12/    1, ite: 15568] train loss: 0.118624, tar: 0.000081 \n","l0: 0.000072, l1: 0.000071, l2: 0.001191, l3: 0.011531, l4: 0.020166, l5: 0.033261, l6: 0.048951\n","\n","[epoch: 15569/100000, batch:    12/    1, ite: 15569] train loss: 0.118622, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001208, l3: 0.011759, l4: 0.020273, l5: 0.033836, l6: 0.048534\n","\n","[epoch: 15570/100000, batch:    12/    1, ite: 15570] train loss: 0.118621, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001469, l3: 0.011107, l4: 0.019839, l5: 0.029974, l6: 0.051985\n","\n","[epoch: 15571/100000, batch:    12/    1, ite: 15571] train loss: 0.118618, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001238, l3: 0.011686, l4: 0.020279, l5: 0.032585, l6: 0.053552\n","\n","[epoch: 15572/100000, batch:    12/    1, ite: 15572] train loss: 0.118618, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001486, l3: 0.011135, l4: 0.019711, l5: 0.029896, l6: 0.051800\n","\n","[epoch: 15573/100000, batch:    12/    1, ite: 15573] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001365, l3: 0.011663, l4: 0.019988, l5: 0.031168, l6: 0.060253\n","\n","[epoch: 15574/100000, batch:    12/    1, ite: 15574] train loss: 0.118619, tar: 0.000081 \n","l0: 0.000061, l1: 0.000060, l2: 0.001542, l3: 0.011782, l4: 0.020478, l5: 0.034737, l6: 0.065732\n","\n","[epoch: 15575/100000, batch:    12/    1, ite: 15575] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000076, l1: 0.000076, l2: 0.001531, l3: 0.011036, l4: 0.019639, l5: 0.029301, l6: 0.050111\n","\n","[epoch: 15576/100000, batch:    12/    1, ite: 15576] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001465, l3: 0.011063, l4: 0.019740, l5: 0.029921, l6: 0.049462\n","\n","[epoch: 15577/100000, batch:    12/    1, ite: 15577] train loss: 0.118621, tar: 0.000081 \n","l0: 0.000072, l1: 0.000072, l2: 0.001234, l3: 0.011625, l4: 0.019973, l5: 0.034109, l6: 0.065332\n","\n","[epoch: 15578/100000, batch:    12/    1, ite: 15578] train loss: 0.118629, tar: 0.000081 \n","l0: 0.000071, l1: 0.000072, l2: 0.001169, l3: 0.011496, l4: 0.019578, l5: 0.032169, l6: 0.044232\n","\n","[epoch: 15579/100000, batch:    12/    1, ite: 15579] train loss: 0.118623, tar: 0.000081 \n","l0: 0.000071, l1: 0.000072, l2: 0.001604, l3: 0.011551, l4: 0.019565, l5: 0.032620, l6: 0.046203\n","\n","[epoch: 15580/100000, batch:    12/    1, ite: 15580] train loss: 0.118619, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001204, l3: 0.011224, l4: 0.020481, l5: 0.030809, l6: 0.055591\n","\n","[epoch: 15581/100000, batch:    12/    1, ite: 15581] train loss: 0.118619, tar: 0.000081 \n","l0: 0.000067, l1: 0.000067, l2: 0.001674, l3: 0.011390, l4: 0.020429, l5: 0.033343, l6: 0.060989\n","\n","[epoch: 15582/100000, batch:    12/    1, ite: 15582] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000062, l1: 0.000063, l2: 0.001151, l3: 0.011429, l4: 0.019519, l5: 0.030069, l6: 0.047832\n","\n","[epoch: 15583/100000, batch:    12/    1, ite: 15583] train loss: 0.118620, tar: 0.000081 \n","l0: 0.000064, l1: 0.000064, l2: 0.001508, l3: 0.011700, l4: 0.020165, l5: 0.033063, l6: 0.048403\n","\n","[epoch: 15584/100000, batch:    12/    1, ite: 15584] train loss: 0.118618, tar: 0.000081 \n","l0: 0.000072, l1: 0.000073, l2: 0.001136, l3: 0.011314, l4: 0.020182, l5: 0.031350, l6: 0.058198\n","\n","[epoch: 15585/100000, batch:    12/    1, ite: 15585] train loss: 0.118620, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001335, l3: 0.011123, l4: 0.020110, l5: 0.032105, l6: 0.048053\n","\n","[epoch: 15586/100000, batch:    12/    1, ite: 15586] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000069, l1: 0.000068, l2: 0.001281, l3: 0.011286, l4: 0.020442, l5: 0.034789, l6: 0.063913\n","\n","[epoch: 15587/100000, batch:    12/    1, ite: 15587] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000070, l1: 0.000070, l2: 0.001253, l3: 0.011585, l4: 0.020120, l5: 0.030426, l6: 0.051062\n","\n","[epoch: 15588/100000, batch:    12/    1, ite: 15588] train loss: 0.118622, tar: 0.000081 \n","l0: 0.000067, l1: 0.000068, l2: 0.001591, l3: 0.011678, l4: 0.020075, l5: 0.031858, l6: 0.057482\n","\n","[epoch: 15589/100000, batch:    12/    1, ite: 15589] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000067, l1: 0.000068, l2: 0.001163, l3: 0.011699, l4: 0.020161, l5: 0.031918, l6: 0.053632\n","\n","[epoch: 15590/100000, batch:    12/    1, ite: 15590] train loss: 0.118625, tar: 0.000081 \n","l0: 0.000059, l1: 0.000060, l2: 0.001178, l3: 0.011373, l4: 0.019898, l5: 0.030737, l6: 0.043065\n","\n","[epoch: 15591/100000, batch:    12/    1, ite: 15591] train loss: 0.118617, tar: 0.000081 \n","l0: 0.000071, l1: 0.000071, l2: 0.001165, l3: 0.011728, l4: 0.020362, l5: 0.033119, l6: 0.049960\n","\n","[epoch: 15592/100000, batch:    12/    1, ite: 15592] train loss: 0.118616, tar: 0.000081 \n","l0: 0.000066, l1: 0.000066, l2: 0.001522, l3: 0.011176, l4: 0.020159, l5: 0.032463, l6: 0.047656\n","\n","[epoch: 15593/100000, batch:    12/    1, ite: 15593] train loss: 0.118612, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001333, l3: 0.011093, l4: 0.020020, l5: 0.029983, l6: 0.048594\n","\n","[epoch: 15594/100000, batch:    12/    1, ite: 15594] train loss: 0.118608, tar: 0.000081 \n","l0: 0.000064, l1: 0.000064, l2: 0.001334, l3: 0.011556, l4: 0.019802, l5: 0.030911, l6: 0.052908\n","\n","[epoch: 15595/100000, batch:    12/    1, ite: 15595] train loss: 0.118606, tar: 0.000081 \n","l0: 0.000068, l1: 0.000073, l2: 0.001219, l3: 0.011203, l4: 0.019305, l5: 0.031214, l6: 0.051428\n","\n","[epoch: 15596/100000, batch:    12/    1, ite: 15596] train loss: 0.118604, tar: 0.000081 \n","l0: 0.000068, l1: 0.000067, l2: 0.001200, l3: 0.011194, l4: 0.020499, l5: 0.031693, l6: 0.050363\n","\n","[epoch: 15597/100000, batch:    12/    1, ite: 15597] train loss: 0.118602, tar: 0.000081 \n","l0: 0.000064, l1: 0.000068, l2: 0.001387, l3: 0.011343, l4: 0.020312, l5: 0.031469, l6: 0.052321\n","\n","[epoch: 15598/100000, batch:    12/    1, ite: 15598] train loss: 0.118601, tar: 0.000081 \n","l0: 0.000076, l1: 0.000077, l2: 0.001171, l3: 0.011458, l4: 0.020953, l5: 0.032022, l6: 0.059968\n","\n","[epoch: 15599/100000, batch:    12/    1, ite: 15599] train loss: 0.118605, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001112, l3: 0.011487, l4: 0.020003, l5: 0.031715, l6: 0.060675\n","\n","[epoch: 15600/100000, batch:    12/    1, ite: 15600] train loss: 0.118609, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001172, l3: 0.011548, l4: 0.019932, l5: 0.034387, l6: 0.065145\n","\n","[epoch: 15601/100000, batch:    12/    1, ite: 15601] train loss: 0.118618, tar: 0.000081 \n","l0: 0.000063, l1: 0.000063, l2: 0.001517, l3: 0.011246, l4: 0.019384, l5: 0.032484, l6: 0.063788\n","\n","[epoch: 15602/100000, batch:    12/    1, ite: 15602] train loss: 0.118624, tar: 0.000081 \n","l0: 0.000068, l1: 0.000068, l2: 0.001406, l3: 0.011449, l4: 0.020111, l5: 0.033169, l6: 0.064544\n","\n","[epoch: 15603/100000, batch:    12/    1, ite: 15603] train loss: 0.118631, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001143, l3: 0.011535, l4: 0.019881, l5: 0.031462, l6: 0.051513\n","\n","[epoch: 15604/100000, batch:    12/    1, ite: 15604] train loss: 0.118630, tar: 0.000080 \n","l0: 0.000076, l1: 0.000072, l2: 0.001500, l3: 0.011361, l4: 0.020290, l5: 0.031062, l6: 0.052140\n","\n","[epoch: 15605/100000, batch:    12/    1, ite: 15605] train loss: 0.118628, tar: 0.000080 \n","l0: 0.000073, l1: 0.000072, l2: 0.001577, l3: 0.011390, l4: 0.020601, l5: 0.033713, l6: 0.060792\n","\n","[epoch: 15606/100000, batch:    12/    1, ite: 15606] train loss: 0.118634, tar: 0.000080 \n","l0: 0.000067, l1: 0.000072, l2: 0.001440, l3: 0.011168, l4: 0.020213, l5: 0.033670, l6: 0.048418\n","\n","[epoch: 15607/100000, batch:    12/    1, ite: 15607] train loss: 0.118632, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001256, l3: 0.011639, l4: 0.020327, l5: 0.031354, l6: 0.053799\n","\n","[epoch: 15608/100000, batch:    12/    1, ite: 15608] train loss: 0.118632, tar: 0.000080 \n","l0: 0.000057, l1: 0.000064, l2: 0.001363, l3: 0.011213, l4: 0.020155, l5: 0.031214, l6: 0.048912\n","\n","[epoch: 15609/100000, batch:    12/    1, ite: 15609] train loss: 0.118628, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001498, l3: 0.011348, l4: 0.020434, l5: 0.031731, l6: 0.053961\n","\n","[epoch: 15610/100000, batch:    12/    1, ite: 15610] train loss: 0.118629, tar: 0.000080 \n","l0: 0.000064, l1: 0.000068, l2: 0.001166, l3: 0.011865, l4: 0.020381, l5: 0.033209, l6: 0.058635\n","\n","[epoch: 15611/100000, batch:    12/    1, ite: 15611] train loss: 0.118633, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001379, l3: 0.011181, l4: 0.019844, l5: 0.030152, l6: 0.046222\n","\n","[epoch: 15612/100000, batch:    12/    1, ite: 15612] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001157, l3: 0.011193, l4: 0.020092, l5: 0.032698, l6: 0.047905\n","\n","[epoch: 15613/100000, batch:    12/    1, ite: 15613] train loss: 0.118624, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001338, l3: 0.011258, l4: 0.019922, l5: 0.030332, l6: 0.043675\n","\n","[epoch: 15614/100000, batch:    12/    1, ite: 15614] train loss: 0.118616, tar: 0.000080 \n","l0: 0.000063, l1: 0.000068, l2: 0.001449, l3: 0.011569, l4: 0.019723, l5: 0.033096, l6: 0.047313\n","\n","[epoch: 15615/100000, batch:    12/    1, ite: 15615] train loss: 0.118613, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001509, l3: 0.011510, l4: 0.019968, l5: 0.030686, l6: 0.045614\n","\n","[epoch: 15616/100000, batch:    12/    1, ite: 15616] train loss: 0.118607, tar: 0.000080 \n","l0: 0.000065, l1: 0.000065, l2: 0.001216, l3: 0.011577, l4: 0.020125, l5: 0.030748, l6: 0.052604\n","\n","[epoch: 15617/100000, batch:    12/    1, ite: 15617] train loss: 0.118606, tar: 0.000080 \n","l0: 0.000071, l1: 0.000067, l2: 0.001506, l3: 0.011115, l4: 0.020239, l5: 0.031522, l6: 0.050892\n","\n","[epoch: 15618/100000, batch:    12/    1, ite: 15618] train loss: 0.118604, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001162, l3: 0.011294, l4: 0.020365, l5: 0.032494, l6: 0.047693\n","\n","[epoch: 15619/100000, batch:    12/    1, ite: 15619] train loss: 0.118600, tar: 0.000080 \n","l0: 0.000072, l1: 0.000072, l2: 0.001265, l3: 0.011612, l4: 0.020300, l5: 0.032817, l6: 0.049687\n","\n","[epoch: 15620/100000, batch:    12/    1, ite: 15620] train loss: 0.118599, tar: 0.000080 \n","l0: 0.000065, l1: 0.000065, l2: 0.001113, l3: 0.011554, l4: 0.019783, l5: 0.029580, l6: 0.050491\n","\n","[epoch: 15621/100000, batch:    12/    1, ite: 15621] train loss: 0.118595, tar: 0.000080 \n","l0: 0.000068, l1: 0.000067, l2: 0.001139, l3: 0.011157, l4: 0.020233, l5: 0.031456, l6: 0.056979\n","\n","[epoch: 15622/100000, batch:    12/    1, ite: 15622] train loss: 0.118597, tar: 0.000080 \n","l0: 0.000074, l1: 0.000074, l2: 0.001374, l3: 0.010881, l4: 0.020297, l5: 0.033863, l6: 0.066860\n","\n","[epoch: 15623/100000, batch:    12/    1, ite: 15623] train loss: 0.118606, tar: 0.000080 \n","l0: 0.000071, l1: 0.000070, l2: 0.001391, l3: 0.011637, l4: 0.019559, l5: 0.032123, l6: 0.044019\n","\n","[epoch: 15624/100000, batch:    12/    1, ite: 15624] train loss: 0.118600, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001427, l3: 0.011072, l4: 0.019897, l5: 0.032280, l6: 0.053199\n","\n","[epoch: 15625/100000, batch:    12/    1, ite: 15625] train loss: 0.118599, tar: 0.000080 \n","l0: 0.000073, l1: 0.000073, l2: 0.001537, l3: 0.011158, l4: 0.020348, l5: 0.034591, l6: 0.066530\n","\n","[epoch: 15626/100000, batch:    12/    1, ite: 15626] train loss: 0.118609, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001493, l3: 0.011772, l4: 0.020317, l5: 0.031772, l6: 0.047901\n","\n","[epoch: 15627/100000, batch:    12/    1, ite: 15627] train loss: 0.118606, tar: 0.000080 \n","l0: 0.000074, l1: 0.000075, l2: 0.001342, l3: 0.010740, l4: 0.020088, l5: 0.033282, l6: 0.066951\n","\n","[epoch: 15628/100000, batch:    12/    1, ite: 15628] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000059, l1: 0.000067, l2: 0.001437, l3: 0.011741, l4: 0.020430, l5: 0.033235, l6: 0.049951\n","\n","[epoch: 15629/100000, batch:    12/    1, ite: 15629] train loss: 0.118613, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001173, l3: 0.011152, l4: 0.019963, l5: 0.033779, l6: 0.049116\n","\n","[epoch: 15630/100000, batch:    12/    1, ite: 15630] train loss: 0.118611, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001302, l3: 0.011129, l4: 0.020248, l5: 0.030165, l6: 0.048997\n","\n","[epoch: 15631/100000, batch:    12/    1, ite: 15631] train loss: 0.118607, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001186, l3: 0.011386, l4: 0.019476, l5: 0.031695, l6: 0.047571\n","\n","[epoch: 15632/100000, batch:    12/    1, ite: 15632] train loss: 0.118603, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001241, l3: 0.011377, l4: 0.020004, l5: 0.032596, l6: 0.063866\n","\n","[epoch: 15633/100000, batch:    12/    1, ite: 15633] train loss: 0.118609, tar: 0.000080 \n","l0: 0.000055, l1: 0.000064, l2: 0.001311, l3: 0.011415, l4: 0.019396, l5: 0.029102, l6: 0.048991\n","\n","[epoch: 15634/100000, batch:    12/    1, ite: 15634] train loss: 0.118604, tar: 0.000080 \n","l0: 0.000070, l1: 0.000070, l2: 0.001390, l3: 0.011537, l4: 0.019258, l5: 0.030178, l6: 0.047454\n","\n","[epoch: 15635/100000, batch:    12/    1, ite: 15635] train loss: 0.118599, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001533, l3: 0.011672, l4: 0.020357, l5: 0.031774, l6: 0.048149\n","\n","[epoch: 15636/100000, batch:    12/    1, ite: 15636] train loss: 0.118596, tar: 0.000080 \n","l0: 0.000061, l1: 0.000061, l2: 0.001343, l3: 0.011293, l4: 0.020555, l5: 0.034736, l6: 0.062479\n","\n","[epoch: 15637/100000, batch:    12/    1, ite: 15637] train loss: 0.118603, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001225, l3: 0.011691, l4: 0.020615, l5: 0.033319, l6: 0.059511\n","\n","[epoch: 15638/100000, batch:    12/    1, ite: 15638] train loss: 0.118608, tar: 0.000080 \n","l0: 0.000073, l1: 0.000073, l2: 0.001380, l3: 0.011445, l4: 0.020344, l5: 0.030755, l6: 0.048893\n","\n","[epoch: 15639/100000, batch:    12/    1, ite: 15639] train loss: 0.118605, tar: 0.000080 \n","l0: 0.000069, l1: 0.000069, l2: 0.001598, l3: 0.011757, l4: 0.020649, l5: 0.035067, l6: 0.065449\n","\n","[epoch: 15640/100000, batch:    12/    1, ite: 15640] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000067, l1: 0.000066, l2: 0.001215, l3: 0.011234, l4: 0.020068, l5: 0.030689, l6: 0.045783\n","\n","[epoch: 15641/100000, batch:    12/    1, ite: 15641] train loss: 0.118609, tar: 0.000080 \n","l0: 0.000071, l1: 0.000067, l2: 0.001501, l3: 0.011121, l4: 0.020235, l5: 0.031476, l6: 0.051007\n","\n","[epoch: 15642/100000, batch:    12/    1, ite: 15642] train loss: 0.118607, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001124, l3: 0.011527, l4: 0.020023, l5: 0.031922, l6: 0.051851\n","\n","[epoch: 15643/100000, batch:    12/    1, ite: 15643] train loss: 0.118605, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001289, l3: 0.011080, l4: 0.020437, l5: 0.031028, l6: 0.051386\n","\n","[epoch: 15644/100000, batch:    12/    1, ite: 15644] train loss: 0.118604, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001180, l3: 0.011313, l4: 0.019571, l5: 0.029793, l6: 0.050554\n","\n","[epoch: 15645/100000, batch:    12/    1, ite: 15645] train loss: 0.118600, tar: 0.000080 \n","l0: 0.000070, l1: 0.000070, l2: 0.001580, l3: 0.011068, l4: 0.020516, l5: 0.034973, l6: 0.066838\n","\n","[epoch: 15646/100000, batch:    12/    1, ite: 15646] train loss: 0.118610, tar: 0.000080 \n","l0: 0.000059, l1: 0.000063, l2: 0.001332, l3: 0.011505, l4: 0.019438, l5: 0.030879, l6: 0.054999\n","\n","[epoch: 15647/100000, batch:    12/    1, ite: 15647] train loss: 0.118610, tar: 0.000080 \n","l0: 0.000058, l1: 0.000058, l2: 0.001313, l3: 0.011414, l4: 0.019226, l5: 0.031829, l6: 0.042732\n","\n","[epoch: 15648/100000, batch:    12/    1, ite: 15648] train loss: 0.118602, tar: 0.000080 \n","l0: 0.000073, l1: 0.000074, l2: 0.001518, l3: 0.011248, l4: 0.020867, l5: 0.035257, l6: 0.067086\n","\n","[epoch: 15649/100000, batch:    12/    1, ite: 15649] train loss: 0.118613, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001528, l3: 0.011641, l4: 0.020383, l5: 0.032109, l6: 0.058978\n","\n","[epoch: 15650/100000, batch:    12/    1, ite: 15650] train loss: 0.118617, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001253, l3: 0.011520, l4: 0.020630, l5: 0.032710, l6: 0.053768\n","\n","[epoch: 15651/100000, batch:    12/    1, ite: 15651] train loss: 0.118618, tar: 0.000080 \n","l0: 0.000065, l1: 0.000064, l2: 0.001503, l3: 0.011206, l4: 0.019994, l5: 0.032306, l6: 0.049492\n","\n","[epoch: 15652/100000, batch:    12/    1, ite: 15652] train loss: 0.118615, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001166, l3: 0.011820, l4: 0.020568, l5: 0.033859, l6: 0.060201\n","\n","[epoch: 15653/100000, batch:    12/    1, ite: 15653] train loss: 0.118621, tar: 0.000080 \n","l0: 0.000061, l1: 0.000062, l2: 0.001461, l3: 0.011404, l4: 0.019184, l5: 0.030954, l6: 0.052869\n","\n","[epoch: 15654/100000, batch:    12/    1, ite: 15654] train loss: 0.118619, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001148, l3: 0.011129, l4: 0.020079, l5: 0.032384, l6: 0.045895\n","\n","[epoch: 15655/100000, batch:    12/    1, ite: 15655] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001516, l3: 0.011614, l4: 0.019991, l5: 0.032281, l6: 0.052081\n","\n","[epoch: 15656/100000, batch:    12/    1, ite: 15656] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000069, l1: 0.000069, l2: 0.001610, l3: 0.011790, l4: 0.020124, l5: 0.033703, l6: 0.065483\n","\n","[epoch: 15657/100000, batch:    12/    1, ite: 15657] train loss: 0.118622, tar: 0.000080 \n","l0: 0.000061, l1: 0.000061, l2: 0.001152, l3: 0.011036, l4: 0.019167, l5: 0.031446, l6: 0.042449\n","\n","[epoch: 15658/100000, batch:    12/    1, ite: 15658] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000062, l1: 0.000063, l2: 0.001309, l3: 0.011546, l4: 0.019722, l5: 0.033511, l6: 0.048521\n","\n","[epoch: 15659/100000, batch:    12/    1, ite: 15659] train loss: 0.118612, tar: 0.000080 \n","l0: 0.000068, l1: 0.000067, l2: 0.001295, l3: 0.011302, l4: 0.019949, l5: 0.033174, l6: 0.063974\n","\n","[epoch: 15660/100000, batch:    12/    1, ite: 15660] train loss: 0.118619, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001182, l3: 0.011618, l4: 0.019719, l5: 0.032575, l6: 0.045620\n","\n","[epoch: 15661/100000, batch:    12/    1, ite: 15661] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001166, l3: 0.011740, l4: 0.021162, l5: 0.032566, l6: 0.048166\n","\n","[epoch: 15662/100000, batch:    12/    1, ite: 15662] train loss: 0.118612, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001504, l3: 0.010948, l4: 0.019860, l5: 0.033258, l6: 0.064931\n","\n","[epoch: 15663/100000, batch:    12/    1, ite: 15663] train loss: 0.118619, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001193, l3: 0.011198, l4: 0.019323, l5: 0.030180, l6: 0.048644\n","\n","[epoch: 15664/100000, batch:    12/    1, ite: 15664] train loss: 0.118614, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001411, l3: 0.011183, l4: 0.020454, l5: 0.033680, l6: 0.049200\n","\n","[epoch: 15665/100000, batch:    12/    1, ite: 15665] train loss: 0.118613, tar: 0.000080 \n","l0: 0.000061, l1: 0.000062, l2: 0.001138, l3: 0.010999, l4: 0.019075, l5: 0.031110, l6: 0.050189\n","\n","[epoch: 15666/100000, batch:    12/    1, ite: 15666] train loss: 0.118609, tar: 0.000080 \n","l0: 0.000072, l1: 0.000072, l2: 0.001481, l3: 0.011038, l4: 0.020202, l5: 0.031699, l6: 0.050822\n","\n","[epoch: 15667/100000, batch:    12/    1, ite: 15667] train loss: 0.118607, tar: 0.000080 \n","l0: 0.000067, l1: 0.000071, l2: 0.001167, l3: 0.011591, l4: 0.019753, l5: 0.032630, l6: 0.045618\n","\n","[epoch: 15668/100000, batch:    12/    1, ite: 15668] train loss: 0.118603, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001216, l3: 0.011438, l4: 0.019960, l5: 0.033991, l6: 0.065604\n","\n","[epoch: 15669/100000, batch:    12/    1, ite: 15669] train loss: 0.118611, tar: 0.000080 \n","l0: 0.000070, l1: 0.000071, l2: 0.001453, l3: 0.011563, l4: 0.019692, l5: 0.029955, l6: 0.048332\n","\n","[epoch: 15670/100000, batch:    12/    1, ite: 15670] train loss: 0.118606, tar: 0.000080 \n","l0: 0.000072, l1: 0.000076, l2: 0.001206, l3: 0.011717, l4: 0.020641, l5: 0.031043, l6: 0.049853\n","\n","[epoch: 15671/100000, batch:    12/    1, ite: 15671] train loss: 0.118604, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001522, l3: 0.011711, l4: 0.020198, l5: 0.034813, l6: 0.064140\n","\n","[epoch: 15672/100000, batch:    12/    1, ite: 15672] train loss: 0.118612, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001580, l3: 0.011359, l4: 0.020219, l5: 0.032784, l6: 0.049188\n","\n","[epoch: 15673/100000, batch:    12/    1, ite: 15673] train loss: 0.118610, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001241, l3: 0.011309, l4: 0.020314, l5: 0.033826, l6: 0.062591\n","\n","[epoch: 15674/100000, batch:    12/    1, ite: 15674] train loss: 0.118617, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001125, l3: 0.011497, l4: 0.019591, l5: 0.031672, l6: 0.046883\n","\n","[epoch: 15675/100000, batch:    12/    1, ite: 15675] train loss: 0.118612, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001166, l3: 0.011679, l4: 0.020504, l5: 0.033655, l6: 0.061820\n","\n","[epoch: 15676/100000, batch:    12/    1, ite: 15676] train loss: 0.118618, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001345, l3: 0.011532, l4: 0.019559, l5: 0.031760, l6: 0.052687\n","\n","[epoch: 15677/100000, batch:    12/    1, ite: 15677] train loss: 0.118617, tar: 0.000080 \n","l0: 0.000070, l1: 0.000071, l2: 0.001175, l3: 0.011633, l4: 0.020133, l5: 0.030669, l6: 0.052437\n","\n","[epoch: 15678/100000, batch:    12/    1, ite: 15678] train loss: 0.118616, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001147, l3: 0.011184, l4: 0.019978, l5: 0.030602, l6: 0.045020\n","\n","[epoch: 15679/100000, batch:    12/    1, ite: 15679] train loss: 0.118610, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001442, l3: 0.011737, l4: 0.020305, l5: 0.031031, l6: 0.055737\n","\n","[epoch: 15680/100000, batch:    12/    1, ite: 15680] train loss: 0.118611, tar: 0.000080 \n","l0: 0.000072, l1: 0.000071, l2: 0.001199, l3: 0.011201, l4: 0.019847, l5: 0.030343, l6: 0.045142\n","\n","[epoch: 15681/100000, batch:    12/    1, ite: 15681] train loss: 0.118604, tar: 0.000080 \n","l0: 0.000072, l1: 0.000071, l2: 0.001129, l3: 0.011511, l4: 0.020127, l5: 0.032133, l6: 0.050150\n","\n","[epoch: 15682/100000, batch:    12/    1, ite: 15682] train loss: 0.118602, tar: 0.000080 \n","l0: 0.000070, l1: 0.000070, l2: 0.001321, l3: 0.010919, l4: 0.020089, l5: 0.033577, l6: 0.066913\n","\n","[epoch: 15683/100000, batch:    12/    1, ite: 15683] train loss: 0.118611, tar: 0.000080 \n","l0: 0.000070, l1: 0.000074, l2: 0.001659, l3: 0.011158, l4: 0.020539, l5: 0.034356, l6: 0.066435\n","\n","[epoch: 15684/100000, batch:    12/    1, ite: 15684] train loss: 0.118620, tar: 0.000080 \n","l0: 0.000068, l1: 0.000069, l2: 0.001309, l3: 0.011373, l4: 0.020122, l5: 0.033522, l6: 0.065221\n","\n","[epoch: 15685/100000, batch:    12/    1, ite: 15685] train loss: 0.118628, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001128, l3: 0.011711, l4: 0.020221, l5: 0.033798, l6: 0.048694\n","\n","[epoch: 15686/100000, batch:    12/    1, ite: 15686] train loss: 0.118626, tar: 0.000080 \n","l0: 0.000076, l1: 0.000076, l2: 0.001149, l3: 0.011764, l4: 0.020366, l5: 0.034358, l6: 0.048978\n","\n","[epoch: 15687/100000, batch:    12/    1, ite: 15687] train loss: 0.118625, tar: 0.000080 \n","l0: 0.000064, l1: 0.000065, l2: 0.001189, l3: 0.011677, l4: 0.019945, l5: 0.033720, l6: 0.066214\n","\n","[epoch: 15688/100000, batch:    12/    1, ite: 15688] train loss: 0.118634, tar: 0.000080 \n","l0: 0.000076, l1: 0.000076, l2: 0.001209, l3: 0.011372, l4: 0.020545, l5: 0.031414, l6: 0.050414\n","\n","[epoch: 15689/100000, batch:    12/    1, ite: 15689] train loss: 0.118631, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001146, l3: 0.011581, l4: 0.019590, l5: 0.029583, l6: 0.049347\n","\n","[epoch: 15690/100000, batch:    12/    1, ite: 15690] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000066, l1: 0.000067, l2: 0.001552, l3: 0.011588, l4: 0.019484, l5: 0.030562, l6: 0.055574\n","\n","[epoch: 15691/100000, batch:    12/    1, ite: 15691] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001223, l3: 0.011093, l4: 0.019677, l5: 0.032300, l6: 0.045072\n","\n","[epoch: 15692/100000, batch:    12/    1, ite: 15692] train loss: 0.118622, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001331, l3: 0.011324, l4: 0.020098, l5: 0.034024, l6: 0.062593\n","\n","[epoch: 15693/100000, batch:    12/    1, ite: 15693] train loss: 0.118628, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001293, l3: 0.011540, l4: 0.020447, l5: 0.033101, l6: 0.060766\n","\n","[epoch: 15694/100000, batch:    12/    1, ite: 15694] train loss: 0.118633, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001297, l3: 0.011253, l4: 0.020553, l5: 0.031736, l6: 0.058711\n","\n","[epoch: 15695/100000, batch:    12/    1, ite: 15695] train loss: 0.118636, tar: 0.000080 \n","l0: 0.000076, l1: 0.000076, l2: 0.001471, l3: 0.011766, l4: 0.020042, l5: 0.030393, l6: 0.052396\n","\n","[epoch: 15696/100000, batch:    12/    1, ite: 15696] train loss: 0.118635, tar: 0.000080 \n","l0: 0.000074, l1: 0.000074, l2: 0.001414, l3: 0.011126, l4: 0.020098, l5: 0.033026, l6: 0.066076\n","\n","[epoch: 15697/100000, batch:    12/    1, ite: 15697] train loss: 0.118643, tar: 0.000080 \n","l0: 0.000067, l1: 0.000071, l2: 0.001441, l3: 0.011130, l4: 0.020457, l5: 0.033537, l6: 0.049619\n","\n","[epoch: 15698/100000, batch:    12/    1, ite: 15698] train loss: 0.118641, tar: 0.000080 \n","l0: 0.000073, l1: 0.000072, l2: 0.001159, l3: 0.011243, l4: 0.020082, l5: 0.030507, l6: 0.044976\n","\n","[epoch: 15699/100000, batch:    12/    1, ite: 15699] train loss: 0.118635, tar: 0.000080 \n","l0: 0.000072, l1: 0.000071, l2: 0.001460, l3: 0.011535, l4: 0.019714, l5: 0.031324, l6: 0.052302\n","\n","[epoch: 15700/100000, batch:    12/    1, ite: 15700] train loss: 0.118634, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001164, l3: 0.011419, l4: 0.020547, l5: 0.031086, l6: 0.056174\n","\n","[epoch: 15701/100000, batch:    12/    1, ite: 15701] train loss: 0.118635, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001154, l3: 0.011390, l4: 0.020766, l5: 0.031121, l6: 0.049995\n","\n","[epoch: 15702/100000, batch:    12/    1, ite: 15702] train loss: 0.118633, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001166, l3: 0.011669, l4: 0.019999, l5: 0.032340, l6: 0.053659\n","\n","[epoch: 15703/100000, batch:    12/    1, ite: 15703] train loss: 0.118633, tar: 0.000080 \n","l0: 0.000071, l1: 0.000072, l2: 0.001315, l3: 0.010959, l4: 0.019752, l5: 0.030019, l6: 0.045135\n","\n","[epoch: 15704/100000, batch:    12/    1, ite: 15704] train loss: 0.118626, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001108, l3: 0.011532, l4: 0.019701, l5: 0.031996, l6: 0.051697\n","\n","[epoch: 15705/100000, batch:    12/    1, ite: 15705] train loss: 0.118625, tar: 0.000080 \n","l0: 0.000067, l1: 0.000071, l2: 0.001225, l3: 0.011158, l4: 0.019256, l5: 0.031314, l6: 0.052790\n","\n","[epoch: 15706/100000, batch:    12/    1, ite: 15706] train loss: 0.118623, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001576, l3: 0.011121, l4: 0.019576, l5: 0.030458, l6: 0.050054\n","\n","[epoch: 15707/100000, batch:    12/    1, ite: 15707] train loss: 0.118620, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001662, l3: 0.011724, l4: 0.019691, l5: 0.032768, l6: 0.064725\n","\n","[epoch: 15708/100000, batch:    12/    1, ite: 15708] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000067, l1: 0.000069, l2: 0.001561, l3: 0.011701, l4: 0.020588, l5: 0.032016, l6: 0.057176\n","\n","[epoch: 15709/100000, batch:    12/    1, ite: 15709] train loss: 0.118630, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001474, l3: 0.011494, l4: 0.019628, l5: 0.030386, l6: 0.051757\n","\n","[epoch: 15710/100000, batch:    12/    1, ite: 15710] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001508, l3: 0.011580, l4: 0.019245, l5: 0.032007, l6: 0.042891\n","\n","[epoch: 15711/100000, batch:    12/    1, ite: 15711] train loss: 0.118621, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001486, l3: 0.011550, l4: 0.019938, l5: 0.030810, l6: 0.047300\n","\n","[epoch: 15712/100000, batch:    12/    1, ite: 15712] train loss: 0.118616, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001207, l3: 0.011621, l4: 0.020500, l5: 0.034994, l6: 0.066576\n","\n","[epoch: 15713/100000, batch:    12/    1, ite: 15713] train loss: 0.118626, tar: 0.000080 \n","l0: 0.000072, l1: 0.000075, l2: 0.001194, l3: 0.011714, l4: 0.020553, l5: 0.031035, l6: 0.050093\n","\n","[epoch: 15714/100000, batch:    12/    1, ite: 15714] train loss: 0.118624, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001263, l3: 0.011326, l4: 0.020082, l5: 0.034456, l6: 0.062990\n","\n","[epoch: 15715/100000, batch:    12/    1, ite: 15715] train loss: 0.118631, tar: 0.000080 \n","l0: 0.000074, l1: 0.000075, l2: 0.001423, l3: 0.011043, l4: 0.020338, l5: 0.034201, l6: 0.067049\n","\n","[epoch: 15716/100000, batch:    12/    1, ite: 15716] train loss: 0.118640, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001422, l3: 0.011101, l4: 0.019600, l5: 0.030119, l6: 0.047801\n","\n","[epoch: 15717/100000, batch:    12/    1, ite: 15717] train loss: 0.118635, tar: 0.000080 \n","l0: 0.000072, l1: 0.000072, l2: 0.001210, l3: 0.011258, l4: 0.020023, l5: 0.032735, l6: 0.047011\n","\n","[epoch: 15718/100000, batch:    12/    1, ite: 15718] train loss: 0.118631, tar: 0.000080 \n","l0: 0.000059, l1: 0.000063, l2: 0.001335, l3: 0.011750, l4: 0.020384, l5: 0.033404, l6: 0.048525\n","\n","[epoch: 15719/100000, batch:    12/    1, ite: 15719] train loss: 0.118629, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001257, l3: 0.011692, l4: 0.020411, l5: 0.034267, l6: 0.065262\n","\n","[epoch: 15720/100000, batch:    12/    1, ite: 15720] train loss: 0.118638, tar: 0.000080 \n","l0: 0.000063, l1: 0.000063, l2: 0.001651, l3: 0.011143, l4: 0.019738, l5: 0.032159, l6: 0.062066\n","\n","[epoch: 15721/100000, batch:    12/    1, ite: 15721] train loss: 0.118642, tar: 0.000080 \n","l0: 0.000059, l1: 0.000060, l2: 0.001379, l3: 0.011680, l4: 0.020366, l5: 0.032463, l6: 0.056899\n","\n","[epoch: 15722/100000, batch:    12/    1, ite: 15722] train loss: 0.118645, tar: 0.000080 \n","l0: 0.000053, l1: 0.000058, l2: 0.001235, l3: 0.011307, l4: 0.018770, l5: 0.030295, l6: 0.047051\n","\n","[epoch: 15723/100000, batch:    12/    1, ite: 15723] train loss: 0.118639, tar: 0.000080 \n","l0: 0.000064, l1: 0.000064, l2: 0.001304, l3: 0.011231, l4: 0.020146, l5: 0.030825, l6: 0.053169\n","\n","[epoch: 15724/100000, batch:    12/    1, ite: 15724] train loss: 0.118638, tar: 0.000080 \n","l0: 0.000068, l1: 0.000068, l2: 0.001518, l3: 0.011325, l4: 0.020691, l5: 0.031317, l6: 0.058665\n","\n","[epoch: 15725/100000, batch:    12/    1, ite: 15725] train loss: 0.118641, tar: 0.000080 \n","l0: 0.000067, l1: 0.000067, l2: 0.001230, l3: 0.011373, l4: 0.020179, l5: 0.033402, l6: 0.047698\n","\n","[epoch: 15726/100000, batch:    12/    1, ite: 15726] train loss: 0.118638, tar: 0.000080 \n","l0: 0.000062, l1: 0.000063, l2: 0.001159, l3: 0.011625, l4: 0.020268, l5: 0.031674, l6: 0.046864\n","\n","[epoch: 15727/100000, batch:    12/    1, ite: 15727] train loss: 0.118634, tar: 0.000080 \n","l0: 0.000061, l1: 0.000061, l2: 0.001146, l3: 0.011300, l4: 0.019060, l5: 0.030607, l6: 0.054190\n","\n","[epoch: 15728/100000, batch:    12/    1, ite: 15728] train loss: 0.118633, tar: 0.000080 \n","l0: 0.000072, l1: 0.000076, l2: 0.001255, l3: 0.011347, l4: 0.020465, l5: 0.032930, l6: 0.047932\n","\n","[epoch: 15729/100000, batch:    12/    1, ite: 15729] train loss: 0.118630, tar: 0.000080 \n","l0: 0.000071, l1: 0.000071, l2: 0.001466, l3: 0.011160, l4: 0.020264, l5: 0.033810, l6: 0.048691\n","\n","[epoch: 15730/100000, batch:    12/    1, ite: 15730] train loss: 0.118629, tar: 0.000080 \n","l0: 0.000064, l1: 0.000065, l2: 0.001158, l3: 0.011132, l4: 0.019196, l5: 0.030304, l6: 0.048574\n","\n","[epoch: 15731/100000, batch:    12/    1, ite: 15731] train loss: 0.118624, tar: 0.000080 \n","l0: 0.000069, l1: 0.000068, l2: 0.001581, l3: 0.011770, l4: 0.020328, l5: 0.034164, l6: 0.063842\n","\n","[epoch: 15732/100000, batch:    12/    1, ite: 15732] train loss: 0.118632, tar: 0.000080 \n","l0: 0.000066, l1: 0.000066, l2: 0.001159, l3: 0.011032, l4: 0.019508, l5: 0.032469, l6: 0.045728\n","\n","[epoch: 15733/100000, batch:    12/    1, ite: 15733] train loss: 0.118627, tar: 0.000080 \n","l0: 0.000060, l1: 0.000064, l2: 0.001343, l3: 0.011122, l4: 0.019996, l5: 0.030784, l6: 0.052436\n","\n","[epoch: 15734/100000, batch:    12/    1, ite: 15734] train loss: 0.118625, tar: 0.000080 \n","l0: 0.000069, l1: 0.000069, l2: 0.001336, l3: 0.010851, l4: 0.020293, l5: 0.033790, l6: 0.066809\n","\n","[epoch: 15735/100000, batch:    12/    1, ite: 15735] train loss: 0.118633, tar: 0.000079 \n","l0: 0.000071, l1: 0.000072, l2: 0.001146, l3: 0.011518, l4: 0.020010, l5: 0.031054, l6: 0.045894\n","\n","[epoch: 15736/100000, batch:    12/    1, ite: 15736] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000068, l1: 0.000072, l2: 0.001157, l3: 0.011296, l4: 0.020049, l5: 0.030802, l6: 0.050241\n","\n","[epoch: 15737/100000, batch:    12/    1, ite: 15737] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001200, l3: 0.011173, l4: 0.020480, l5: 0.030789, l6: 0.055910\n","\n","[epoch: 15738/100000, batch:    12/    1, ite: 15738] train loss: 0.118626, tar: 0.000079 \n","l0: 0.000055, l1: 0.000064, l2: 0.001346, l3: 0.011572, l4: 0.020083, l5: 0.033479, l6: 0.048765\n","\n","[epoch: 15739/100000, batch:    12/    1, ite: 15739] train loss: 0.118624, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001222, l3: 0.011635, l4: 0.020232, l5: 0.030425, l6: 0.052757\n","\n","[epoch: 15740/100000, batch:    12/    1, ite: 15740] train loss: 0.118623, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001495, l3: 0.011595, l4: 0.020370, l5: 0.031495, l6: 0.046759\n","\n","[epoch: 15741/100000, batch:    12/    1, ite: 15741] train loss: 0.118619, tar: 0.000079 \n","l0: 0.000062, l1: 0.000062, l2: 0.001131, l3: 0.011233, l4: 0.019036, l5: 0.030609, l6: 0.054141\n","\n","[epoch: 15742/100000, batch:    12/    1, ite: 15742] train loss: 0.118618, tar: 0.000079 \n","l0: 0.000059, l1: 0.000059, l2: 0.001343, l3: 0.011380, l4: 0.020129, l5: 0.032723, l6: 0.063728\n","\n","[epoch: 15743/100000, batch:    12/    1, ite: 15743] train loss: 0.118624, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001151, l3: 0.011522, l4: 0.019563, l5: 0.031740, l6: 0.047023\n","\n","[epoch: 15744/100000, batch:    12/    1, ite: 15744] train loss: 0.118620, tar: 0.000079 \n","l0: 0.000063, l1: 0.000071, l2: 0.001175, l3: 0.011865, l4: 0.020434, l5: 0.032416, l6: 0.062604\n","\n","[epoch: 15745/100000, batch:    12/    1, ite: 15745] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001338, l3: 0.011205, l4: 0.020093, l5: 0.032552, l6: 0.047187\n","\n","[epoch: 15746/100000, batch:    12/    1, ite: 15746] train loss: 0.118622, tar: 0.000079 \n","l0: 0.000075, l1: 0.000071, l2: 0.001494, l3: 0.011022, l4: 0.019818, l5: 0.032380, l6: 0.043707\n","\n","[epoch: 15747/100000, batch:    12/    1, ite: 15747] train loss: 0.118616, tar: 0.000079 \n","l0: 0.000069, l1: 0.000069, l2: 0.001554, l3: 0.011669, l4: 0.020170, l5: 0.033829, l6: 0.064915\n","\n","[epoch: 15748/100000, batch:    12/    1, ite: 15748] train loss: 0.118624, tar: 0.000079 \n","l0: 0.000060, l1: 0.000064, l2: 0.001285, l3: 0.011195, l4: 0.020296, l5: 0.032931, l6: 0.049290\n","\n","[epoch: 15749/100000, batch:    12/    1, ite: 15749] train loss: 0.118622, tar: 0.000079 \n","l0: 0.000076, l1: 0.000076, l2: 0.001175, l3: 0.011352, l4: 0.019872, l5: 0.029960, l6: 0.052074\n","\n","[epoch: 15750/100000, batch:    12/    1, ite: 15750] train loss: 0.118620, tar: 0.000079 \n","l0: 0.000067, l1: 0.000068, l2: 0.001299, l3: 0.011490, l4: 0.020240, l5: 0.034295, l6: 0.065288\n","\n","[epoch: 15751/100000, batch:    12/    1, ite: 15751] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000071, l1: 0.000076, l2: 0.001365, l3: 0.011250, l4: 0.020184, l5: 0.032311, l6: 0.047944\n","\n","[epoch: 15752/100000, batch:    12/    1, ite: 15752] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001182, l3: 0.011746, l4: 0.020352, l5: 0.034728, l6: 0.066749\n","\n","[epoch: 15753/100000, batch:    12/    1, ite: 15753] train loss: 0.118634, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001110, l3: 0.011123, l4: 0.020062, l5: 0.031763, l6: 0.045093\n","\n","[epoch: 15754/100000, batch:    12/    1, ite: 15754] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000066, l1: 0.000071, l2: 0.001122, l3: 0.011770, l4: 0.020241, l5: 0.032360, l6: 0.059434\n","\n","[epoch: 15755/100000, batch:    12/    1, ite: 15755] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000078, l1: 0.000078, l2: 0.001590, l3: 0.010879, l4: 0.019886, l5: 0.033101, l6: 0.064527\n","\n","[epoch: 15756/100000, batch:    12/    1, ite: 15756] train loss: 0.118639, tar: 0.000079 \n","l0: 0.000062, l1: 0.000063, l2: 0.001139, l3: 0.011281, l4: 0.019947, l5: 0.030961, l6: 0.059301\n","\n","[epoch: 15757/100000, batch:    12/    1, ite: 15757] train loss: 0.118641, tar: 0.000079 \n","l0: 0.000073, l1: 0.000073, l2: 0.001228, l3: 0.011316, l4: 0.020299, l5: 0.031321, l6: 0.050399\n","\n","[epoch: 15758/100000, batch:    12/    1, ite: 15758] train loss: 0.118639, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001443, l3: 0.011599, l4: 0.020304, l5: 0.031755, l6: 0.059405\n","\n","[epoch: 15759/100000, batch:    12/    1, ite: 15759] train loss: 0.118642, tar: 0.000079 \n","l0: 0.000071, l1: 0.000072, l2: 0.001131, l3: 0.011415, l4: 0.020815, l5: 0.032820, l6: 0.048597\n","\n","[epoch: 15760/100000, batch:    12/    1, ite: 15760] train loss: 0.118640, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001136, l3: 0.011346, l4: 0.020700, l5: 0.033152, l6: 0.048880\n","\n","[epoch: 15761/100000, batch:    12/    1, ite: 15761] train loss: 0.118638, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001095, l3: 0.011177, l4: 0.019523, l5: 0.032081, l6: 0.043774\n","\n","[epoch: 15762/100000, batch:    12/    1, ite: 15762] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001451, l3: 0.011678, l4: 0.020007, l5: 0.030380, l6: 0.052297\n","\n","[epoch: 15763/100000, batch:    12/    1, ite: 15763] train loss: 0.118631, tar: 0.000079 \n","l0: 0.000059, l1: 0.000063, l2: 0.001296, l3: 0.011209, l4: 0.020451, l5: 0.033725, l6: 0.048298\n","\n","[epoch: 15764/100000, batch:    12/    1, ite: 15764] train loss: 0.118629, tar: 0.000079 \n","l0: 0.000059, l1: 0.000063, l2: 0.001267, l3: 0.011089, l4: 0.020108, l5: 0.030045, l6: 0.048642\n","\n","[epoch: 15765/100000, batch:    12/    1, ite: 15765] train loss: 0.118624, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001082, l3: 0.011265, l4: 0.020216, l5: 0.031235, l6: 0.057617\n","\n","[epoch: 15766/100000, batch:    12/    1, ite: 15766] train loss: 0.118626, tar: 0.000079 \n","l0: 0.000075, l1: 0.000075, l2: 0.001096, l3: 0.011288, l4: 0.019853, l5: 0.029906, l6: 0.051831\n","\n","[epoch: 15767/100000, batch:    12/    1, ite: 15767] train loss: 0.118623, tar: 0.000079 \n","l0: 0.000067, l1: 0.000068, l2: 0.001147, l3: 0.011697, l4: 0.019835, l5: 0.030493, l6: 0.057472\n","\n","[epoch: 15768/100000, batch:    12/    1, ite: 15768] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000077, l1: 0.000077, l2: 0.001737, l3: 0.011132, l4: 0.020473, l5: 0.034170, l6: 0.066417\n","\n","[epoch: 15769/100000, batch:    12/    1, ite: 15769] train loss: 0.118633, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001510, l3: 0.011174, l4: 0.020078, l5: 0.032318, l6: 0.048852\n","\n","[epoch: 15770/100000, batch:    12/    1, ite: 15770] train loss: 0.118631, tar: 0.000079 \n","l0: 0.000076, l1: 0.000072, l2: 0.001441, l3: 0.011103, l4: 0.019855, l5: 0.029995, l6: 0.048003\n","\n","[epoch: 15771/100000, batch:    12/    1, ite: 15771] train loss: 0.118626, tar: 0.000079 \n","l0: 0.000072, l1: 0.000073, l2: 0.001489, l3: 0.010952, l4: 0.019856, l5: 0.032736, l6: 0.064388\n","\n","[epoch: 15772/100000, batch:    12/    1, ite: 15772] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001511, l3: 0.011677, l4: 0.020093, l5: 0.031641, l6: 0.057606\n","\n","[epoch: 15773/100000, batch:    12/    1, ite: 15773] train loss: 0.118635, tar: 0.000079 \n","l0: 0.000073, l1: 0.000077, l2: 0.001374, l3: 0.011181, l4: 0.020074, l5: 0.030482, l6: 0.051929\n","\n","[epoch: 15774/100000, batch:    12/    1, ite: 15774] train loss: 0.118633, tar: 0.000079 \n","l0: 0.000072, l1: 0.000071, l2: 0.001305, l3: 0.011629, l4: 0.020035, l5: 0.031412, l6: 0.044808\n","\n","[epoch: 15775/100000, batch:    12/    1, ite: 15775] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001524, l3: 0.011460, l4: 0.020127, l5: 0.030922, l6: 0.045775\n","\n","[epoch: 15776/100000, batch:    12/    1, ite: 15776] train loss: 0.118623, tar: 0.000079 \n","l0: 0.000067, l1: 0.000069, l2: 0.001603, l3: 0.011491, l4: 0.019769, l5: 0.033062, l6: 0.064960\n","\n","[epoch: 15777/100000, batch:    12/    1, ite: 15777] train loss: 0.118630, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001407, l3: 0.011205, l4: 0.019995, l5: 0.032107, l6: 0.045322\n","\n","[epoch: 15778/100000, batch:    12/    1, ite: 15778] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000053, l1: 0.000054, l2: 0.001249, l3: 0.011466, l4: 0.020238, l5: 0.031236, l6: 0.058904\n","\n","[epoch: 15779/100000, batch:    12/    1, ite: 15779] train loss: 0.118627, tar: 0.000079 \n","l0: 0.000075, l1: 0.000078, l2: 0.001522, l3: 0.011117, l4: 0.020542, l5: 0.034245, l6: 0.066095\n","\n","[epoch: 15780/100000, batch:    12/    1, ite: 15780] train loss: 0.118636, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001418, l3: 0.011737, l4: 0.020416, l5: 0.031857, l6: 0.049967\n","\n","[epoch: 15781/100000, batch:    12/    1, ite: 15781] train loss: 0.118634, tar: 0.000079 \n","l0: 0.000067, l1: 0.000066, l2: 0.001531, l3: 0.011318, l4: 0.020110, l5: 0.030511, l6: 0.051484\n","\n","[epoch: 15782/100000, batch:    12/    1, ite: 15782] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000063, l1: 0.000063, l2: 0.001161, l3: 0.011438, l4: 0.020389, l5: 0.032199, l6: 0.046970\n","\n","[epoch: 15783/100000, batch:    12/    1, ite: 15783] train loss: 0.118629, tar: 0.000079 \n","l0: 0.000060, l1: 0.000064, l2: 0.001319, l3: 0.011583, l4: 0.020209, l5: 0.033431, l6: 0.048269\n","\n","[epoch: 15784/100000, batch:    12/    1, ite: 15784] train loss: 0.118627, tar: 0.000079 \n","l0: 0.000071, l1: 0.000072, l2: 0.001162, l3: 0.011722, l4: 0.020364, l5: 0.033588, l6: 0.061635\n","\n","[epoch: 15785/100000, batch:    12/    1, ite: 15785] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001137, l3: 0.011350, l4: 0.019856, l5: 0.031936, l6: 0.052999\n","\n","[epoch: 15786/100000, batch:    12/    1, ite: 15786] train loss: 0.118631, tar: 0.000079 \n","l0: 0.000067, l1: 0.000068, l2: 0.001196, l3: 0.011578, l4: 0.019729, l5: 0.029446, l6: 0.049855\n","\n","[epoch: 15787/100000, batch:    12/    1, ite: 15787] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001482, l3: 0.011531, l4: 0.020146, l5: 0.031178, l6: 0.045151\n","\n","[epoch: 15788/100000, batch:    12/    1, ite: 15788] train loss: 0.118623, tar: 0.000079 \n","l0: 0.000068, l1: 0.000068, l2: 0.001150, l3: 0.011359, l4: 0.020754, l5: 0.032904, l6: 0.048206\n","\n","[epoch: 15789/100000, batch:    12/    1, ite: 15789] train loss: 0.118620, tar: 0.000079 \n","l0: 0.000068, l1: 0.000068, l2: 0.001553, l3: 0.011612, l4: 0.020200, l5: 0.034312, l6: 0.064047\n","\n","[epoch: 15790/100000, batch:    12/    1, ite: 15790] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000073, l1: 0.000072, l2: 0.001136, l3: 0.011477, l4: 0.020496, l5: 0.032687, l6: 0.048051\n","\n","[epoch: 15791/100000, batch:    12/    1, ite: 15791] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000067, l1: 0.000066, l2: 0.001459, l3: 0.011159, l4: 0.019609, l5: 0.032045, l6: 0.043499\n","\n","[epoch: 15792/100000, batch:    12/    1, ite: 15792] train loss: 0.118619, tar: 0.000079 \n","l0: 0.000069, l1: 0.000069, l2: 0.001586, l3: 0.011269, l4: 0.019760, l5: 0.033712, l6: 0.067468\n","\n","[epoch: 15793/100000, batch:    12/    1, ite: 15793] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000064, l1: 0.000064, l2: 0.001163, l3: 0.011604, l4: 0.020002, l5: 0.033541, l6: 0.048310\n","\n","[epoch: 15794/100000, batch:    12/    1, ite: 15794] train loss: 0.118626, tar: 0.000079 \n","l0: 0.000073, l1: 0.000073, l2: 0.001338, l3: 0.011519, l4: 0.020359, l5: 0.031370, l6: 0.047280\n","\n","[epoch: 15795/100000, batch:    12/    1, ite: 15795] train loss: 0.118622, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001073, l3: 0.011312, l4: 0.020378, l5: 0.032321, l6: 0.047595\n","\n","[epoch: 15796/100000, batch:    12/    1, ite: 15796] train loss: 0.118619, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001153, l3: 0.011295, l4: 0.020129, l5: 0.033298, l6: 0.048795\n","\n","[epoch: 15797/100000, batch:    12/    1, ite: 15797] train loss: 0.118617, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001124, l3: 0.011137, l4: 0.019772, l5: 0.033492, l6: 0.048777\n","\n","[epoch: 15798/100000, batch:    12/    1, ite: 15798] train loss: 0.118614, tar: 0.000079 \n","l0: 0.000071, l1: 0.000075, l2: 0.001106, l3: 0.011699, l4: 0.020355, l5: 0.032544, l6: 0.049312\n","\n","[epoch: 15799/100000, batch:    12/    1, ite: 15799] train loss: 0.118612, tar: 0.000079 \n","l0: 0.000068, l1: 0.000068, l2: 0.001569, l3: 0.011665, l4: 0.020169, l5: 0.033760, l6: 0.064545\n","\n","[epoch: 15800/100000, batch:    12/    1, ite: 15800] train loss: 0.118620, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001185, l3: 0.011265, l4: 0.019669, l5: 0.033577, l6: 0.065357\n","\n","[epoch: 15801/100000, batch:    12/    1, ite: 15801] train loss: 0.118627, tar: 0.000079 \n","l0: 0.000062, l1: 0.000062, l2: 0.001166, l3: 0.011673, l4: 0.020267, l5: 0.032019, l6: 0.062457\n","\n","[epoch: 15802/100000, batch:    12/    1, ite: 15802] train loss: 0.118632, tar: 0.000079 \n","l0: 0.000071, l1: 0.000076, l2: 0.001185, l3: 0.011290, l4: 0.019870, l5: 0.033018, l6: 0.046871\n","\n","[epoch: 15803/100000, batch:    12/    1, ite: 15803] train loss: 0.118628, tar: 0.000079 \n","l0: 0.000067, l1: 0.000068, l2: 0.001140, l3: 0.011220, l4: 0.020154, l5: 0.031489, l6: 0.050197\n","\n","[epoch: 15804/100000, batch:    12/    1, ite: 15804] train loss: 0.118626, tar: 0.000079 \n","l0: 0.000068, l1: 0.000067, l2: 0.001490, l3: 0.011229, l4: 0.020271, l5: 0.033271, l6: 0.049045\n","\n","[epoch: 15805/100000, batch:    12/    1, ite: 15805] train loss: 0.118624, tar: 0.000079 \n","l0: 0.000063, l1: 0.000067, l2: 0.001436, l3: 0.011672, l4: 0.020112, l5: 0.031954, l6: 0.054640\n","\n","[epoch: 15806/100000, batch:    12/    1, ite: 15806] train loss: 0.118625, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001176, l3: 0.011517, l4: 0.020198, l5: 0.031606, l6: 0.047383\n","\n","[epoch: 15807/100000, batch:    12/    1, ite: 15807] train loss: 0.118621, tar: 0.000079 \n","l0: 0.000068, l1: 0.000068, l2: 0.001122, l3: 0.011095, l4: 0.019559, l5: 0.031867, l6: 0.045179\n","\n","[epoch: 15808/100000, batch:    12/    1, ite: 15808] train loss: 0.118616, tar: 0.000079 \n","l0: 0.000068, l1: 0.000068, l2: 0.001148, l3: 0.011392, l4: 0.020692, l5: 0.032105, l6: 0.056165\n","\n","[epoch: 15809/100000, batch:    12/    1, ite: 15809] train loss: 0.118617, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001492, l3: 0.011505, l4: 0.019778, l5: 0.030752, l6: 0.050554\n","\n","[epoch: 15810/100000, batch:    12/    1, ite: 15810] train loss: 0.118615, tar: 0.000079 \n","l0: 0.000059, l1: 0.000063, l2: 0.001299, l3: 0.011167, l4: 0.020008, l5: 0.032666, l6: 0.047331\n","\n","[epoch: 15811/100000, batch:    12/    1, ite: 15811] train loss: 0.118612, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001459, l3: 0.011507, l4: 0.019558, l5: 0.032618, l6: 0.046070\n","\n","[epoch: 15812/100000, batch:    12/    1, ite: 15812] train loss: 0.118608, tar: 0.000079 \n","l0: 0.000063, l1: 0.000063, l2: 0.001128, l3: 0.011121, l4: 0.020027, l5: 0.032460, l6: 0.050839\n","\n","[epoch: 15813/100000, batch:    12/    1, ite: 15813] train loss: 0.118606, tar: 0.000079 \n","l0: 0.000062, l1: 0.000063, l2: 0.001128, l3: 0.011563, l4: 0.020210, l5: 0.030082, l6: 0.051493\n","\n","[epoch: 15814/100000, batch:    12/    1, ite: 15814] train loss: 0.118604, tar: 0.000079 \n","l0: 0.000077, l1: 0.000076, l2: 0.001142, l3: 0.011422, l4: 0.020565, l5: 0.033279, l6: 0.049329\n","\n","[epoch: 15815/100000, batch:    12/    1, ite: 15815] train loss: 0.118602, tar: 0.000079 \n","l0: 0.000063, l1: 0.000062, l2: 0.001127, l3: 0.011048, l4: 0.019942, l5: 0.032177, l6: 0.045808\n","\n","[epoch: 15816/100000, batch:    12/    1, ite: 15816] train loss: 0.118598, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001507, l3: 0.011480, l4: 0.019357, l5: 0.030046, l6: 0.048397\n","\n","[epoch: 15817/100000, batch:    12/    1, ite: 15817] train loss: 0.118594, tar: 0.000079 \n","l0: 0.000063, l1: 0.000064, l2: 0.001121, l3: 0.011342, l4: 0.019759, l5: 0.030736, l6: 0.042901\n","\n","[epoch: 15818/100000, batch:    12/    1, ite: 15818] train loss: 0.118587, tar: 0.000079 \n","l0: 0.000064, l1: 0.000064, l2: 0.001285, l3: 0.011279, l4: 0.020439, l5: 0.033436, l6: 0.048960\n","\n","[epoch: 15819/100000, batch:    12/    1, ite: 15819] train loss: 0.118585, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001513, l3: 0.011121, l4: 0.019758, l5: 0.032456, l6: 0.045086\n","\n","[epoch: 15820/100000, batch:    12/    1, ite: 15820] train loss: 0.118580, tar: 0.000079 \n","l0: 0.000060, l1: 0.000063, l2: 0.001270, l3: 0.011119, l4: 0.019923, l5: 0.032432, l6: 0.053792\n","\n","[epoch: 15821/100000, batch:    12/    1, ite: 15821] train loss: 0.118580, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001437, l3: 0.011657, l4: 0.019699, l5: 0.033640, l6: 0.048826\n","\n","[epoch: 15822/100000, batch:    12/    1, ite: 15822] train loss: 0.118579, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001304, l3: 0.011141, l4: 0.020154, l5: 0.033249, l6: 0.049538\n","\n","[epoch: 15823/100000, batch:    12/    1, ite: 15823] train loss: 0.118577, tar: 0.000079 \n","l0: 0.000064, l1: 0.000064, l2: 0.001134, l3: 0.011309, l4: 0.020009, l5: 0.032795, l6: 0.048459\n","\n","[epoch: 15824/100000, batch:    12/    1, ite: 15824] train loss: 0.118574, tar: 0.000079 \n","l0: 0.000064, l1: 0.000069, l2: 0.001279, l3: 0.011486, l4: 0.019934, l5: 0.033018, l6: 0.064652\n","\n","[epoch: 15825/100000, batch:    12/    1, ite: 15825] train loss: 0.118581, tar: 0.000079 \n","l0: 0.000058, l1: 0.000059, l2: 0.001294, l3: 0.011631, l4: 0.020140, l5: 0.031127, l6: 0.047822\n","\n","[epoch: 15826/100000, batch:    12/    1, ite: 15826] train loss: 0.118577, tar: 0.000079 \n","l0: 0.000061, l1: 0.000061, l2: 0.001079, l3: 0.011158, l4: 0.018898, l5: 0.030090, l6: 0.046285\n","\n","[epoch: 15827/100000, batch:    12/    1, ite: 15827] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001310, l3: 0.011102, l4: 0.020240, l5: 0.032035, l6: 0.047931\n","\n","[epoch: 15828/100000, batch:    12/    1, ite: 15828] train loss: 0.118568, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001448, l3: 0.011849, l4: 0.020390, l5: 0.032592, l6: 0.057129\n","\n","[epoch: 15829/100000, batch:    12/    1, ite: 15829] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000059, l1: 0.000063, l2: 0.001263, l3: 0.011232, l4: 0.020119, l5: 0.032909, l6: 0.051257\n","\n","[epoch: 15830/100000, batch:    12/    1, ite: 15830] train loss: 0.118570, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001334, l3: 0.011371, l4: 0.020550, l5: 0.031795, l6: 0.054888\n","\n","[epoch: 15831/100000, batch:    12/    1, ite: 15831] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000058, l1: 0.000058, l2: 0.001270, l3: 0.011792, l4: 0.020332, l5: 0.031741, l6: 0.061462\n","\n","[epoch: 15832/100000, batch:    12/    1, ite: 15832] train loss: 0.118575, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001420, l3: 0.011834, l4: 0.020536, l5: 0.031797, l6: 0.061510\n","\n","[epoch: 15833/100000, batch:    12/    1, ite: 15833] train loss: 0.118580, tar: 0.000079 \n","l0: 0.000061, l1: 0.000064, l2: 0.001390, l3: 0.011313, l4: 0.020444, l5: 0.031513, l6: 0.053632\n","\n","[epoch: 15834/100000, batch:    12/    1, ite: 15834] train loss: 0.118580, tar: 0.000079 \n","l0: 0.000055, l1: 0.000063, l2: 0.001272, l3: 0.011085, l4: 0.020002, l5: 0.029369, l6: 0.049469\n","\n","[epoch: 15835/100000, batch:    12/    1, ite: 15835] train loss: 0.118576, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001141, l3: 0.011484, l4: 0.020454, l5: 0.032865, l6: 0.050532\n","\n","[epoch: 15836/100000, batch:    12/    1, ite: 15836] train loss: 0.118575, tar: 0.000079 \n","l0: 0.000058, l1: 0.000058, l2: 0.001229, l3: 0.011642, l4: 0.020158, l5: 0.031301, l6: 0.046527\n","\n","[epoch: 15837/100000, batch:    12/    1, ite: 15837] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001439, l3: 0.011633, l4: 0.020446, l5: 0.030986, l6: 0.049389\n","\n","[epoch: 15838/100000, batch:    12/    1, ite: 15838] train loss: 0.118568, tar: 0.000079 \n","l0: 0.000058, l1: 0.000062, l2: 0.001267, l3: 0.011156, l4: 0.019565, l5: 0.030255, l6: 0.050741\n","\n","[epoch: 15839/100000, batch:    12/    1, ite: 15839] train loss: 0.118565, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001514, l3: 0.011665, l4: 0.019745, l5: 0.031262, l6: 0.060537\n","\n","[epoch: 15840/100000, batch:    12/    1, ite: 15840] train loss: 0.118569, tar: 0.000079 \n","l0: 0.000063, l1: 0.000063, l2: 0.001152, l3: 0.011707, l4: 0.020271, l5: 0.032496, l6: 0.059384\n","\n","[epoch: 15841/100000, batch:    12/    1, ite: 15841] train loss: 0.118572, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001142, l3: 0.011572, l4: 0.019650, l5: 0.029566, l6: 0.050559\n","\n","[epoch: 15842/100000, batch:    12/    1, ite: 15842] train loss: 0.118569, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001417, l3: 0.011186, l4: 0.019991, l5: 0.032305, l6: 0.049312\n","\n","[epoch: 15843/100000, batch:    12/    1, ite: 15843] train loss: 0.118567, tar: 0.000079 \n","l0: 0.000072, l1: 0.000072, l2: 0.001568, l3: 0.011492, l4: 0.020598, l5: 0.033877, l6: 0.063928\n","\n","[epoch: 15844/100000, batch:    12/    1, ite: 15844] train loss: 0.118574, tar: 0.000079 \n","l0: 0.000070, l1: 0.000071, l2: 0.001169, l3: 0.011643, l4: 0.019779, l5: 0.029729, l6: 0.052756\n","\n","[epoch: 15845/100000, batch:    12/    1, ite: 15845] train loss: 0.118572, tar: 0.000079 \n","l0: 0.000060, l1: 0.000065, l2: 0.001557, l3: 0.011350, l4: 0.019378, l5: 0.032473, l6: 0.064001\n","\n","[epoch: 15846/100000, batch:    12/    1, ite: 15846] train loss: 0.118578, tar: 0.000079 \n","l0: 0.000070, l1: 0.000070, l2: 0.001486, l3: 0.011809, l4: 0.020803, l5: 0.032288, l6: 0.049038\n","\n","[epoch: 15847/100000, batch:    12/    1, ite: 15847] train loss: 0.118576, tar: 0.000079 \n","l0: 0.000064, l1: 0.000064, l2: 0.001235, l3: 0.011338, l4: 0.020373, l5: 0.033713, l6: 0.062483\n","\n","[epoch: 15848/100000, batch:    12/    1, ite: 15848] train loss: 0.118582, tar: 0.000079 \n","l0: 0.000054, l1: 0.000058, l2: 0.001291, l3: 0.011558, l4: 0.020065, l5: 0.031363, l6: 0.044864\n","\n","[epoch: 15849/100000, batch:    12/    1, ite: 15849] train loss: 0.118577, tar: 0.000079 \n","l0: 0.000073, l1: 0.000073, l2: 0.001452, l3: 0.011544, l4: 0.020044, l5: 0.032371, l6: 0.053854\n","\n","[epoch: 15850/100000, batch:    12/    1, ite: 15850] train loss: 0.118577, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001141, l3: 0.011664, l4: 0.020568, l5: 0.032595, l6: 0.049096\n","\n","[epoch: 15851/100000, batch:    12/    1, ite: 15851] train loss: 0.118575, tar: 0.000079 \n","l0: 0.000059, l1: 0.000058, l2: 0.001243, l3: 0.011583, l4: 0.019539, l5: 0.028913, l6: 0.049603\n","\n","[epoch: 15852/100000, batch:    12/    1, ite: 15852] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001237, l3: 0.011256, l4: 0.019675, l5: 0.032964, l6: 0.064706\n","\n","[epoch: 15853/100000, batch:    12/    1, ite: 15853] train loss: 0.118577, tar: 0.000079 \n","l0: 0.000058, l1: 0.000063, l2: 0.001258, l3: 0.011404, l4: 0.019234, l5: 0.029167, l6: 0.048520\n","\n","[epoch: 15854/100000, batch:    12/    1, ite: 15854] train loss: 0.118573, tar: 0.000079 \n","l0: 0.000062, l1: 0.000062, l2: 0.001247, l3: 0.011127, l4: 0.020016, l5: 0.032767, l6: 0.046457\n","\n","[epoch: 15855/100000, batch:    12/    1, ite: 15855] train loss: 0.118569, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001423, l3: 0.011181, l4: 0.020354, l5: 0.031732, l6: 0.054398\n","\n","[epoch: 15856/100000, batch:    12/    1, ite: 15856] train loss: 0.118569, tar: 0.000079 \n","l0: 0.000064, l1: 0.000064, l2: 0.001161, l3: 0.011535, l4: 0.020556, l5: 0.032479, l6: 0.052056\n","\n","[epoch: 15857/100000, batch:    12/    1, ite: 15857] train loss: 0.118569, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001442, l3: 0.011666, l4: 0.019886, l5: 0.029968, l6: 0.052716\n","\n","[epoch: 15858/100000, batch:    12/    1, ite: 15858] train loss: 0.118567, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001490, l3: 0.011648, l4: 0.020741, l5: 0.032711, l6: 0.057809\n","\n","[epoch: 15859/100000, batch:    12/    1, ite: 15859] train loss: 0.118571, tar: 0.000079 \n","l0: 0.000058, l1: 0.000059, l2: 0.001271, l3: 0.011577, l4: 0.020727, l5: 0.033015, l6: 0.059239\n","\n","[epoch: 15860/100000, batch:    12/    1, ite: 15860] train loss: 0.118575, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001210, l3: 0.011465, l4: 0.020010, l5: 0.032078, l6: 0.053240\n","\n","[epoch: 15861/100000, batch:    12/    1, ite: 15861] train loss: 0.118574, tar: 0.000079 \n","l0: 0.000063, l1: 0.000063, l2: 0.001133, l3: 0.011523, l4: 0.020634, l5: 0.032047, l6: 0.049651\n","\n","[epoch: 15862/100000, batch:    12/    1, ite: 15862] train loss: 0.118572, tar: 0.000079 \n","l0: 0.000067, l1: 0.000071, l2: 0.001163, l3: 0.011175, l4: 0.019641, l5: 0.032002, l6: 0.052871\n","\n","[epoch: 15863/100000, batch:    12/    1, ite: 15863] train loss: 0.118572, tar: 0.000079 \n","l0: 0.000075, l1: 0.000071, l2: 0.001455, l3: 0.010919, l4: 0.019703, l5: 0.030062, l6: 0.045314\n","\n","[epoch: 15864/100000, batch:    12/    1, ite: 15864] train loss: 0.118566, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001474, l3: 0.011002, l4: 0.019520, l5: 0.030466, l6: 0.049997\n","\n","[epoch: 15865/100000, batch:    12/    1, ite: 15865] train loss: 0.118563, tar: 0.000079 \n","l0: 0.000063, l1: 0.000064, l2: 0.001116, l3: 0.011121, l4: 0.019163, l5: 0.030234, l6: 0.048009\n","\n","[epoch: 15866/100000, batch:    12/    1, ite: 15866] train loss: 0.118558, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001478, l3: 0.011571, l4: 0.020699, l5: 0.032777, l6: 0.057777\n","\n","[epoch: 15867/100000, batch:    12/    1, ite: 15867] train loss: 0.118561, tar: 0.000079 \n","l0: 0.000061, l1: 0.000061, l2: 0.001064, l3: 0.010870, l4: 0.019795, l5: 0.031685, l6: 0.043232\n","\n","[epoch: 15868/100000, batch:    12/    1, ite: 15868] train loss: 0.118555, tar: 0.000079 \n","l0: 0.000059, l1: 0.000063, l2: 0.001273, l3: 0.011230, l4: 0.020348, l5: 0.031484, l6: 0.052489\n","\n","[epoch: 15869/100000, batch:    12/    1, ite: 15869] train loss: 0.118554, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001478, l3: 0.011033, l4: 0.019673, l5: 0.029875, l6: 0.049429\n","\n","[epoch: 15870/100000, batch:    12/    1, ite: 15870] train loss: 0.118550, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001486, l3: 0.011537, l4: 0.020230, l5: 0.031497, l6: 0.061227\n","\n","[epoch: 15871/100000, batch:    12/    1, ite: 15871] train loss: 0.118554, tar: 0.000079 \n","l0: 0.000073, l1: 0.000072, l2: 0.001468, l3: 0.011087, l4: 0.019793, l5: 0.030037, l6: 0.051885\n","\n","[epoch: 15872/100000, batch:    12/    1, ite: 15872] train loss: 0.118552, tar: 0.000079 \n","l0: 0.000063, l1: 0.000063, l2: 0.001122, l3: 0.011586, l4: 0.020537, l5: 0.032226, l6: 0.048588\n","\n","[epoch: 15873/100000, batch:    12/    1, ite: 15873] train loss: 0.118550, tar: 0.000079 \n","l0: 0.000073, l1: 0.000073, l2: 0.001372, l3: 0.010940, l4: 0.020012, l5: 0.032949, l6: 0.066080\n","\n","[epoch: 15874/100000, batch:    12/    1, ite: 15874] train loss: 0.118556, tar: 0.000079 \n","l0: 0.000059, l1: 0.000059, l2: 0.001265, l3: 0.011169, l4: 0.020363, l5: 0.033572, l6: 0.049232\n","\n","[epoch: 15875/100000, batch:    12/    1, ite: 15875] train loss: 0.118555, tar: 0.000079 \n","l0: 0.000067, l1: 0.000067, l2: 0.001516, l3: 0.011881, l4: 0.020023, l5: 0.031857, l6: 0.051358\n","\n","[epoch: 15876/100000, batch:    12/    1, ite: 15876] train loss: 0.118554, tar: 0.000079 \n","l0: 0.000058, l1: 0.000058, l2: 0.001258, l3: 0.011455, l4: 0.019674, l5: 0.031990, l6: 0.044520\n","\n","[epoch: 15877/100000, batch:    12/    1, ite: 15877] train loss: 0.118549, tar: 0.000079 \n","l0: 0.000066, l1: 0.000067, l2: 0.001517, l3: 0.011395, l4: 0.019758, l5: 0.031205, l6: 0.051349\n","\n","[epoch: 15878/100000, batch:    12/    1, ite: 15878] train loss: 0.118547, tar: 0.000079 \n","l0: 0.000066, l1: 0.000066, l2: 0.001452, l3: 0.011038, l4: 0.019822, l5: 0.031467, l6: 0.045222\n","\n","[epoch: 15879/100000, batch:    12/    1, ite: 15879] train loss: 0.118542, tar: 0.000079 \n","l0: 0.000071, l1: 0.000071, l2: 0.001378, l3: 0.011536, l4: 0.020136, l5: 0.033097, l6: 0.048400\n","\n","[epoch: 15880/100000, batch:    12/    1, ite: 15880] train loss: 0.118540, tar: 0.000078 \n","l0: 0.000068, l1: 0.000072, l2: 0.001408, l3: 0.011111, l4: 0.020108, l5: 0.030015, l6: 0.049416\n","\n","[epoch: 15881/100000, batch:    12/    1, ite: 15881] train loss: 0.118537, tar: 0.000078 \n","l0: 0.000072, l1: 0.000071, l2: 0.001237, l3: 0.011313, l4: 0.019948, l5: 0.033464, l6: 0.063181\n","\n","[epoch: 15882/100000, batch:    12/    1, ite: 15882] train loss: 0.118543, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001424, l3: 0.011474, l4: 0.019125, l5: 0.030427, l6: 0.047872\n","\n","[epoch: 15883/100000, batch:    12/    1, ite: 15883] train loss: 0.118538, tar: 0.000078 \n","l0: 0.000071, l1: 0.000070, l2: 0.001249, l3: 0.011225, l4: 0.020512, l5: 0.030914, l6: 0.052543\n","\n","[epoch: 15884/100000, batch:    12/    1, ite: 15884] train loss: 0.118537, tar: 0.000078 \n","l0: 0.000064, l1: 0.000063, l2: 0.001179, l3: 0.011260, l4: 0.020701, l5: 0.031817, l6: 0.055482\n","\n","[epoch: 15885/100000, batch:    12/    1, ite: 15885] train loss: 0.118538, tar: 0.000078 \n","l0: 0.000066, l1: 0.000067, l2: 0.001368, l3: 0.010982, l4: 0.020250, l5: 0.034172, l6: 0.067204\n","\n","[epoch: 15886/100000, batch:    12/    1, ite: 15886] train loss: 0.118547, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001361, l3: 0.010983, l4: 0.019253, l5: 0.030766, l6: 0.048494\n","\n","[epoch: 15887/100000, batch:    12/    1, ite: 15887] train loss: 0.118543, tar: 0.000078 \n","l0: 0.000071, l1: 0.000067, l2: 0.001472, l3: 0.011651, l4: 0.020059, l5: 0.033735, l6: 0.048766\n","\n","[epoch: 15888/100000, batch:    12/    1, ite: 15888] train loss: 0.118541, tar: 0.000078 \n","l0: 0.000068, l1: 0.000069, l2: 0.001274, l3: 0.011006, l4: 0.020411, l5: 0.034532, l6: 0.067675\n","\n","[epoch: 15889/100000, batch:    12/    1, ite: 15889] train loss: 0.118550, tar: 0.000078 \n","l0: 0.000072, l1: 0.000073, l2: 0.001426, l3: 0.011330, l4: 0.019492, l5: 0.032748, l6: 0.064403\n","\n","[epoch: 15890/100000, batch:    12/    1, ite: 15890] train loss: 0.118556, tar: 0.000078 \n","l0: 0.000061, l1: 0.000062, l2: 0.001574, l3: 0.011062, l4: 0.019762, l5: 0.033416, l6: 0.067368\n","\n","[epoch: 15891/100000, batch:    12/    1, ite: 15891] train loss: 0.118563, tar: 0.000078 \n","l0: 0.000059, l1: 0.000060, l2: 0.001461, l3: 0.011140, l4: 0.019874, l5: 0.033431, l6: 0.067366\n","\n","[epoch: 15892/100000, batch:    12/    1, ite: 15892] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001440, l3: 0.011040, l4: 0.019753, l5: 0.032510, l6: 0.045593\n","\n","[epoch: 15893/100000, batch:    12/    1, ite: 15893] train loss: 0.118567, tar: 0.000078 \n","l0: 0.000072, l1: 0.000072, l2: 0.001342, l3: 0.011907, l4: 0.020658, l5: 0.033380, l6: 0.059409\n","\n","[epoch: 15894/100000, batch:    12/    1, ite: 15894] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001133, l3: 0.011565, l4: 0.019616, l5: 0.029789, l6: 0.049308\n","\n","[epoch: 15895/100000, batch:    12/    1, ite: 15895] train loss: 0.118568, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001506, l3: 0.011321, l4: 0.020386, l5: 0.033016, l6: 0.047980\n","\n","[epoch: 15896/100000, batch:    12/    1, ite: 15896] train loss: 0.118565, tar: 0.000078 \n","l0: 0.000067, l1: 0.000069, l2: 0.001150, l3: 0.011434, l4: 0.019501, l5: 0.030051, l6: 0.047052\n","\n","[epoch: 15897/100000, batch:    12/    1, ite: 15897] train loss: 0.118561, tar: 0.000078 \n","l0: 0.000059, l1: 0.000059, l2: 0.001408, l3: 0.011275, l4: 0.020232, l5: 0.034298, l6: 0.063337\n","\n","[epoch: 15898/100000, batch:    12/    1, ite: 15898] train loss: 0.118567, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001245, l3: 0.011284, l4: 0.020157, l5: 0.033670, l6: 0.062795\n","\n","[epoch: 15899/100000, batch:    12/    1, ite: 15899] train loss: 0.118573, tar: 0.000078 \n","l0: 0.000070, l1: 0.000070, l2: 0.001138, l3: 0.011178, l4: 0.019060, l5: 0.030439, l6: 0.047854\n","\n","[epoch: 15900/100000, batch:    12/    1, ite: 15900] train loss: 0.118568, tar: 0.000078 \n","l0: 0.000063, l1: 0.000067, l2: 0.001173, l3: 0.011361, l4: 0.020694, l5: 0.033052, l6: 0.049026\n","\n","[epoch: 15901/100000, batch:    12/    1, ite: 15901] train loss: 0.118566, tar: 0.000078 \n","l0: 0.000070, l1: 0.000070, l2: 0.001377, l3: 0.011535, l4: 0.019176, l5: 0.031766, l6: 0.042693\n","\n","[epoch: 15902/100000, batch:    12/    1, ite: 15902] train loss: 0.118560, tar: 0.000078 \n","l0: 0.000058, l1: 0.000058, l2: 0.001284, l3: 0.011470, l4: 0.019259, l5: 0.030720, l6: 0.053737\n","\n","[epoch: 15903/100000, batch:    12/    1, ite: 15903] train loss: 0.118559, tar: 0.000078 \n","l0: 0.000062, l1: 0.000063, l2: 0.001146, l3: 0.011483, l4: 0.019432, l5: 0.032392, l6: 0.045196\n","\n","[epoch: 15904/100000, batch:    12/    1, ite: 15904] train loss: 0.118554, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001474, l3: 0.011510, l4: 0.019313, l5: 0.030735, l6: 0.052361\n","\n","[epoch: 15905/100000, batch:    12/    1, ite: 15905] train loss: 0.118553, tar: 0.000078 \n","l0: 0.000072, l1: 0.000072, l2: 0.001422, l3: 0.011475, l4: 0.019985, l5: 0.034270, l6: 0.064345\n","\n","[epoch: 15906/100000, batch:    12/    1, ite: 15906] train loss: 0.118560, tar: 0.000078 \n","l0: 0.000060, l1: 0.000060, l2: 0.001411, l3: 0.011434, l4: 0.019580, l5: 0.032752, l6: 0.064626\n","\n","[epoch: 15907/100000, batch:    12/    1, ite: 15907] train loss: 0.118566, tar: 0.000078 \n","l0: 0.000064, l1: 0.000064, l2: 0.001152, l3: 0.011238, l4: 0.020390, l5: 0.031065, l6: 0.055485\n","\n","[epoch: 15908/100000, batch:    12/    1, ite: 15908] train loss: 0.118566, tar: 0.000078 \n","l0: 0.000068, l1: 0.000069, l2: 0.001237, l3: 0.010867, l4: 0.019849, l5: 0.033007, l6: 0.066375\n","\n","[epoch: 15909/100000, batch:    12/    1, ite: 15909] train loss: 0.118573, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001371, l3: 0.011781, l4: 0.020612, l5: 0.031900, l6: 0.049277\n","\n","[epoch: 15910/100000, batch:    12/    1, ite: 15910] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001270, l3: 0.011255, l4: 0.020051, l5: 0.031179, l6: 0.058765\n","\n","[epoch: 15911/100000, batch:    12/    1, ite: 15911] train loss: 0.118573, tar: 0.000078 \n","l0: 0.000064, l1: 0.000067, l2: 0.001353, l3: 0.011265, l4: 0.020234, l5: 0.033513, l6: 0.048727\n","\n","[epoch: 15912/100000, batch:    12/    1, ite: 15912] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000067, l1: 0.000068, l2: 0.001313, l3: 0.011637, l4: 0.019752, l5: 0.029534, l6: 0.050289\n","\n","[epoch: 15913/100000, batch:    12/    1, ite: 15913] train loss: 0.118568, tar: 0.000078 \n","l0: 0.000073, l1: 0.000072, l2: 0.001322, l3: 0.011507, l4: 0.019717, l5: 0.029482, l6: 0.051078\n","\n","[epoch: 15914/100000, batch:    12/    1, ite: 15914] train loss: 0.118566, tar: 0.000078 \n","l0: 0.000064, l1: 0.000068, l2: 0.001176, l3: 0.011586, l4: 0.019997, l5: 0.033502, l6: 0.048253\n","\n","[epoch: 15915/100000, batch:    12/    1, ite: 15915] train loss: 0.118564, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001264, l3: 0.011397, l4: 0.020476, l5: 0.034566, l6: 0.064788\n","\n","[epoch: 15916/100000, batch:    12/    1, ite: 15916] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001179, l3: 0.011622, l4: 0.020315, l5: 0.031407, l6: 0.054151\n","\n","[epoch: 15917/100000, batch:    12/    1, ite: 15917] train loss: 0.118571, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001556, l3: 0.011339, l4: 0.020601, l5: 0.034534, l6: 0.062727\n","\n","[epoch: 15918/100000, batch:    12/    1, ite: 15918] train loss: 0.118577, tar: 0.000078 \n","l0: 0.000068, l1: 0.000067, l2: 0.001136, l3: 0.011583, l4: 0.020049, l5: 0.031158, l6: 0.045839\n","\n","[epoch: 15919/100000, batch:    12/    1, ite: 15919] train loss: 0.118573, tar: 0.000078 \n","l0: 0.000080, l1: 0.000079, l2: 0.001306, l3: 0.011106, l4: 0.020569, l5: 0.034454, l6: 0.067830\n","\n","[epoch: 15920/100000, batch:    12/    1, ite: 15920] train loss: 0.118582, tar: 0.000078 \n","l0: 0.000059, l1: 0.000063, l2: 0.001304, l3: 0.011050, l4: 0.020372, l5: 0.030999, l6: 0.051138\n","\n","[epoch: 15921/100000, batch:    12/    1, ite: 15921] train loss: 0.118580, tar: 0.000078 \n","l0: 0.000066, l1: 0.000067, l2: 0.001138, l3: 0.011496, l4: 0.020200, l5: 0.030042, l6: 0.051735\n","\n","[epoch: 15922/100000, batch:    12/    1, ite: 15922] train loss: 0.118578, tar: 0.000078 \n","l0: 0.000068, l1: 0.000072, l2: 0.001253, l3: 0.011330, l4: 0.020644, l5: 0.032291, l6: 0.047792\n","\n","[epoch: 15923/100000, batch:    12/    1, ite: 15923] train loss: 0.118575, tar: 0.000078 \n","l0: 0.000074, l1: 0.000080, l2: 0.001419, l3: 0.011566, l4: 0.020378, l5: 0.031003, l6: 0.053535\n","\n","[epoch: 15924/100000, batch:    12/    1, ite: 15924] train loss: 0.118575, tar: 0.000078 \n","l0: 0.000065, l1: 0.000065, l2: 0.001579, l3: 0.011615, l4: 0.020377, l5: 0.034759, l6: 0.065506\n","\n","[epoch: 15925/100000, batch:    12/    1, ite: 15925] train loss: 0.118583, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001144, l3: 0.011466, l4: 0.019470, l5: 0.032356, l6: 0.045996\n","\n","[epoch: 15926/100000, batch:    12/    1, ite: 15926] train loss: 0.118579, tar: 0.000078 \n","l0: 0.000067, l1: 0.000068, l2: 0.001437, l3: 0.011293, l4: 0.020439, l5: 0.030262, l6: 0.050324\n","\n","[epoch: 15927/100000, batch:    12/    1, ite: 15927] train loss: 0.118576, tar: 0.000078 \n","l0: 0.000071, l1: 0.000072, l2: 0.001349, l3: 0.011320, l4: 0.020333, l5: 0.033721, l6: 0.048511\n","\n","[epoch: 15928/100000, batch:    12/    1, ite: 15928] train loss: 0.118575, tar: 0.000078 \n","l0: 0.000067, l1: 0.000068, l2: 0.001462, l3: 0.011141, l4: 0.020112, l5: 0.030155, l6: 0.054334\n","\n","[epoch: 15929/100000, batch:    12/    1, ite: 15929] train loss: 0.118574, tar: 0.000078 \n","l0: 0.000059, l1: 0.000063, l2: 0.001345, l3: 0.011104, l4: 0.019744, l5: 0.032183, l6: 0.062592\n","\n","[epoch: 15930/100000, batch:    12/    1, ite: 15930] train loss: 0.118578, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001304, l3: 0.011570, l4: 0.019806, l5: 0.030811, l6: 0.051449\n","\n","[epoch: 15931/100000, batch:    12/    1, ite: 15931] train loss: 0.118577, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001448, l3: 0.011167, l4: 0.020356, l5: 0.032461, l6: 0.051774\n","\n","[epoch: 15932/100000, batch:    12/    1, ite: 15932] train loss: 0.118576, tar: 0.000078 \n","l0: 0.000074, l1: 0.000075, l2: 0.001382, l3: 0.010880, l4: 0.020266, l5: 0.033607, l6: 0.067196\n","\n","[epoch: 15933/100000, batch:    12/    1, ite: 15933] train loss: 0.118584, tar: 0.000078 \n","l0: 0.000058, l1: 0.000059, l2: 0.001260, l3: 0.011546, l4: 0.019538, l5: 0.031680, l6: 0.052384\n","\n","[epoch: 15934/100000, batch:    12/    1, ite: 15934] train loss: 0.118583, tar: 0.000078 \n","l0: 0.000072, l1: 0.000071, l2: 0.001332, l3: 0.011805, l4: 0.020607, l5: 0.033373, l6: 0.059423\n","\n","[epoch: 15935/100000, batch:    12/    1, ite: 15935] train loss: 0.118587, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001184, l3: 0.011590, l4: 0.020434, l5: 0.033148, l6: 0.049516\n","\n","[epoch: 15936/100000, batch:    12/    1, ite: 15936] train loss: 0.118585, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001173, l3: 0.011392, l4: 0.019971, l5: 0.034351, l6: 0.064982\n","\n","[epoch: 15937/100000, batch:    12/    1, ite: 15937] train loss: 0.118592, tar: 0.000078 \n","l0: 0.000060, l1: 0.000060, l2: 0.001568, l3: 0.011013, l4: 0.019717, l5: 0.033348, l6: 0.067453\n","\n","[epoch: 15938/100000, batch:    12/    1, ite: 15938] train loss: 0.118600, tar: 0.000078 \n","l0: 0.000061, l1: 0.000066, l2: 0.001425, l3: 0.011143, l4: 0.020153, l5: 0.033359, l6: 0.066078\n","\n","[epoch: 15939/100000, batch:    12/    1, ite: 15939] train loss: 0.118607, tar: 0.000078 \n","l0: 0.000071, l1: 0.000072, l2: 0.001323, l3: 0.011106, l4: 0.020114, l5: 0.030097, l6: 0.049184\n","\n","[epoch: 15940/100000, batch:    12/    1, ite: 15940] train loss: 0.118604, tar: 0.000078 \n","l0: 0.000058, l1: 0.000059, l2: 0.001097, l3: 0.010898, l4: 0.018996, l5: 0.030534, l6: 0.048269\n","\n","[epoch: 15941/100000, batch:    12/    1, ite: 15941] train loss: 0.118599, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001333, l3: 0.011392, l4: 0.019030, l5: 0.030093, l6: 0.047625\n","\n","[epoch: 15942/100000, batch:    12/    1, ite: 15942] train loss: 0.118594, tar: 0.000078 \n","l0: 0.000065, l1: 0.000068, l2: 0.001341, l3: 0.010764, l4: 0.019760, l5: 0.032711, l6: 0.066403\n","\n","[epoch: 15943/100000, batch:    12/    1, ite: 15943] train loss: 0.118601, tar: 0.000078 \n","l0: 0.000063, l1: 0.000067, l2: 0.001172, l3: 0.011312, l4: 0.020073, l5: 0.033763, l6: 0.049439\n","\n","[epoch: 15944/100000, batch:    12/    1, ite: 15944] train loss: 0.118599, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001583, l3: 0.011501, l4: 0.019727, l5: 0.033134, l6: 0.064999\n","\n","[epoch: 15945/100000, batch:    12/    1, ite: 15945] train loss: 0.118606, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001137, l3: 0.011463, l4: 0.020558, l5: 0.032027, l6: 0.055941\n","\n","[epoch: 15946/100000, batch:    12/    1, ite: 15946] train loss: 0.118607, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001165, l3: 0.011629, l4: 0.020405, l5: 0.033084, l6: 0.050470\n","\n","[epoch: 15947/100000, batch:    12/    1, ite: 15947] train loss: 0.118606, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001335, l3: 0.011001, l4: 0.019571, l5: 0.032150, l6: 0.061553\n","\n","[epoch: 15948/100000, batch:    12/    1, ite: 15948] train loss: 0.118610, tar: 0.000078 \n","l0: 0.000071, l1: 0.000072, l2: 0.001316, l3: 0.011374, l4: 0.019933, l5: 0.031876, l6: 0.045926\n","\n","[epoch: 15949/100000, batch:    12/    1, ite: 15949] train loss: 0.118606, tar: 0.000078 \n","l0: 0.000059, l1: 0.000062, l2: 0.001314, l3: 0.011271, l4: 0.019835, l5: 0.030865, l6: 0.051098\n","\n","[epoch: 15950/100000, batch:    12/    1, ite: 15950] train loss: 0.118604, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001121, l3: 0.010996, l4: 0.019722, l5: 0.032295, l6: 0.046419\n","\n","[epoch: 15951/100000, batch:    12/    1, ite: 15951] train loss: 0.118600, tar: 0.000078 \n","l0: 0.000068, l1: 0.000069, l2: 0.001132, l3: 0.011604, l4: 0.020011, l5: 0.033778, l6: 0.049053\n","\n","[epoch: 15952/100000, batch:    12/    1, ite: 15952] train loss: 0.118598, tar: 0.000078 \n","l0: 0.000059, l1: 0.000059, l2: 0.001279, l3: 0.011435, l4: 0.019248, l5: 0.030811, l6: 0.053508\n","\n","[epoch: 15953/100000, batch:    12/    1, ite: 15953] train loss: 0.118597, tar: 0.000078 \n","l0: 0.000063, l1: 0.000072, l2: 0.001257, l3: 0.011128, l4: 0.019121, l5: 0.030454, l6: 0.047593\n","\n","[epoch: 15954/100000, batch:    12/    1, ite: 15954] train loss: 0.118593, tar: 0.000078 \n","l0: 0.000072, l1: 0.000076, l2: 0.001225, l3: 0.011737, l4: 0.020536, l5: 0.034958, l6: 0.066555\n","\n","[epoch: 15955/100000, batch:    12/    1, ite: 15955] train loss: 0.118601, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001134, l3: 0.011315, l4: 0.020262, l5: 0.033713, l6: 0.048554\n","\n","[epoch: 15956/100000, batch:    12/    1, ite: 15956] train loss: 0.118599, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001143, l3: 0.011560, l4: 0.020169, l5: 0.030165, l6: 0.051618\n","\n","[epoch: 15957/100000, batch:    12/    1, ite: 15957] train loss: 0.118597, tar: 0.000078 \n","l0: 0.000063, l1: 0.000068, l2: 0.001194, l3: 0.011237, l4: 0.019811, l5: 0.030265, l6: 0.052331\n","\n","[epoch: 15958/100000, batch:    12/    1, ite: 15958] train loss: 0.118595, tar: 0.000078 \n","l0: 0.000062, l1: 0.000066, l2: 0.001136, l3: 0.011045, l4: 0.019900, l5: 0.030188, l6: 0.044480\n","\n","[epoch: 15959/100000, batch:    12/    1, ite: 15959] train loss: 0.118589, tar: 0.000078 \n","l0: 0.000069, l1: 0.000069, l2: 0.001525, l3: 0.011687, l4: 0.020092, l5: 0.033771, l6: 0.063966\n","\n","[epoch: 15960/100000, batch:    12/    1, ite: 15960] train loss: 0.118596, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001390, l3: 0.011331, l4: 0.019892, l5: 0.031001, l6: 0.051656\n","\n","[epoch: 15961/100000, batch:    12/    1, ite: 15961] train loss: 0.118594, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001169, l3: 0.011943, l4: 0.020877, l5: 0.033345, l6: 0.062542\n","\n","[epoch: 15962/100000, batch:    12/    1, ite: 15962] train loss: 0.118600, tar: 0.000078 \n","l0: 0.000066, l1: 0.000067, l2: 0.001435, l3: 0.011507, l4: 0.019270, l5: 0.030789, l6: 0.054249\n","\n","[epoch: 15963/100000, batch:    12/    1, ite: 15963] train loss: 0.118599, tar: 0.000078 \n","l0: 0.000072, l1: 0.000072, l2: 0.001323, l3: 0.011146, l4: 0.019994, l5: 0.029709, l6: 0.048965\n","\n","[epoch: 15964/100000, batch:    12/    1, ite: 15964] train loss: 0.118596, tar: 0.000078 \n","l0: 0.000058, l1: 0.000058, l2: 0.001362, l3: 0.011676, l4: 0.020328, l5: 0.034096, l6: 0.063955\n","\n","[epoch: 15965/100000, batch:    12/    1, ite: 15965] train loss: 0.118602, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001204, l3: 0.011328, l4: 0.020431, l5: 0.034759, l6: 0.063482\n","\n","[epoch: 15966/100000, batch:    12/    1, ite: 15966] train loss: 0.118609, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001114, l3: 0.011320, l4: 0.020683, l5: 0.032178, l6: 0.055707\n","\n","[epoch: 15967/100000, batch:    12/    1, ite: 15967] train loss: 0.118610, tar: 0.000078 \n","l0: 0.000068, l1: 0.000068, l2: 0.001160, l3: 0.011616, l4: 0.020300, l5: 0.034634, l6: 0.065595\n","\n","[epoch: 15968/100000, batch:    12/    1, ite: 15968] train loss: 0.118618, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001104, l3: 0.011245, l4: 0.020012, l5: 0.030746, l6: 0.050154\n","\n","[epoch: 15969/100000, batch:    12/    1, ite: 15969] train loss: 0.118615, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001534, l3: 0.011236, l4: 0.020409, l5: 0.033582, l6: 0.049412\n","\n","[epoch: 15970/100000, batch:    12/    1, ite: 15970] train loss: 0.118614, tar: 0.000078 \n","l0: 0.000067, l1: 0.000068, l2: 0.001114, l3: 0.011734, l4: 0.020297, l5: 0.030886, l6: 0.057395\n","\n","[epoch: 15971/100000, batch:    12/    1, ite: 15971] train loss: 0.118615, tar: 0.000078 \n","l0: 0.000070, l1: 0.000069, l2: 0.001238, l3: 0.010834, l4: 0.019884, l5: 0.033137, l6: 0.066483\n","\n","[epoch: 15972/100000, batch:    12/    1, ite: 15972] train loss: 0.118622, tar: 0.000078 \n","l0: 0.000065, l1: 0.000069, l2: 0.001214, l3: 0.011300, l4: 0.020204, l5: 0.033760, l6: 0.048451\n","\n","[epoch: 15973/100000, batch:    12/    1, ite: 15973] train loss: 0.118620, tar: 0.000078 \n","l0: 0.000066, l1: 0.000066, l2: 0.001508, l3: 0.011508, l4: 0.019985, l5: 0.031854, l6: 0.046681\n","\n","[epoch: 15974/100000, batch:    12/    1, ite: 15974] train loss: 0.118617, tar: 0.000078 \n","l0: 0.000073, l1: 0.000074, l2: 0.001530, l3: 0.011129, l4: 0.020537, l5: 0.034247, l6: 0.066551\n","\n","[epoch: 15975/100000, batch:    12/    1, ite: 15975] train loss: 0.118624, tar: 0.000078 \n","l0: 0.000071, l1: 0.000070, l2: 0.001331, l3: 0.011565, l4: 0.020051, l5: 0.031209, l6: 0.045887\n","\n","[epoch: 15976/100000, batch:    12/    1, ite: 15976] train loss: 0.118620, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001308, l3: 0.011696, l4: 0.020399, l5: 0.031790, l6: 0.050069\n","\n","[epoch: 15977/100000, batch:    12/    1, ite: 15977] train loss: 0.118619, tar: 0.000078 \n","l0: 0.000064, l1: 0.000072, l2: 0.001668, l3: 0.011485, l4: 0.020056, l5: 0.033272, l6: 0.064382\n","\n","[epoch: 15978/100000, batch:    12/    1, ite: 15978] train loss: 0.118625, tar: 0.000078 \n","l0: 0.000068, l1: 0.000068, l2: 0.001185, l3: 0.011666, l4: 0.020571, l5: 0.032656, l6: 0.048798\n","\n","[epoch: 15979/100000, batch:    12/    1, ite: 15979] train loss: 0.118623, tar: 0.000078 \n","l0: 0.000075, l1: 0.000071, l2: 0.001447, l3: 0.011169, l4: 0.020116, l5: 0.030061, l6: 0.054155\n","\n","[epoch: 15980/100000, batch:    12/    1, ite: 15980] train loss: 0.118622, tar: 0.000078 \n","l0: 0.000072, l1: 0.000072, l2: 0.001387, l3: 0.011108, l4: 0.020073, l5: 0.032837, l6: 0.048713\n","\n","[epoch: 15981/100000, batch:    12/    1, ite: 15981] train loss: 0.118620, tar: 0.000078 \n","l0: 0.000062, l1: 0.000062, l2: 0.001180, l3: 0.011069, l4: 0.019984, l5: 0.032208, l6: 0.045867\n","\n","[epoch: 15982/100000, batch:    12/    1, ite: 15982] train loss: 0.118616, tar: 0.000078 \n","l0: 0.000060, l1: 0.000065, l2: 0.001460, l3: 0.011153, l4: 0.020134, l5: 0.033383, l6: 0.066265\n","\n","[epoch: 15983/100000, batch:    12/    1, ite: 15983] train loss: 0.118623, tar: 0.000078 \n","l0: 0.000069, l1: 0.000069, l2: 0.001176, l3: 0.011387, l4: 0.020588, l5: 0.031044, l6: 0.055450\n","\n","[epoch: 15984/100000, batch:    12/    1, ite: 15984] train loss: 0.118624, tar: 0.000078 \n","l0: 0.000068, l1: 0.000067, l2: 0.001534, l3: 0.011200, l4: 0.020383, l5: 0.031768, l6: 0.049136\n","\n","[epoch: 15985/100000, batch:    12/    1, ite: 15985] train loss: 0.118621, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001131, l3: 0.011520, l4: 0.019926, l5: 0.031272, l6: 0.045938\n","\n","[epoch: 15986/100000, batch:    12/    1, ite: 15986] train loss: 0.118617, tar: 0.000078 \n","l0: 0.000059, l1: 0.000059, l2: 0.001379, l3: 0.011634, l4: 0.020414, l5: 0.032459, l6: 0.056823\n","\n","[epoch: 15987/100000, batch:    12/    1, ite: 15987] train loss: 0.118619, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001130, l3: 0.011494, l4: 0.019940, l5: 0.031681, l6: 0.052069\n","\n","[epoch: 15988/100000, batch:    12/    1, ite: 15988] train loss: 0.118618, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001222, l3: 0.011257, l4: 0.020076, l5: 0.034306, l6: 0.062802\n","\n","[epoch: 15989/100000, batch:    12/    1, ite: 15989] train loss: 0.118624, tar: 0.000078 \n","l0: 0.000067, l1: 0.000066, l2: 0.001120, l3: 0.011221, l4: 0.019594, l5: 0.030536, l6: 0.049825\n","\n","[epoch: 15990/100000, batch:    12/    1, ite: 15990] train loss: 0.118620, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001525, l3: 0.011729, l4: 0.020558, l5: 0.031660, l6: 0.053920\n","\n","[epoch: 15991/100000, batch:    12/    1, ite: 15991] train loss: 0.118621, tar: 0.000078 \n","l0: 0.000071, l1: 0.000071, l2: 0.001185, l3: 0.011331, l4: 0.020687, l5: 0.033144, l6: 0.049189\n","\n","[epoch: 15992/100000, batch:    12/    1, ite: 15992] train loss: 0.118619, tar: 0.000078 \n","l0: 0.000064, l1: 0.000066, l2: 0.001591, l3: 0.011036, l4: 0.019981, l5: 0.033989, l6: 0.068206\n","\n","[epoch: 15993/100000, batch:    12/    1, ite: 15993] train loss: 0.118628, tar: 0.000078 \n","l0: 0.000063, l1: 0.000063, l2: 0.001172, l3: 0.011422, l4: 0.020181, l5: 0.031287, l6: 0.054244\n","\n","[epoch: 15994/100000, batch:    12/    1, ite: 15994] train loss: 0.118627, tar: 0.000078 \n","l0: 0.000067, l1: 0.000067, l2: 0.001148, l3: 0.011249, l4: 0.019731, l5: 0.032299, l6: 0.046524\n","\n","[epoch: 15995/100000, batch:    12/    1, ite: 15995] train loss: 0.118624, tar: 0.000078 \n","l0: 0.000062, l1: 0.000063, l2: 0.001109, l3: 0.010984, l4: 0.019993, l5: 0.031710, l6: 0.045003\n","\n","[epoch: 15996/100000, batch:    12/    1, ite: 15996] train loss: 0.118619, tar: 0.000078 \n","l0: 0.000061, l1: 0.000062, l2: 0.001135, l3: 0.011239, l4: 0.019768, l5: 0.030749, l6: 0.043118\n","\n","[epoch: 15997/100000, batch:    12/    1, ite: 15997] train loss: 0.118613, tar: 0.000078 \n","l0: 0.000061, l1: 0.000061, l2: 0.001509, l3: 0.011069, l4: 0.020135, l5: 0.033755, l6: 0.066287\n","\n","[epoch: 15998/100000, batch:    12/    1, ite: 15998] train loss: 0.118620, tar: 0.000078 \n","l0: 0.000068, l1: 0.000068, l2: 0.001506, l3: 0.011129, l4: 0.019995, l5: 0.031156, l6: 0.053372\n","\n","[epoch: 15999/100000, batch:    12/    1, ite: 15999] train loss: 0.118619, tar: 0.000078 \n","l0: 0.000059, l1: 0.000059, l2: 0.001344, l3: 0.011091, l4: 0.019647, l5: 0.032184, l6: 0.063466\n","\n","[epoch: 16000/100000, batch:    12/    1, ite: 16000] train loss: 0.118624, tar: 0.000078 \n","l0: 0.000071, l1: 0.000070, l2: 0.001319, l3: 0.011616, l4: 0.019902, l5: 0.030564, l6: 0.051871\n","\n","[epoch: 16001/100000, batch:    12/    1, ite: 16001] train loss: 0.115414, tar: 0.000071 \n","l0: 0.000069, l1: 0.000068, l2: 0.001182, l3: 0.011137, l4: 0.019956, l5: 0.032654, l6: 0.048032\n","\n","[epoch: 16002/100000, batch:    12/    1, ite: 16002] train loss: 0.114256, tar: 0.000070 \n","l0: 0.000069, l1: 0.000068, l2: 0.001482, l3: 0.011681, l4: 0.019742, l5: 0.029398, l6: 0.051211\n","\n","[epoch: 16003/100000, batch:    12/    1, ite: 16003] train loss: 0.114054, tar: 0.000069 \n","l0: 0.000071, l1: 0.000075, l2: 0.001140, l3: 0.011382, l4: 0.020159, l5: 0.031280, l6: 0.053990\n","\n","[epoch: 16004/100000, batch:    12/    1, ite: 16004] train loss: 0.115065, tar: 0.000070 \n","l0: 0.000067, l1: 0.000067, l2: 0.001199, l3: 0.011112, l4: 0.019821, l5: 0.032560, l6: 0.047795\n","\n","[epoch: 16005/100000, batch:    12/    1, ite: 16005] train loss: 0.114576, tar: 0.000069 \n","l0: 0.000065, l1: 0.000070, l2: 0.001597, l3: 0.010843, l4: 0.019693, l5: 0.032537, l6: 0.064254\n","\n","[epoch: 16006/100000, batch:    12/    1, ite: 16006] train loss: 0.116990, tar: 0.000069 \n","l0: 0.000071, l1: 0.000071, l2: 0.001353, l3: 0.011347, l4: 0.020073, l5: 0.030806, l6: 0.055623\n","\n","[epoch: 16007/100000, batch:    12/    1, ite: 16007] train loss: 0.117326, tar: 0.000069 \n","l0: 0.000064, l1: 0.000064, l2: 0.001206, l3: 0.011113, l4: 0.020269, l5: 0.030258, l6: 0.048753\n","\n","[epoch: 16008/100000, batch:    12/    1, ite: 16008] train loss: 0.116626, tar: 0.000068 \n","l0: 0.000060, l1: 0.000064, l2: 0.001454, l3: 0.011043, l4: 0.020113, l5: 0.033995, l6: 0.065892\n","\n","[epoch: 16009/100000, batch:    12/    1, ite: 16009] train loss: 0.118404, tar: 0.000067 \n","l0: 0.000072, l1: 0.000074, l2: 0.001331, l3: 0.011240, l4: 0.020010, l5: 0.030251, l6: 0.047727\n","\n","[epoch: 16010/100000, batch:    12/    1, ite: 16010] train loss: 0.117634, tar: 0.000068 \n","l0: 0.000064, l1: 0.000069, l2: 0.001537, l3: 0.011634, l4: 0.020507, l5: 0.031541, l6: 0.049757\n","\n","[epoch: 16011/100000, batch:    12/    1, ite: 16011] train loss: 0.117404, tar: 0.000067 \n","l0: 0.000074, l1: 0.000073, l2: 0.001356, l3: 0.011200, l4: 0.020282, l5: 0.033271, l6: 0.048081\n","\n","[epoch: 16012/100000, batch:    12/    1, ite: 16012] train loss: 0.117149, tar: 0.000068 \n","l0: 0.000071, l1: 0.000071, l2: 0.001321, l3: 0.011711, l4: 0.020135, l5: 0.030956, l6: 0.046303\n","\n","[epoch: 16013/100000, batch:    12/    1, ite: 16013] train loss: 0.116642, tar: 0.000068 \n","l0: 0.000067, l1: 0.000067, l2: 0.001298, l3: 0.011335, l4: 0.020072, l5: 0.033032, l6: 0.064627\n","\n","[epoch: 16014/100000, batch:    12/    1, ite: 16014] train loss: 0.117632, tar: 0.000068 \n","l0: 0.000067, l1: 0.000067, l2: 0.001179, l3: 0.011613, l4: 0.019539, l5: 0.030538, l6: 0.057540\n","\n","[epoch: 16015/100000, batch:    12/    1, ite: 16015] train loss: 0.117826, tar: 0.000068 \n","l0: 0.000063, l1: 0.000063, l2: 0.001141, l3: 0.011674, l4: 0.019727, l5: 0.032227, l6: 0.054803\n","\n","[epoch: 16016/100000, batch:    12/    1, ite: 16016] train loss: 0.117943, tar: 0.000068 \n","l0: 0.000067, l1: 0.000067, l2: 0.001227, l3: 0.011246, l4: 0.020243, l5: 0.033372, l6: 0.048153\n","\n","[epoch: 16017/100000, batch:    12/    1, ite: 16017] train loss: 0.117733, tar: 0.000068 \n","l0: 0.000067, l1: 0.000067, l2: 0.001113, l3: 0.010955, l4: 0.019176, l5: 0.029212, l6: 0.049279\n","\n","[epoch: 16018/100000, batch:    12/    1, ite: 16018] train loss: 0.117296, tar: 0.000068 \n","l0: 0.000067, l1: 0.000067, l2: 0.001119, l3: 0.011193, l4: 0.020431, l5: 0.031352, l6: 0.050466\n","\n","[epoch: 16019/100000, batch:    12/    1, ite: 16019] train loss: 0.117159, tar: 0.000068 \n","l0: 0.000065, l1: 0.000065, l2: 0.001513, l3: 0.011013, l4: 0.019374, l5: 0.031408, l6: 0.049606\n","\n","[epoch: 16020/100000, batch:    12/    1, ite: 16020] train loss: 0.116954, tar: 0.000068 \n","l0: 0.000072, l1: 0.000072, l2: 0.001437, l3: 0.011550, l4: 0.019578, l5: 0.032478, l6: 0.063940\n","\n","[epoch: 16021/100000, batch:    12/    1, ite: 16021] train loss: 0.117533, tar: 0.000068 \n","l0: 0.000066, l1: 0.000066, l2: 0.001150, l3: 0.011312, l4: 0.019606, l5: 0.029778, l6: 0.050554\n","\n","[epoch: 16022/100000, batch:    12/    1, ite: 16022] train loss: 0.117306, tar: 0.000068 \n","l0: 0.000067, l1: 0.000073, l2: 0.001654, l3: 0.011570, l4: 0.019754, l5: 0.033142, l6: 0.047194\n","\n","[epoch: 16023/100000, batch:    12/    1, ite: 16023] train loss: 0.117139, tar: 0.000068 \n","l0: 0.000071, l1: 0.000072, l2: 0.001334, l3: 0.011130, l4: 0.020082, l5: 0.032763, l6: 0.050258\n","\n","[epoch: 16024/100000, batch:    12/    1, ite: 16024] train loss: 0.117079, tar: 0.000068 \n","l0: 0.000065, l1: 0.000066, l2: 0.001154, l3: 0.011325, l4: 0.019281, l5: 0.029999, l6: 0.046133\n","\n","[epoch: 16025/100000, batch:    12/    1, ite: 16025] train loss: 0.116717, tar: 0.000068 \n","l0: 0.000069, l1: 0.000069, l2: 0.001469, l3: 0.011509, l4: 0.020263, l5: 0.032517, l6: 0.054543\n","\n","[epoch: 16026/100000, batch:    12/    1, ite: 16026] train loss: 0.116860, tar: 0.000068 \n","l0: 0.000062, l1: 0.000063, l2: 0.001094, l3: 0.011468, l4: 0.019892, l5: 0.031608, l6: 0.061369\n","\n","[epoch: 16027/100000, batch:    12/    1, ite: 16027] train loss: 0.117182, tar: 0.000068 \n","l0: 0.000065, l1: 0.000065, l2: 0.001367, l3: 0.010941, l4: 0.020244, l5: 0.033759, l6: 0.067044\n","\n","[epoch: 16028/100000, batch:    12/    1, ite: 16028] train loss: 0.117764, tar: 0.000067 \n","l0: 0.000068, l1: 0.000069, l2: 0.001244, l3: 0.011032, l4: 0.020411, l5: 0.034285, l6: 0.067550\n","\n","[epoch: 16029/100000, batch:    12/    1, ite: 16029] train loss: 0.118347, tar: 0.000068 \n","l0: 0.000068, l1: 0.000068, l2: 0.001556, l3: 0.011342, l4: 0.020380, l5: 0.032959, l6: 0.048431\n","\n","[epoch: 16030/100000, batch:    12/    1, ite: 16030] train loss: 0.118229, tar: 0.000068 \n","l0: 0.000063, l1: 0.000064, l2: 0.001607, l3: 0.010975, l4: 0.019486, l5: 0.032327, l6: 0.065406\n","\n","[epoch: 16031/100000, batch:    12/    1, ite: 16031] train loss: 0.118606, tar: 0.000067 \n","l0: 0.000060, l1: 0.000060, l2: 0.001394, l3: 0.011184, l4: 0.019695, l5: 0.031602, l6: 0.052701\n","\n","[epoch: 16032/100000, batch:    12/    1, ite: 16032] train loss: 0.118546, tar: 0.000067 \n","l0: 0.000066, l1: 0.000066, l2: 0.001675, l3: 0.011220, l4: 0.020157, l5: 0.032367, l6: 0.063468\n","\n","[epoch: 16033/100000, batch:    12/    1, ite: 16033] train loss: 0.118864, tar: 0.000067 \n","l0: 0.000071, l1: 0.000071, l2: 0.001156, l3: 0.011729, l4: 0.020252, l5: 0.033806, l6: 0.048183\n","\n","[epoch: 16034/100000, batch:    12/    1, ite: 16034] train loss: 0.118758, tar: 0.000067 \n","l0: 0.000067, l1: 0.000071, l2: 0.001180, l3: 0.011413, l4: 0.019281, l5: 0.030937, l6: 0.049758\n","\n","[epoch: 16035/100000, batch:    12/    1, ite: 16035] train loss: 0.118585, tar: 0.000067 \n","l0: 0.000064, l1: 0.000064, l2: 0.001170, l3: 0.011451, l4: 0.019718, l5: 0.030091, l6: 0.053933\n","\n","[epoch: 16036/100000, batch:    12/    1, ite: 16036] train loss: 0.118527, tar: 0.000067 \n","l0: 0.000068, l1: 0.000068, l2: 0.001247, l3: 0.011611, l4: 0.020430, l5: 0.033663, l6: 0.061722\n","\n","[epoch: 16037/100000, batch:    12/    1, ite: 16037] train loss: 0.118805, tar: 0.000067 \n","l0: 0.000072, l1: 0.000072, l2: 0.001360, l3: 0.011159, l4: 0.020284, l5: 0.032883, l6: 0.060958\n","\n","[epoch: 16038/100000, batch:    12/    1, ite: 16038] train loss: 0.119015, tar: 0.000067 \n","l0: 0.000074, l1: 0.000074, l2: 0.001453, l3: 0.011728, l4: 0.020442, l5: 0.033950, l6: 0.064190\n","\n","[epoch: 16039/100000, batch:    12/    1, ite: 16039] train loss: 0.119346, tar: 0.000067 \n","l0: 0.000063, l1: 0.000063, l2: 0.001124, l3: 0.011558, l4: 0.020085, l5: 0.031548, l6: 0.054282\n","\n","[epoch: 16040/100000, batch:    12/    1, ite: 16040] train loss: 0.119330, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001563, l3: 0.011523, l4: 0.019605, l5: 0.032841, l6: 0.064939\n","\n","[epoch: 16041/100000, batch:    12/    1, ite: 16041] train loss: 0.119605, tar: 0.000067 \n","l0: 0.000068, l1: 0.000069, l2: 0.001854, l3: 0.011680, l4: 0.020468, l5: 0.032759, l6: 0.048286\n","\n","[epoch: 16042/100000, batch:    12/    1, ite: 16042] train loss: 0.119500, tar: 0.000067 \n","l0: 0.000060, l1: 0.000060, l2: 0.001642, l3: 0.011149, l4: 0.019956, l5: 0.034013, l6: 0.067914\n","\n","[epoch: 16043/100000, batch:    12/    1, ite: 16043] train loss: 0.119855, tar: 0.000067 \n","l0: 0.000060, l1: 0.000060, l2: 0.001474, l3: 0.011038, l4: 0.020470, l5: 0.034808, l6: 0.066911\n","\n","[epoch: 16044/100000, batch:    12/    1, ite: 16044] train loss: 0.120196, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001573, l3: 0.011654, l4: 0.020074, l5: 0.031650, l6: 0.057504\n","\n","[epoch: 16045/100000, batch:    12/    1, ite: 16045] train loss: 0.120249, tar: 0.000067 \n","l0: 0.000054, l1: 0.000059, l2: 0.001604, l3: 0.011557, l4: 0.020564, l5: 0.031557, l6: 0.057505\n","\n","[epoch: 16046/100000, batch:    12/    1, ite: 16046] train loss: 0.120306, tar: 0.000067 \n","l0: 0.000075, l1: 0.000075, l2: 0.001421, l3: 0.010834, l4: 0.019884, l5: 0.032502, l6: 0.066558\n","\n","[epoch: 16047/100000, batch:    12/    1, ite: 16047] train loss: 0.120541, tar: 0.000067 \n","l0: 0.000065, l1: 0.000065, l2: 0.001289, l3: 0.011581, l4: 0.020311, l5: 0.034196, l6: 0.065822\n","\n","[epoch: 16048/100000, batch:    12/    1, ite: 16048] train loss: 0.120808, tar: 0.000067 \n","l0: 0.000074, l1: 0.000074, l2: 0.001510, l3: 0.011156, l4: 0.020774, l5: 0.034901, l6: 0.066944\n","\n","[epoch: 16049/100000, batch:    12/    1, ite: 16049] train loss: 0.121106, tar: 0.000067 \n","l0: 0.000066, l1: 0.000066, l2: 0.001533, l3: 0.011175, l4: 0.020207, l5: 0.033095, l6: 0.061923\n","\n","[epoch: 16050/100000, batch:    12/    1, ite: 16050] train loss: 0.121245, tar: 0.000067 \n","l0: 0.000071, l1: 0.000072, l2: 0.001262, l3: 0.011587, l4: 0.019628, l5: 0.033177, l6: 0.046196\n","\n","[epoch: 16051/100000, batch:    12/    1, ite: 16051] train loss: 0.121064, tar: 0.000067 \n","l0: 0.000068, l1: 0.000069, l2: 0.001213, l3: 0.011478, l4: 0.019978, l5: 0.030164, l6: 0.051036\n","\n","[epoch: 16052/100000, batch:    12/    1, ite: 16052] train loss: 0.120928, tar: 0.000067 \n","l0: 0.000071, l1: 0.000072, l2: 0.001221, l3: 0.011444, l4: 0.019825, l5: 0.030992, l6: 0.050132\n","\n","[epoch: 16053/100000, batch:    12/    1, ite: 16053] train loss: 0.120793, tar: 0.000067 \n","l0: 0.000068, l1: 0.000068, l2: 0.001218, l3: 0.011319, l4: 0.019900, l5: 0.030136, l6: 0.054123\n","\n","[epoch: 16054/100000, batch:    12/    1, ite: 16054] train loss: 0.120720, tar: 0.000067 \n","l0: 0.000050, l1: 0.000055, l2: 0.001635, l3: 0.011306, l4: 0.019240, l5: 0.032631, l6: 0.063347\n","\n","[epoch: 16055/100000, batch:    12/    1, ite: 16055] train loss: 0.120857, tar: 0.000067 \n","l0: 0.000071, l1: 0.000070, l2: 0.001404, l3: 0.011471, l4: 0.020170, l5: 0.031677, l6: 0.058559\n","\n","[epoch: 16056/100000, batch:    12/    1, ite: 16056] train loss: 0.120903, tar: 0.000067 \n","l0: 0.000063, l1: 0.000064, l2: 0.001577, l3: 0.011677, l4: 0.020674, l5: 0.033527, l6: 0.060148\n","\n","[epoch: 16057/100000, batch:    12/    1, ite: 16057] train loss: 0.121022, tar: 0.000067 \n","l0: 0.000062, l1: 0.000062, l2: 0.001256, l3: 0.011559, l4: 0.019695, l5: 0.031688, l6: 0.061827\n","\n","[epoch: 16058/100000, batch:    12/    1, ite: 16058] train loss: 0.121111, tar: 0.000067 \n","l0: 0.000072, l1: 0.000071, l2: 0.001345, l3: 0.011160, l4: 0.020037, l5: 0.033407, l6: 0.049047\n","\n","[epoch: 16059/100000, batch:    12/    1, ite: 16059] train loss: 0.121010, tar: 0.000067 \n","l0: 0.000063, l1: 0.000063, l2: 0.001441, l3: 0.011102, l4: 0.020061, l5: 0.029531, l6: 0.048835\n","\n","[epoch: 16060/100000, batch:    12/    1, ite: 16060] train loss: 0.120844, tar: 0.000067 \n","l0: 0.000072, l1: 0.000072, l2: 0.001604, l3: 0.011844, l4: 0.021048, l5: 0.032518, l6: 0.061445\n","\n","[epoch: 16061/100000, batch:    12/    1, ite: 16061] train loss: 0.120971, tar: 0.000067 \n","l0: 0.000071, l1: 0.000071, l2: 0.001339, l3: 0.011601, l4: 0.020441, l5: 0.032310, l6: 0.049490\n","\n","[epoch: 16062/100000, batch:    12/    1, ite: 16062] train loss: 0.120880, tar: 0.000067 \n","l0: 0.000072, l1: 0.000072, l2: 0.001510, l3: 0.011706, l4: 0.019955, l5: 0.030501, l6: 0.052354\n","\n","[epoch: 16063/100000, batch:    12/    1, ite: 16063] train loss: 0.120806, tar: 0.000067 \n","l0: 0.000062, l1: 0.000062, l2: 0.001095, l3: 0.011401, l4: 0.019753, l5: 0.031724, l6: 0.045769\n","\n","[epoch: 16064/100000, batch:    12/    1, ite: 16064] train loss: 0.120635, tar: 0.000067 \n","l0: 0.000066, l1: 0.000066, l2: 0.001546, l3: 0.011056, l4: 0.019763, l5: 0.031192, l6: 0.050541\n","\n","[epoch: 16065/100000, batch:    12/    1, ite: 16065] train loss: 0.120536, tar: 0.000067 \n","l0: 0.000069, l1: 0.000068, l2: 0.001205, l3: 0.011405, l4: 0.019935, l5: 0.030122, l6: 0.049992\n","\n","[epoch: 16066/100000, batch:    12/    1, ite: 16066] train loss: 0.120419, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001161, l3: 0.011359, l4: 0.020585, l5: 0.032901, l6: 0.057953\n","\n","[epoch: 16067/100000, batch:    12/    1, ite: 16067] train loss: 0.120474, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001179, l3: 0.011474, l4: 0.019912, l5: 0.031645, l6: 0.051964\n","\n","[epoch: 16068/100000, batch:    12/    1, ite: 16068] train loss: 0.120412, tar: 0.000067 \n","l0: 0.000072, l1: 0.000071, l2: 0.001170, l3: 0.011666, l4: 0.020004, l5: 0.030111, l6: 0.050628\n","\n","[epoch: 16069/100000, batch:    12/    1, ite: 16069] train loss: 0.120315, tar: 0.000067 \n","l0: 0.000071, l1: 0.000072, l2: 0.001181, l3: 0.011596, l4: 0.020433, l5: 0.031459, l6: 0.054014\n","\n","[epoch: 16070/100000, batch:    12/    1, ite: 16070] train loss: 0.120294, tar: 0.000067 \n","l0: 0.000058, l1: 0.000058, l2: 0.001308, l3: 0.011272, l4: 0.020217, l5: 0.030975, l6: 0.058882\n","\n","[epoch: 16071/100000, batch:    12/    1, ite: 16071] train loss: 0.120329, tar: 0.000067 \n","l0: 0.000063, l1: 0.000063, l2: 0.001177, l3: 0.011841, l4: 0.020452, l5: 0.032445, l6: 0.062680\n","\n","[epoch: 16072/100000, batch:    12/    1, ite: 16072] train loss: 0.120446, tar: 0.000067 \n","l0: 0.000065, l1: 0.000069, l2: 0.001265, l3: 0.011793, l4: 0.020478, l5: 0.034994, l6: 0.066440\n","\n","[epoch: 16073/100000, batch:    12/    1, ite: 16073] train loss: 0.120646, tar: 0.000067 \n","l0: 0.000059, l1: 0.000060, l2: 0.001498, l3: 0.011103, l4: 0.020206, l5: 0.032582, l6: 0.067191\n","\n","[epoch: 16074/100000, batch:    12/    1, ite: 16074] train loss: 0.120809, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001506, l3: 0.011596, l4: 0.019915, l5: 0.031085, l6: 0.046380\n","\n","[epoch: 16075/100000, batch:    12/    1, ite: 16075] train loss: 0.120673, tar: 0.000067 \n","l0: 0.000067, l1: 0.000068, l2: 0.001142, l3: 0.011136, l4: 0.019993, l5: 0.032614, l6: 0.047979\n","\n","[epoch: 16076/100000, batch:    12/    1, ite: 16076] train loss: 0.120572, tar: 0.000067 \n","l0: 0.000062, l1: 0.000062, l2: 0.001240, l3: 0.011075, l4: 0.019347, l5: 0.032526, l6: 0.062463\n","\n","[epoch: 16077/100000, batch:    12/    1, ite: 16077] train loss: 0.120653, tar: 0.000067 \n","l0: 0.000062, l1: 0.000067, l2: 0.001137, l3: 0.011302, l4: 0.019467, l5: 0.031044, l6: 0.049050\n","\n","[epoch: 16078/100000, batch:    12/    1, ite: 16078] train loss: 0.120544, tar: 0.000067 \n","l0: 0.000067, l1: 0.000067, l2: 0.001099, l3: 0.011588, l4: 0.020010, l5: 0.033800, l6: 0.049049\n","\n","[epoch: 16079/100000, batch:    12/    1, ite: 16079] train loss: 0.120482, tar: 0.000067 \n","l0: 0.000068, l1: 0.000068, l2: 0.001312, l3: 0.011580, l4: 0.020575, l5: 0.032967, l6: 0.050139\n","\n","[epoch: 16080/100000, batch:    12/    1, ite: 16080] train loss: 0.120435, tar: 0.000067 \n","l0: 0.000063, l1: 0.000063, l2: 0.001148, l3: 0.010963, l4: 0.019677, l5: 0.032210, l6: 0.044853\n","\n","[epoch: 16081/100000, batch:    12/    1, ite: 16081] train loss: 0.120293, tar: 0.000067 \n","l0: 0.000063, l1: 0.000063, l2: 0.001152, l3: 0.011393, l4: 0.019319, l5: 0.031590, l6: 0.054913\n","\n","[epoch: 16082/100000, batch:    12/    1, ite: 16082] train loss: 0.120271, tar: 0.000067 \n","l0: 0.000058, l1: 0.000059, l2: 0.001078, l3: 0.010835, l4: 0.019040, l5: 0.030552, l6: 0.047496\n","\n","[epoch: 16083/100000, batch:    12/    1, ite: 16083] train loss: 0.120137, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001572, l3: 0.011191, l4: 0.019904, l5: 0.030624, l6: 0.051377\n","\n","[epoch: 16084/100000, batch:    12/    1, ite: 16084] train loss: 0.120074, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001284, l3: 0.011200, l4: 0.020249, l5: 0.032253, l6: 0.046904\n","\n","[epoch: 16085/100000, batch:    12/    1, ite: 16085] train loss: 0.119979, tar: 0.000066 \n","l0: 0.000077, l1: 0.000078, l2: 0.001414, l3: 0.011165, l4: 0.020224, l5: 0.029896, l6: 0.049149\n","\n","[epoch: 16086/100000, batch:    12/    1, ite: 16086] train loss: 0.119886, tar: 0.000067 \n","l0: 0.000067, l1: 0.000068, l2: 0.001564, l3: 0.011085, l4: 0.020179, l5: 0.031496, l6: 0.051308\n","\n","[epoch: 16087/100000, batch:    12/    1, ite: 16087] train loss: 0.119839, tar: 0.000067 \n","l0: 0.000066, l1: 0.000066, l2: 0.001419, l3: 0.010935, l4: 0.020275, l5: 0.033796, l6: 0.067139\n","\n","[epoch: 16088/100000, batch:    12/    1, ite: 16088] train loss: 0.119996, tar: 0.000067 \n","l0: 0.000066, l1: 0.000067, l2: 0.001507, l3: 0.011846, l4: 0.020529, l5: 0.031924, l6: 0.061462\n","\n","[epoch: 16089/100000, batch:    12/    1, ite: 16089] train loss: 0.120079, tar: 0.000067 \n","l0: 0.000066, l1: 0.000066, l2: 0.001540, l3: 0.011020, l4: 0.019755, l5: 0.032390, l6: 0.043579\n","\n","[epoch: 16090/100000, batch:    12/    1, ite: 16090] train loss: 0.119950, tar: 0.000067 \n","l0: 0.000067, l1: 0.000076, l2: 0.001204, l3: 0.011380, l4: 0.020692, l5: 0.033096, l6: 0.048812\n","\n","[epoch: 16091/100000, batch:    12/    1, ite: 16091] train loss: 0.119899, tar: 0.000067 \n","l0: 0.000059, l1: 0.000064, l2: 0.001313, l3: 0.011065, l4: 0.020329, l5: 0.031170, l6: 0.051117\n","\n","[epoch: 16092/100000, batch:    12/    1, ite: 16092] train loss: 0.119847, tar: 0.000067 \n","l0: 0.000064, l1: 0.000069, l2: 0.001263, l3: 0.011766, l4: 0.020355, l5: 0.033174, l6: 0.058838\n","\n","[epoch: 16093/100000, batch:    12/    1, ite: 16093] train loss: 0.119908, tar: 0.000067 \n","l0: 0.000062, l1: 0.000062, l2: 0.001120, l3: 0.010963, l4: 0.019872, l5: 0.030339, l6: 0.044530\n","\n","[epoch: 16094/100000, batch:    12/    1, ite: 16094] train loss: 0.119770, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001232, l3: 0.011147, l4: 0.020462, l5: 0.033104, l6: 0.048482\n","\n","[epoch: 16095/100000, batch:    12/    1, ite: 16095] train loss: 0.119715, tar: 0.000066 \n","l0: 0.000074, l1: 0.000075, l2: 0.001656, l3: 0.010950, l4: 0.019896, l5: 0.033221, l6: 0.065384\n","\n","[epoch: 16096/100000, batch:    12/    1, ite: 16096] train loss: 0.119836, tar: 0.000067 \n","l0: 0.000066, l1: 0.000067, l2: 0.001500, l3: 0.011098, l4: 0.020037, l5: 0.030162, l6: 0.048350\n","\n","[epoch: 16097/100000, batch:    12/    1, ite: 16097] train loss: 0.119747, tar: 0.000067 \n","l0: 0.000058, l1: 0.000058, l2: 0.001361, l3: 0.011679, l4: 0.020651, l5: 0.031710, l6: 0.048364\n","\n","[epoch: 16098/100000, batch:    12/    1, ite: 16098] train loss: 0.119688, tar: 0.000066 \n","l0: 0.000066, l1: 0.000068, l2: 0.001181, l3: 0.011651, l4: 0.020171, l5: 0.034022, l6: 0.065323\n","\n","[epoch: 16099/100000, batch:    12/    1, ite: 16099] train loss: 0.119817, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001224, l3: 0.011453, l4: 0.019570, l5: 0.030196, l6: 0.047459\n","\n","[epoch: 16100/100000, batch:    12/    1, ite: 16100] train loss: 0.119719, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001140, l3: 0.011087, l4: 0.019824, l5: 0.032618, l6: 0.047795\n","\n","[epoch: 16101/100000, batch:    12/    1, ite: 16101] train loss: 0.119648, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001552, l3: 0.011565, l4: 0.019542, l5: 0.032295, l6: 0.044723\n","\n","[epoch: 16102/100000, batch:    12/    1, ite: 16102] train loss: 0.119552, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001273, l3: 0.011016, l4: 0.019769, l5: 0.031545, l6: 0.053679\n","\n","[epoch: 16103/100000, batch:    12/    1, ite: 16103] train loss: 0.119531, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001167, l3: 0.011551, l4: 0.019756, l5: 0.033090, l6: 0.046536\n","\n","[epoch: 16104/100000, batch:    12/    1, ite: 16104] train loss: 0.119461, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001161, l3: 0.011290, l4: 0.019734, l5: 0.031968, l6: 0.053024\n","\n","[epoch: 16105/100000, batch:    12/    1, ite: 16105] train loss: 0.119440, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001507, l3: 0.011618, l4: 0.020040, l5: 0.031211, l6: 0.051231\n","\n","[epoch: 16106/100000, batch:    12/    1, ite: 16106] train loss: 0.119405, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001529, l3: 0.011213, l4: 0.020469, l5: 0.031385, l6: 0.053729\n","\n","[epoch: 16107/100000, batch:    12/    1, ite: 16107] train loss: 0.119397, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001333, l3: 0.010826, l4: 0.020392, l5: 0.033819, l6: 0.066937\n","\n","[epoch: 16108/100000, batch:    12/    1, ite: 16108] train loss: 0.119527, tar: 0.000066 \n","l0: 0.000070, l1: 0.000075, l2: 0.001638, l3: 0.011053, l4: 0.020228, l5: 0.034339, l6: 0.066298\n","\n","[epoch: 16109/100000, batch:    12/    1, ite: 16109] train loss: 0.119657, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001549, l3: 0.011145, l4: 0.019998, l5: 0.032435, l6: 0.063482\n","\n","[epoch: 16110/100000, batch:    12/    1, ite: 16110] train loss: 0.119739, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001485, l3: 0.011102, l4: 0.020040, l5: 0.031124, l6: 0.052582\n","\n","[epoch: 16111/100000, batch:    12/    1, ite: 16111] train loss: 0.119710, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001148, l3: 0.011452, l4: 0.019819, l5: 0.031421, l6: 0.046281\n","\n","[epoch: 16112/100000, batch:    12/    1, ite: 16112] train loss: 0.119625, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001436, l3: 0.011006, l4: 0.019826, l5: 0.032121, l6: 0.044702\n","\n","[epoch: 16113/100000, batch:    12/    1, ite: 16113] train loss: 0.119533, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001127, l3: 0.011384, l4: 0.019854, l5: 0.030148, l6: 0.050311\n","\n","[epoch: 16114/100000, batch:    12/    1, ite: 16114] train loss: 0.119476, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001192, l3: 0.011561, l4: 0.020334, l5: 0.032390, l6: 0.053769\n","\n","[epoch: 16115/100000, batch:    12/    1, ite: 16115] train loss: 0.119475, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001587, l3: 0.011802, l4: 0.020387, l5: 0.031035, l6: 0.047271\n","\n","[epoch: 16116/100000, batch:    12/    1, ite: 16116] train loss: 0.119412, tar: 0.000066 \n","l0: 0.000072, l1: 0.000071, l2: 0.001218, l3: 0.011506, l4: 0.020449, l5: 0.030527, l6: 0.047842\n","\n","[epoch: 16117/100000, batch:    12/    1, ite: 16117] train loss: 0.119346, tar: 0.000066 \n","l0: 0.000070, l1: 0.000075, l2: 0.001218, l3: 0.011328, l4: 0.020787, l5: 0.031225, l6: 0.049799\n","\n","[epoch: 16118/100000, batch:    12/    1, ite: 16118] train loss: 0.119305, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001575, l3: 0.011762, l4: 0.019990, l5: 0.033578, l6: 0.065520\n","\n","[epoch: 16119/100000, batch:    12/    1, ite: 16119] train loss: 0.119417, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001189, l3: 0.011621, l4: 0.019998, l5: 0.033652, l6: 0.065986\n","\n","[epoch: 16120/100000, batch:    12/    1, ite: 16120] train loss: 0.119526, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001348, l3: 0.011732, l4: 0.020125, l5: 0.034037, l6: 0.048235\n","\n","[epoch: 16121/100000, batch:    12/    1, ite: 16121] train loss: 0.119494, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001133, l3: 0.011397, l4: 0.020043, l5: 0.033234, l6: 0.049265\n","\n","[epoch: 16122/100000, batch:    12/    1, ite: 16122] train loss: 0.119459, tar: 0.000066 \n","l0: 0.000064, l1: 0.000068, l2: 0.001290, l3: 0.011805, l4: 0.019999, l5: 0.033708, l6: 0.066192\n","\n","[epoch: 16123/100000, batch:    12/    1, ite: 16123] train loss: 0.119570, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001137, l3: 0.011411, l4: 0.019445, l5: 0.030165, l6: 0.047094\n","\n","[epoch: 16124/100000, batch:    12/    1, ite: 16124] train loss: 0.119487, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001288, l3: 0.011698, l4: 0.020323, l5: 0.032980, l6: 0.058704\n","\n","[epoch: 16125/100000, batch:    12/    1, ite: 16125] train loss: 0.119533, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001622, l3: 0.011150, l4: 0.019999, l5: 0.033006, l6: 0.061566\n","\n","[epoch: 16126/100000, batch:    12/    1, ite: 16126] train loss: 0.119596, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001314, l3: 0.011559, l4: 0.020524, l5: 0.031530, l6: 0.057038\n","\n","[epoch: 16127/100000, batch:    12/    1, ite: 16127] train loss: 0.119615, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001179, l3: 0.011444, l4: 0.020757, l5: 0.030931, l6: 0.049214\n","\n","[epoch: 16128/100000, batch:    12/    1, ite: 16128] train loss: 0.119569, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001473, l3: 0.011482, l4: 0.019556, l5: 0.032520, l6: 0.063931\n","\n","[epoch: 16129/100000, batch:    12/    1, ite: 16129] train loss: 0.119643, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001553, l3: 0.010898, l4: 0.019257, l5: 0.030896, l6: 0.048380\n","\n","[epoch: 16130/100000, batch:    12/    1, ite: 16130] train loss: 0.119577, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001369, l3: 0.011071, l4: 0.019975, l5: 0.029422, l6: 0.048459\n","\n","[epoch: 16131/100000, batch:    12/    1, ite: 16131] train loss: 0.119507, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001440, l3: 0.011572, l4: 0.020584, l5: 0.033234, l6: 0.048788\n","\n","[epoch: 16132/100000, batch:    12/    1, ite: 16132] train loss: 0.119479, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001142, l3: 0.011175, l4: 0.020003, l5: 0.030502, l6: 0.044980\n","\n","[epoch: 16133/100000, batch:    12/    1, ite: 16133] train loss: 0.119392, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001305, l3: 0.011380, l4: 0.020402, l5: 0.034165, l6: 0.062627\n","\n","[epoch: 16134/100000, batch:    12/    1, ite: 16134] train loss: 0.119471, tar: 0.000066 \n","l0: 0.000058, l1: 0.000059, l2: 0.001385, l3: 0.011198, l4: 0.020095, l5: 0.030498, l6: 0.053879\n","\n","[epoch: 16135/100000, batch:    12/    1, ite: 16135] train loss: 0.119454, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001320, l3: 0.010780, l4: 0.019774, l5: 0.033016, l6: 0.065652\n","\n","[epoch: 16136/100000, batch:    12/    1, ite: 16136] train loss: 0.119537, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001098, l3: 0.010994, l4: 0.019987, l5: 0.030800, l6: 0.058106\n","\n","[epoch: 16137/100000, batch:    12/    1, ite: 16137] train loss: 0.119548, tar: 0.000066 \n","l0: 0.000068, l1: 0.000072, l2: 0.001242, l3: 0.011312, l4: 0.019565, l5: 0.032537, l6: 0.064016\n","\n","[epoch: 16138/100000, batch:    12/    1, ite: 16138] train loss: 0.119615, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001458, l3: 0.011714, l4: 0.019863, l5: 0.030140, l6: 0.051301\n","\n","[epoch: 16139/100000, batch:    12/    1, ite: 16139] train loss: 0.119579, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001140, l3: 0.011559, l4: 0.019458, l5: 0.030567, l6: 0.056007\n","\n","[epoch: 16140/100000, batch:    12/    1, ite: 16140] train loss: 0.119574, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001259, l3: 0.011340, l4: 0.019659, l5: 0.033578, l6: 0.065568\n","\n","[epoch: 16141/100000, batch:    12/    1, ite: 16141] train loss: 0.119659, tar: 0.000066 \n","l0: 0.000068, l1: 0.000072, l2: 0.001136, l3: 0.011179, l4: 0.020032, l5: 0.030558, l6: 0.045551\n","\n","[epoch: 16142/100000, batch:    12/    1, ite: 16142] train loss: 0.119581, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001215, l3: 0.011394, l4: 0.019791, l5: 0.033518, l6: 0.065518\n","\n","[epoch: 16143/100000, batch:    12/    1, ite: 16143] train loss: 0.119665, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001571, l3: 0.011494, l4: 0.019711, l5: 0.031222, l6: 0.051461\n","\n","[epoch: 16144/100000, batch:    12/    1, ite: 16144] train loss: 0.119637, tar: 0.000066 \n","l0: 0.000063, l1: 0.000068, l2: 0.001198, l3: 0.011653, l4: 0.019737, l5: 0.033105, l6: 0.047254\n","\n","[epoch: 16145/100000, batch:    12/    1, ite: 16145] train loss: 0.119591, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001154, l3: 0.011315, l4: 0.020091, l5: 0.032390, l6: 0.050705\n","\n","[epoch: 16146/100000, batch:    12/    1, ite: 16146] train loss: 0.119565, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001569, l3: 0.010787, l4: 0.019550, l5: 0.032402, l6: 0.064206\n","\n","[epoch: 16147/100000, batch:    12/    1, ite: 16147] train loss: 0.119627, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001415, l3: 0.011162, l4: 0.020267, l5: 0.033242, l6: 0.048097\n","\n","[epoch: 16148/100000, batch:    12/    1, ite: 16148] train loss: 0.119591, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001465, l3: 0.010930, l4: 0.019586, l5: 0.030222, l6: 0.045905\n","\n","[epoch: 16149/100000, batch:    12/    1, ite: 16149] train loss: 0.119515, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001167, l3: 0.011557, l4: 0.019927, l5: 0.031739, l6: 0.061435\n","\n","[epoch: 16150/100000, batch:    12/    1, ite: 16150] train loss: 0.119558, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001268, l3: 0.011523, l4: 0.019778, l5: 0.030963, l6: 0.052611\n","\n","[epoch: 16151/100000, batch:    12/    1, ite: 16151] train loss: 0.119536, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001290, l3: 0.011305, l4: 0.020302, l5: 0.031337, l6: 0.051199\n","\n","[epoch: 16152/100000, batch:    12/    1, ite: 16152] train loss: 0.119510, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001134, l3: 0.011613, l4: 0.019651, l5: 0.029773, l6: 0.052641\n","\n","[epoch: 16153/100000, batch:    12/    1, ite: 16153] train loss: 0.119480, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001295, l3: 0.011261, l4: 0.019947, l5: 0.033180, l6: 0.062351\n","\n","[epoch: 16154/100000, batch:    12/    1, ite: 16154] train loss: 0.119536, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001155, l3: 0.011398, l4: 0.020253, l5: 0.031040, l6: 0.054286\n","\n","[epoch: 16155/100000, batch:    12/    1, ite: 16155] train loss: 0.119528, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001188, l3: 0.011630, l4: 0.020238, l5: 0.031977, l6: 0.062558\n","\n","[epoch: 16156/100000, batch:    12/    1, ite: 16156] train loss: 0.119581, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001386, l3: 0.011165, l4: 0.020323, l5: 0.033519, l6: 0.049636\n","\n","[epoch: 16157/100000, batch:    12/    1, ite: 16157] train loss: 0.119559, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001415, l3: 0.010936, l4: 0.019756, l5: 0.030057, l6: 0.046915\n","\n","[epoch: 16158/100000, batch:    12/    1, ite: 16158] train loss: 0.119494, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001487, l3: 0.010956, l4: 0.019502, l5: 0.029301, l6: 0.050396\n","\n","[epoch: 16159/100000, batch:    12/    1, ite: 16159] train loss: 0.119445, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001308, l3: 0.011219, l4: 0.020163, l5: 0.034235, l6: 0.063031\n","\n","[epoch: 16160/100000, batch:    12/    1, ite: 16160] train loss: 0.119511, tar: 0.000066 \n","l0: 0.000072, l1: 0.000069, l2: 0.001572, l3: 0.011744, l4: 0.020447, l5: 0.034758, l6: 0.065517\n","\n","[epoch: 16161/100000, batch:    12/    1, ite: 16161] train loss: 0.119603, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001151, l3: 0.011678, l4: 0.020119, l5: 0.032577, l6: 0.054313\n","\n","[epoch: 16162/100000, batch:    12/    1, ite: 16162] train loss: 0.119605, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001332, l3: 0.011123, l4: 0.020487, l5: 0.031726, l6: 0.057989\n","\n","[epoch: 16163/100000, batch:    12/    1, ite: 16163] train loss: 0.119624, tar: 0.000066 \n","l0: 0.000067, l1: 0.000072, l2: 0.001151, l3: 0.011279, l4: 0.020079, l5: 0.033456, l6: 0.049568\n","\n","[epoch: 16164/100000, batch:    12/    1, ite: 16164] train loss: 0.119600, tar: 0.000066 \n","l0: 0.000072, l1: 0.000068, l2: 0.001513, l3: 0.011112, l4: 0.019594, l5: 0.032232, l6: 0.063177\n","\n","[epoch: 16165/100000, batch:    12/    1, ite: 16165] train loss: 0.119650, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001060, l3: 0.011228, l4: 0.019612, l5: 0.031051, l6: 0.049649\n","\n","[epoch: 16166/100000, batch:    12/    1, ite: 16166] train loss: 0.119608, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001396, l3: 0.011782, l4: 0.020069, l5: 0.030588, l6: 0.051036\n","\n","[epoch: 16167/100000, batch:    12/    1, ite: 16167] train loss: 0.119581, tar: 0.000066 \n","l0: 0.000054, l1: 0.000058, l2: 0.001248, l3: 0.011686, l4: 0.020032, l5: 0.032443, l6: 0.054149\n","\n","[epoch: 16168/100000, batch:    12/    1, ite: 16168] train loss: 0.119581, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001087, l3: 0.011572, l4: 0.019875, l5: 0.031280, l6: 0.045928\n","\n","[epoch: 16169/100000, batch:    12/    1, ite: 16169] train loss: 0.119524, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001167, l3: 0.011488, l4: 0.020311, l5: 0.030690, l6: 0.047973\n","\n","[epoch: 16170/100000, batch:    12/    1, ite: 16170] train loss: 0.119478, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001496, l3: 0.011259, l4: 0.019856, l5: 0.030299, l6: 0.044391\n","\n","[epoch: 16171/100000, batch:    12/    1, ite: 16171] train loss: 0.119408, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001118, l3: 0.011107, l4: 0.019151, l5: 0.030231, l6: 0.048195\n","\n","[epoch: 16172/100000, batch:    12/    1, ite: 16172] train loss: 0.119353, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001055, l3: 0.010908, l4: 0.019812, l5: 0.030072, l6: 0.044711\n","\n","[epoch: 16173/100000, batch:    12/    1, ite: 16173] train loss: 0.119279, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001076, l3: 0.011463, l4: 0.019427, l5: 0.030535, l6: 0.056264\n","\n","[epoch: 16174/100000, batch:    12/    1, ite: 16174] train loss: 0.119277, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001095, l3: 0.011510, l4: 0.020061, l5: 0.032530, l6: 0.054043\n","\n","[epoch: 16175/100000, batch:    12/    1, ite: 16175] train loss: 0.119278, tar: 0.000066 \n","l0: 0.000071, l1: 0.000075, l2: 0.001136, l3: 0.011333, l4: 0.020734, l5: 0.033107, l6: 0.049280\n","\n","[epoch: 16176/100000, batch:    12/    1, ite: 16176] train loss: 0.119257, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001127, l3: 0.011140, l4: 0.020124, l5: 0.033094, l6: 0.047420\n","\n","[epoch: 16177/100000, batch:    12/    1, ite: 16177] train loss: 0.119222, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001128, l3: 0.011469, l4: 0.020063, l5: 0.031623, l6: 0.045090\n","\n","[epoch: 16178/100000, batch:    12/    1, ite: 16178] train loss: 0.119168, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001187, l3: 0.011499, l4: 0.020155, l5: 0.031818, l6: 0.046444\n","\n","[epoch: 16179/100000, batch:    12/    1, ite: 16179] train loss: 0.119123, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001110, l3: 0.011595, l4: 0.019982, l5: 0.033696, l6: 0.049129\n","\n","[epoch: 16180/100000, batch:    12/    1, ite: 16180] train loss: 0.119104, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001517, l3: 0.011748, l4: 0.019970, l5: 0.032590, l6: 0.054255\n","\n","[epoch: 16181/100000, batch:    12/    1, ite: 16181] train loss: 0.119110, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001137, l3: 0.011172, l4: 0.020500, l5: 0.031626, l6: 0.050363\n","\n","[epoch: 16182/100000, batch:    12/    1, ite: 16182] train loss: 0.119087, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001323, l3: 0.010810, l4: 0.019820, l5: 0.032394, l6: 0.066297\n","\n","[epoch: 16183/100000, batch:    12/    1, ite: 16183] train loss: 0.119151, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001448, l3: 0.011681, l4: 0.020520, l5: 0.031918, l6: 0.057155\n","\n","[epoch: 16184/100000, batch:    12/    1, ite: 16184] train loss: 0.119171, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001350, l3: 0.011678, l4: 0.019879, l5: 0.030226, l6: 0.056185\n","\n","[epoch: 16185/100000, batch:    12/    1, ite: 16185] train loss: 0.119173, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001443, l3: 0.011701, l4: 0.020200, l5: 0.031622, l6: 0.049848\n","\n","[epoch: 16186/100000, batch:    12/    1, ite: 16186] train loss: 0.119150, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001140, l3: 0.011676, l4: 0.019711, l5: 0.033585, l6: 0.048928\n","\n","[epoch: 16187/100000, batch:    12/    1, ite: 16187] train loss: 0.119129, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001112, l3: 0.011236, l4: 0.020447, l5: 0.033390, l6: 0.049618\n","\n","[epoch: 16188/100000, batch:    12/    1, ite: 16188] train loss: 0.119112, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001192, l3: 0.011268, l4: 0.020104, l5: 0.033422, l6: 0.062878\n","\n","[epoch: 16189/100000, batch:    12/    1, ite: 16189] train loss: 0.119164, tar: 0.000066 \n","l0: 0.000054, l1: 0.000058, l2: 0.001284, l3: 0.011603, l4: 0.019942, l5: 0.033347, l6: 0.048422\n","\n","[epoch: 16190/100000, batch:    12/    1, ite: 16190] train loss: 0.119141, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001165, l3: 0.011645, l4: 0.019742, l5: 0.032752, l6: 0.045885\n","\n","[epoch: 16191/100000, batch:    12/    1, ite: 16191] train loss: 0.119100, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001111, l3: 0.011616, l4: 0.019718, l5: 0.033704, l6: 0.048848\n","\n","[epoch: 16192/100000, batch:    12/    1, ite: 16192] train loss: 0.119079, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001143, l3: 0.011278, l4: 0.019512, l5: 0.029880, l6: 0.050484\n","\n","[epoch: 16193/100000, batch:    12/    1, ite: 16193] train loss: 0.119045, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001157, l3: 0.011120, l4: 0.019305, l5: 0.031734, l6: 0.043793\n","\n","[epoch: 16194/100000, batch:    12/    1, ite: 16194] train loss: 0.118984, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001213, l3: 0.011583, l4: 0.020131, l5: 0.032784, l6: 0.048914\n","\n","[epoch: 16195/100000, batch:    12/    1, ite: 16195] train loss: 0.118962, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001444, l3: 0.011404, l4: 0.019102, l5: 0.030810, l6: 0.052948\n","\n","[epoch: 16196/100000, batch:    12/    1, ite: 16196] train loss: 0.118946, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001193, l3: 0.011392, l4: 0.019850, l5: 0.033114, l6: 0.065445\n","\n","[epoch: 16197/100000, batch:    12/    1, ite: 16197] train loss: 0.119008, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001322, l3: 0.011445, l4: 0.020237, l5: 0.032445, l6: 0.054807\n","\n","[epoch: 16198/100000, batch:    12/    1, ite: 16198] train loss: 0.119015, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001090, l3: 0.011098, l4: 0.019322, l5: 0.031039, l6: 0.051274\n","\n","[epoch: 16199/100000, batch:    12/    1, ite: 16199] train loss: 0.118990, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001586, l3: 0.011614, l4: 0.020273, l5: 0.034164, l6: 0.064301\n","\n","[epoch: 16200/100000, batch:    12/    1, ite: 16200] train loss: 0.119055, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001294, l3: 0.011795, l4: 0.020369, l5: 0.031830, l6: 0.054344\n","\n","[epoch: 16201/100000, batch:    12/    1, ite: 16201] train loss: 0.119059, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001312, l3: 0.011079, l4: 0.019923, l5: 0.032791, l6: 0.047408\n","\n","[epoch: 16202/100000, batch:    12/    1, ite: 16202] train loss: 0.119027, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001334, l3: 0.011220, l4: 0.019628, l5: 0.030204, l6: 0.050752\n","\n","[epoch: 16203/100000, batch:    12/    1, ite: 16203] train loss: 0.118999, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001142, l3: 0.011480, l4: 0.020138, l5: 0.034155, l6: 0.064966\n","\n","[epoch: 16204/100000, batch:    12/    1, ite: 16204] train loss: 0.119062, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001346, l3: 0.011668, l4: 0.019959, l5: 0.030071, l6: 0.051349\n","\n","[epoch: 16205/100000, batch:    12/    1, ite: 16205] train loss: 0.119040, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001280, l3: 0.011175, l4: 0.020606, l5: 0.030579, l6: 0.050408\n","\n","[epoch: 16206/100000, batch:    12/    1, ite: 16206] train loss: 0.119017, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001456, l3: 0.011359, l4: 0.019054, l5: 0.030888, l6: 0.053151\n","\n","[epoch: 16207/100000, batch:    12/    1, ite: 16207] train loss: 0.119002, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001492, l3: 0.011139, l4: 0.020058, l5: 0.029890, l6: 0.050295\n","\n","[epoch: 16208/100000, batch:    12/    1, ite: 16208] train loss: 0.118973, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001427, l3: 0.011475, l4: 0.020058, l5: 0.033749, l6: 0.049456\n","\n","[epoch: 16209/100000, batch:    12/    1, ite: 16209] train loss: 0.118961, tar: 0.000066 \n","l0: 0.000060, l1: 0.000061, l2: 0.001393, l3: 0.010988, l4: 0.019798, l5: 0.033731, l6: 0.067991\n","\n","[epoch: 16210/100000, batch:    12/    1, ite: 16210] train loss: 0.119032, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001444, l3: 0.010911, l4: 0.019543, l5: 0.031186, l6: 0.042700\n","\n","[epoch: 16211/100000, batch:    12/    1, ite: 16211] train loss: 0.118970, tar: 0.000066 \n","l0: 0.000068, l1: 0.000072, l2: 0.001128, l3: 0.011620, l4: 0.019933, l5: 0.033847, l6: 0.065660\n","\n","[epoch: 16212/100000, batch:    12/    1, ite: 16212] train loss: 0.119033, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001138, l3: 0.011309, l4: 0.020698, l5: 0.031286, l6: 0.049625\n","\n","[epoch: 16213/100000, batch:    12/    1, ite: 16213] train loss: 0.119010, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001069, l3: 0.011071, l4: 0.019763, l5: 0.030113, l6: 0.044117\n","\n","[epoch: 16214/100000, batch:    12/    1, ite: 16214] train loss: 0.118951, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001442, l3: 0.011481, l4: 0.019581, l5: 0.032655, l6: 0.046256\n","\n","[epoch: 16215/100000, batch:    12/    1, ite: 16215] train loss: 0.118916, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001133, l3: 0.011296, l4: 0.020641, l5: 0.032503, l6: 0.047645\n","\n","[epoch: 16216/100000, batch:    12/    1, ite: 16216] train loss: 0.118891, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001146, l3: 0.011454, l4: 0.020172, l5: 0.031514, l6: 0.045455\n","\n","[epoch: 16217/100000, batch:    12/    1, ite: 16217] train loss: 0.118849, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001468, l3: 0.011485, l4: 0.019400, l5: 0.030072, l6: 0.049195\n","\n","[epoch: 16218/100000, batch:    12/    1, ite: 16218] train loss: 0.118817, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001145, l3: 0.011536, l4: 0.019982, l5: 0.031397, l6: 0.048119\n","\n","[epoch: 16219/100000, batch:    12/    1, ite: 16219] train loss: 0.118787, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001102, l3: 0.011205, l4: 0.020437, l5: 0.031125, l6: 0.055661\n","\n","[epoch: 16220/100000, batch:    12/    1, ite: 16220] train loss: 0.118791, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001187, l3: 0.011127, l4: 0.019660, l5: 0.032105, l6: 0.052813\n","\n","[epoch: 16221/100000, batch:    12/    1, ite: 16221] train loss: 0.118783, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001095, l3: 0.011453, l4: 0.019494, l5: 0.029841, l6: 0.052667\n","\n","[epoch: 16222/100000, batch:    12/    1, ite: 16222] train loss: 0.118764, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001279, l3: 0.011144, l4: 0.020129, l5: 0.032090, l6: 0.048046\n","\n","[epoch: 16223/100000, batch:    12/    1, ite: 16223] train loss: 0.118738, tar: 0.000066 \n","l0: 0.000071, l1: 0.000075, l2: 0.001261, l3: 0.010997, l4: 0.020287, l5: 0.033350, l6: 0.066929\n","\n","[epoch: 16224/100000, batch:    12/    1, ite: 16224] train loss: 0.118801, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001352, l3: 0.011277, l4: 0.020393, l5: 0.033428, l6: 0.060671\n","\n","[epoch: 16225/100000, batch:    12/    1, ite: 16225] train loss: 0.118839, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001183, l3: 0.011447, l4: 0.020068, l5: 0.034628, l6: 0.063080\n","\n","[epoch: 16226/100000, batch:    12/    1, ite: 16226] train loss: 0.118890, tar: 0.000066 \n","l0: 0.000064, l1: 0.000069, l2: 0.001223, l3: 0.011369, l4: 0.020342, l5: 0.034179, l6: 0.062333\n","\n","[epoch: 16227/100000, batch:    12/    1, ite: 16227] train loss: 0.118938, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001138, l3: 0.011545, l4: 0.020150, l5: 0.030796, l6: 0.052572\n","\n","[epoch: 16228/100000, batch:    12/    1, ite: 16228] train loss: 0.118926, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001100, l3: 0.011299, l4: 0.020447, l5: 0.033461, l6: 0.048649\n","\n","[epoch: 16229/100000, batch:    12/    1, ite: 16229] train loss: 0.118909, tar: 0.000066 \n","l0: 0.000072, l1: 0.000076, l2: 0.001131, l3: 0.011687, l4: 0.020505, l5: 0.034873, l6: 0.066340\n","\n","[epoch: 16230/100000, batch:    12/    1, ite: 16230] train loss: 0.118978, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001145, l3: 0.011115, l4: 0.019929, l5: 0.031891, l6: 0.053454\n","\n","[epoch: 16231/100000, batch:    12/    1, ite: 16231] train loss: 0.118972, tar: 0.000066 \n","l0: 0.000063, l1: 0.000062, l2: 0.001047, l3: 0.011273, l4: 0.019385, l5: 0.032587, l6: 0.064315\n","\n","[epoch: 16232/100000, batch:    12/    1, ite: 16232] train loss: 0.119014, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001544, l3: 0.011551, l4: 0.020167, l5: 0.034165, l6: 0.063897\n","\n","[epoch: 16233/100000, batch:    12/    1, ite: 16233] train loss: 0.119068, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001452, l3: 0.010928, l4: 0.019486, l5: 0.031476, l6: 0.048079\n","\n","[epoch: 16234/100000, batch:    12/    1, ite: 16234] train loss: 0.119036, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001112, l3: 0.011716, l4: 0.020634, l5: 0.032428, l6: 0.050442\n","\n","[epoch: 16235/100000, batch:    12/    1, ite: 16235] train loss: 0.119025, tar: 0.000066 \n","l0: 0.000073, l1: 0.000069, l2: 0.001487, l3: 0.011161, l4: 0.019862, l5: 0.031991, l6: 0.052720\n","\n","[epoch: 16236/100000, batch:    12/    1, ite: 16236] train loss: 0.119018, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001220, l3: 0.011176, l4: 0.020051, l5: 0.033310, l6: 0.063388\n","\n","[epoch: 16237/100000, batch:    12/    1, ite: 16237] train loss: 0.119061, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001103, l3: 0.011359, l4: 0.019877, l5: 0.033885, l6: 0.049171\n","\n","[epoch: 16238/100000, batch:    12/    1, ite: 16238] train loss: 0.119046, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001097, l3: 0.011165, l4: 0.019465, l5: 0.029477, l6: 0.049596\n","\n","[epoch: 16239/100000, batch:    12/    1, ite: 16239] train loss: 0.119012, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001548, l3: 0.011672, l4: 0.020356, l5: 0.033970, l6: 0.064035\n","\n","[epoch: 16240/100000, batch:    12/    1, ite: 16240] train loss: 0.119065, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001260, l3: 0.011750, l4: 0.020599, l5: 0.033778, l6: 0.059686\n","\n","[epoch: 16241/100000, batch:    12/    1, ite: 16241] train loss: 0.119099, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001106, l3: 0.011218, l4: 0.020463, l5: 0.032981, l6: 0.048314\n","\n","[epoch: 16242/100000, batch:    12/    1, ite: 16242] train loss: 0.119079, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001346, l3: 0.011112, l4: 0.019925, l5: 0.032384, l6: 0.053395\n","\n","[epoch: 16243/100000, batch:    12/    1, ite: 16243] train loss: 0.119075, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001159, l3: 0.011666, l4: 0.020133, l5: 0.031868, l6: 0.052981\n","\n","[epoch: 16244/100000, batch:    12/    1, ite: 16244] train loss: 0.119071, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001291, l3: 0.011524, l4: 0.020445, l5: 0.033163, l6: 0.060769\n","\n","[epoch: 16245/100000, batch:    12/    1, ite: 16245] train loss: 0.119104, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001484, l3: 0.011062, l4: 0.020086, l5: 0.029927, l6: 0.050145\n","\n","[epoch: 16246/100000, batch:    12/    1, ite: 16246] train loss: 0.119079, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001359, l3: 0.010908, l4: 0.020408, l5: 0.033528, l6: 0.066836\n","\n","[epoch: 16247/100000, batch:    12/    1, ite: 16247] train loss: 0.119136, tar: 0.000066 \n","l0: 0.000059, l1: 0.000063, l2: 0.001299, l3: 0.011103, l4: 0.020302, l5: 0.033245, l6: 0.049402\n","\n","[epoch: 16248/100000, batch:    12/    1, ite: 16248] train loss: 0.119121, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001165, l3: 0.011647, l4: 0.019985, l5: 0.033766, l6: 0.065972\n","\n","[epoch: 16249/100000, batch:    12/    1, ite: 16249] train loss: 0.119176, tar: 0.000066 \n","l0: 0.000067, l1: 0.000071, l2: 0.001514, l3: 0.010872, l4: 0.019191, l5: 0.030907, l6: 0.048589\n","\n","[epoch: 16250/100000, batch:    12/    1, ite: 16250] train loss: 0.119144, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001143, l3: 0.011605, l4: 0.020185, l5: 0.031328, l6: 0.052267\n","\n","[epoch: 16251/100000, batch:    12/    1, ite: 16251] train loss: 0.119134, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001494, l3: 0.011128, l4: 0.019853, l5: 0.032260, l6: 0.046744\n","\n","[epoch: 16252/100000, batch:    12/    1, ite: 16252] train loss: 0.119104, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001165, l3: 0.011297, l4: 0.020530, l5: 0.031829, l6: 0.050421\n","\n","[epoch: 16253/100000, batch:    12/    1, ite: 16253] train loss: 0.119089, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001534, l3: 0.011145, l4: 0.019867, l5: 0.032568, l6: 0.067168\n","\n","[epoch: 16254/100000, batch:    12/    1, ite: 16254] train loss: 0.119142, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001163, l3: 0.011520, l4: 0.020464, l5: 0.032688, l6: 0.049291\n","\n","[epoch: 16255/100000, batch:    12/    1, ite: 16255] train loss: 0.119127, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001446, l3: 0.011104, l4: 0.020221, l5: 0.030230, l6: 0.048329\n","\n","[epoch: 16256/100000, batch:    12/    1, ite: 16256] train loss: 0.119097, tar: 0.000066 \n","l0: 0.000059, l1: 0.000060, l2: 0.001412, l3: 0.011101, l4: 0.019891, l5: 0.033888, l6: 0.067730\n","\n","[epoch: 16257/100000, batch:    12/    1, ite: 16257] train loss: 0.119155, tar: 0.000066 \n","l0: 0.000058, l1: 0.000059, l2: 0.001260, l3: 0.011662, l4: 0.019901, l5: 0.031513, l6: 0.051913\n","\n","[epoch: 16258/100000, batch:    12/    1, ite: 16258] train loss: 0.119144, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001109, l3: 0.011424, l4: 0.019522, l5: 0.030108, l6: 0.046957\n","\n","[epoch: 16259/100000, batch:    12/    1, ite: 16259] train loss: 0.119106, tar: 0.000066 \n","l0: 0.000058, l1: 0.000059, l2: 0.001467, l3: 0.011330, l4: 0.019898, l5: 0.031837, l6: 0.044015\n","\n","[epoch: 16260/100000, batch:    12/    1, ite: 16260] train loss: 0.119066, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001259, l3: 0.010746, l4: 0.019767, l5: 0.032732, l6: 0.066285\n","\n","[epoch: 16261/100000, batch:    12/    1, ite: 16261] train loss: 0.119111, tar: 0.000066 \n","l0: 0.000062, l1: 0.000064, l2: 0.001397, l3: 0.011479, l4: 0.019654, l5: 0.033226, l6: 0.047677\n","\n","[epoch: 16262/100000, batch:    12/    1, ite: 16262] train loss: 0.119090, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001223, l3: 0.011427, l4: 0.020051, l5: 0.033971, l6: 0.065434\n","\n","[epoch: 16263/100000, batch:    12/    1, ite: 16263] train loss: 0.119140, tar: 0.000066 \n","l0: 0.000068, l1: 0.000067, l2: 0.001107, l3: 0.011357, l4: 0.019948, l5: 0.030201, l6: 0.049947\n","\n","[epoch: 16264/100000, batch:    12/    1, ite: 16264] train loss: 0.119116, tar: 0.000066 \n","l0: 0.000061, l1: 0.000062, l2: 0.001439, l3: 0.011375, l4: 0.019139, l5: 0.031022, l6: 0.052968\n","\n","[epoch: 16265/100000, batch:    12/    1, ite: 16265] train loss: 0.119104, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001101, l3: 0.011061, l4: 0.019976, l5: 0.031792, l6: 0.053516\n","\n","[epoch: 16266/100000, batch:    12/    1, ite: 16266] train loss: 0.119099, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001472, l3: 0.011621, l4: 0.019844, l5: 0.031321, l6: 0.060375\n","\n","[epoch: 16267/100000, batch:    12/    1, ite: 16267] train loss: 0.119120, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001288, l3: 0.011769, l4: 0.020727, l5: 0.033633, l6: 0.059528\n","\n","[epoch: 16268/100000, batch:    12/    1, ite: 16268] train loss: 0.119149, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001096, l3: 0.011325, l4: 0.019939, l5: 0.033793, l6: 0.049216\n","\n","[epoch: 16269/100000, batch:    12/    1, ite: 16269] train loss: 0.119136, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001150, l3: 0.011476, l4: 0.019981, l5: 0.033988, l6: 0.065436\n","\n","[epoch: 16270/100000, batch:    12/    1, ite: 16270] train loss: 0.119184, tar: 0.000066 \n","l0: 0.000072, l1: 0.000071, l2: 0.001307, l3: 0.011509, l4: 0.020517, l5: 0.031584, l6: 0.057147\n","\n","[epoch: 16271/100000, batch:    12/    1, ite: 16271] train loss: 0.119195, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001236, l3: 0.011227, l4: 0.019679, l5: 0.031226, l6: 0.052306\n","\n","[epoch: 16272/100000, batch:    12/    1, ite: 16272] train loss: 0.119183, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001467, l3: 0.011069, l4: 0.019836, l5: 0.029966, l6: 0.051357\n","\n","[epoch: 16273/100000, batch:    12/    1, ite: 16273] train loss: 0.119163, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001143, l3: 0.011843, l4: 0.020157, l5: 0.030606, l6: 0.057882\n","\n","[epoch: 16274/100000, batch:    12/    1, ite: 16274] train loss: 0.119173, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001291, l3: 0.011048, l4: 0.019707, l5: 0.032103, l6: 0.045964\n","\n","[epoch: 16275/100000, batch:    12/    1, ite: 16275] train loss: 0.119140, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001192, l3: 0.011321, l4: 0.020402, l5: 0.034655, l6: 0.062353\n","\n","[epoch: 16276/100000, batch:    12/    1, ite: 16276] train loss: 0.119180, tar: 0.000066 \n","l0: 0.000069, l1: 0.000068, l2: 0.001205, l3: 0.011579, l4: 0.020632, l5: 0.031792, l6: 0.047708\n","\n","[epoch: 16277/100000, batch:    12/    1, ite: 16277] train loss: 0.119158, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001190, l3: 0.011696, l4: 0.020459, l5: 0.035219, l6: 0.066286\n","\n","[epoch: 16278/100000, batch:    12/    1, ite: 16278] train loss: 0.119215, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001296, l3: 0.010849, l4: 0.020291, l5: 0.033618, l6: 0.067216\n","\n","[epoch: 16279/100000, batch:    12/    1, ite: 16279] train loss: 0.119265, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001117, l3: 0.011495, l4: 0.020110, l5: 0.030755, l6: 0.046139\n","\n","[epoch: 16280/100000, batch:    12/    1, ite: 16280] train loss: 0.119231, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001261, l3: 0.011096, l4: 0.020205, l5: 0.032200, l6: 0.048663\n","\n","[epoch: 16281/100000, batch:    12/    1, ite: 16281] train loss: 0.119211, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001274, l3: 0.011081, l4: 0.020211, l5: 0.033045, l6: 0.066696\n","\n","[epoch: 16282/100000, batch:    12/    1, ite: 16282] train loss: 0.119258, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001467, l3: 0.011054, l4: 0.020108, l5: 0.032388, l6: 0.054027\n","\n","[epoch: 16283/100000, batch:    12/    1, ite: 16283] train loss: 0.119258, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001133, l3: 0.011566, l4: 0.020355, l5: 0.035018, l6: 0.066137\n","\n","[epoch: 16284/100000, batch:    12/    1, ite: 16284] train loss: 0.119311, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001194, l3: 0.011580, l4: 0.019477, l5: 0.031905, l6: 0.046879\n","\n","[epoch: 16285/100000, batch:    12/    1, ite: 16285] train loss: 0.119282, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001504, l3: 0.011589, l4: 0.020041, l5: 0.033455, l6: 0.048430\n","\n","[epoch: 16286/100000, batch:    12/    1, ite: 16286] train loss: 0.119268, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001360, l3: 0.011001, l4: 0.020097, l5: 0.029476, l6: 0.051249\n","\n","[epoch: 16287/100000, batch:    12/    1, ite: 16287] train loss: 0.119247, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001300, l3: 0.011319, l4: 0.019388, l5: 0.032478, l6: 0.045360\n","\n","[epoch: 16288/100000, batch:    12/    1, ite: 16288] train loss: 0.119215, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001245, l3: 0.011362, l4: 0.019243, l5: 0.029182, l6: 0.048578\n","\n","[epoch: 16289/100000, batch:    12/    1, ite: 16289] train loss: 0.119182, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001282, l3: 0.011515, l4: 0.020318, l5: 0.032105, l6: 0.050143\n","\n","[epoch: 16290/100000, batch:    12/    1, ite: 16290] train loss: 0.119169, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001140, l3: 0.011694, l4: 0.019635, l5: 0.032897, l6: 0.046373\n","\n","[epoch: 16291/100000, batch:    12/    1, ite: 16291] train loss: 0.119144, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001488, l3: 0.011684, l4: 0.020857, l5: 0.033322, l6: 0.059926\n","\n","[epoch: 16292/100000, batch:    12/    1, ite: 16292] train loss: 0.119173, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001516, l3: 0.011040, l4: 0.020100, l5: 0.034203, l6: 0.066619\n","\n","[epoch: 16293/100000, batch:    12/    1, ite: 16293] train loss: 0.119222, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001354, l3: 0.011287, l4: 0.019872, l5: 0.031862, l6: 0.050092\n","\n","[epoch: 16294/100000, batch:    12/    1, ite: 16294] train loss: 0.119206, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001506, l3: 0.011149, l4: 0.019848, l5: 0.032950, l6: 0.062630\n","\n","[epoch: 16295/100000, batch:    12/    1, ite: 16295] train loss: 0.119237, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001201, l3: 0.011747, l4: 0.019956, l5: 0.033350, l6: 0.065716\n","\n","[epoch: 16296/100000, batch:    12/    1, ite: 16296] train loss: 0.119280, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001117, l3: 0.011588, l4: 0.020057, l5: 0.031262, l6: 0.052318\n","\n","[epoch: 16297/100000, batch:    12/    1, ite: 16297] train loss: 0.119271, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001504, l3: 0.011121, l4: 0.020572, l5: 0.032791, l6: 0.049887\n","\n","[epoch: 16298/100000, batch:    12/    1, ite: 16298] train loss: 0.119260, tar: 0.000066 \n","l0: 0.000059, l1: 0.000058, l2: 0.001297, l3: 0.010995, l4: 0.019471, l5: 0.031580, l6: 0.048802\n","\n","[epoch: 16299/100000, batch:    12/    1, ite: 16299] train loss: 0.119236, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001599, l3: 0.011002, l4: 0.019108, l5: 0.032624, l6: 0.064381\n","\n","[epoch: 16300/100000, batch:    12/    1, ite: 16300] train loss: 0.119269, tar: 0.000066 \n","l0: 0.000052, l1: 0.000053, l2: 0.001220, l3: 0.011263, l4: 0.018952, l5: 0.031607, l6: 0.043040\n","\n","[epoch: 16301/100000, batch:    12/    1, ite: 16301] train loss: 0.119225, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001237, l3: 0.010844, l4: 0.019652, l5: 0.032810, l6: 0.064695\n","\n","[epoch: 16302/100000, batch:    12/    1, ite: 16302] train loss: 0.119259, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001125, l3: 0.011399, l4: 0.020557, l5: 0.032686, l6: 0.057336\n","\n","[epoch: 16303/100000, batch:    12/    1, ite: 16303] train loss: 0.119272, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001570, l3: 0.011117, l4: 0.019680, l5: 0.032263, l6: 0.062943\n","\n","[epoch: 16304/100000, batch:    12/    1, ite: 16304] train loss: 0.119300, tar: 0.000066 \n","l0: 0.000070, l1: 0.000069, l2: 0.001563, l3: 0.011647, l4: 0.020149, l5: 0.033876, l6: 0.064194\n","\n","[epoch: 16305/100000, batch:    12/    1, ite: 16305] train loss: 0.119340, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001300, l3: 0.011381, l4: 0.019043, l5: 0.030175, l6: 0.047653\n","\n","[epoch: 16306/100000, batch:    12/    1, ite: 16306] train loss: 0.119308, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001449, l3: 0.011695, l4: 0.020306, l5: 0.032893, l6: 0.049792\n","\n","[epoch: 16307/100000, batch:    12/    1, ite: 16307] train loss: 0.119298, tar: 0.000066 \n","l0: 0.000069, l1: 0.000070, l2: 0.001480, l3: 0.011641, l4: 0.020112, l5: 0.031169, l6: 0.051287\n","\n","[epoch: 16308/100000, batch:    12/    1, ite: 16308] train loss: 0.119287, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001464, l3: 0.011767, l4: 0.020635, l5: 0.033218, l6: 0.058524\n","\n","[epoch: 16309/100000, batch:    12/    1, ite: 16309] train loss: 0.119308, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001151, l3: 0.011637, l4: 0.019951, l5: 0.032369, l6: 0.053565\n","\n","[epoch: 16310/100000, batch:    12/    1, ite: 16310] train loss: 0.119306, tar: 0.000066 \n","l0: 0.000069, l1: 0.000068, l2: 0.001231, l3: 0.011355, l4: 0.019988, l5: 0.032942, l6: 0.064658\n","\n","[epoch: 16311/100000, batch:    12/    1, ite: 16311] train loss: 0.119342, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001590, l3: 0.011329, l4: 0.020497, l5: 0.033815, l6: 0.060765\n","\n","[epoch: 16312/100000, batch:    12/    1, ite: 16312] train loss: 0.119370, tar: 0.000066 \n","l0: 0.000076, l1: 0.000073, l2: 0.001414, l3: 0.011111, l4: 0.020610, l5: 0.030694, l6: 0.050781\n","\n","[epoch: 16313/100000, batch:    12/    1, ite: 16313] train loss: 0.119355, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001324, l3: 0.011744, l4: 0.019872, l5: 0.031871, l6: 0.052534\n","\n","[epoch: 16314/100000, batch:    12/    1, ite: 16314] train loss: 0.119349, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001125, l3: 0.011090, l4: 0.019714, l5: 0.031854, l6: 0.052493\n","\n","[epoch: 16315/100000, batch:    12/    1, ite: 16315] train loss: 0.119340, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001370, l3: 0.011591, l4: 0.020109, l5: 0.032998, l6: 0.048445\n","\n","[epoch: 16316/100000, batch:    12/    1, ite: 16316] train loss: 0.119325, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001479, l3: 0.011718, l4: 0.020027, l5: 0.030456, l6: 0.052173\n","\n","[epoch: 16317/100000, batch:    12/    1, ite: 16317] train loss: 0.119314, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001421, l3: 0.010909, l4: 0.019269, l5: 0.032270, l6: 0.064258\n","\n","[epoch: 16318/100000, batch:    12/    1, ite: 16318] train loss: 0.119342, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001146, l3: 0.011662, l4: 0.020138, l5: 0.034115, l6: 0.048289\n","\n","[epoch: 16319/100000, batch:    12/    1, ite: 16319] train loss: 0.119330, tar: 0.000066 \n","l0: 0.000072, l1: 0.000073, l2: 0.001364, l3: 0.011106, l4: 0.020094, l5: 0.030071, l6: 0.054452\n","\n","[epoch: 16320/100000, batch:    12/    1, ite: 16320] train loss: 0.119324, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001478, l3: 0.011096, l4: 0.020078, l5: 0.031205, l6: 0.052476\n","\n","[epoch: 16321/100000, batch:    12/    1, ite: 16321] train loss: 0.119315, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001172, l3: 0.011337, l4: 0.020766, l5: 0.030957, l6: 0.049101\n","\n","[epoch: 16322/100000, batch:    12/    1, ite: 16322] train loss: 0.119297, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001142, l3: 0.011350, l4: 0.019417, l5: 0.032669, l6: 0.045431\n","\n","[epoch: 16323/100000, batch:    12/    1, ite: 16323] train loss: 0.119268, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001130, l3: 0.011484, l4: 0.020023, l5: 0.031515, l6: 0.048164\n","\n","[epoch: 16324/100000, batch:    12/    1, ite: 16324] train loss: 0.119247, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001106, l3: 0.011578, l4: 0.020928, l5: 0.031631, l6: 0.048930\n","\n","[epoch: 16325/100000, batch:    12/    1, ite: 16325] train loss: 0.119232, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001520, l3: 0.011259, l4: 0.020730, l5: 0.034545, l6: 0.062821\n","\n","[epoch: 16326/100000, batch:    12/    1, ite: 16326] train loss: 0.119268, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001088, l3: 0.011283, l4: 0.020382, l5: 0.033494, l6: 0.048949\n","\n","[epoch: 16327/100000, batch:    12/    1, ite: 16327] train loss: 0.119256, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001142, l3: 0.011359, l4: 0.019672, l5: 0.030965, l6: 0.043982\n","\n","[epoch: 16328/100000, batch:    12/    1, ite: 16328] train loss: 0.119220, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001146, l3: 0.011414, l4: 0.019724, l5: 0.031759, l6: 0.046815\n","\n","[epoch: 16329/100000, batch:    12/    1, ite: 16329] train loss: 0.119195, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001305, l3: 0.011741, l4: 0.019928, l5: 0.033765, l6: 0.048422\n","\n","[epoch: 16330/100000, batch:    12/    1, ite: 16330] train loss: 0.119183, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001464, l3: 0.011261, l4: 0.020014, l5: 0.030299, l6: 0.043629\n","\n","[epoch: 16331/100000, batch:    12/    1, ite: 16331] train loss: 0.119145, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001390, l3: 0.011140, l4: 0.020227, l5: 0.031295, l6: 0.051165\n","\n","[epoch: 16332/100000, batch:    12/    1, ite: 16332] train loss: 0.119134, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001429, l3: 0.011516, l4: 0.020513, l5: 0.033029, l6: 0.050156\n","\n","[epoch: 16333/100000, batch:    12/    1, ite: 16333] train loss: 0.119127, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001451, l3: 0.011101, l4: 0.021657, l5: 0.032944, l6: 0.049691\n","\n","[epoch: 16334/100000, batch:    12/    1, ite: 16334] train loss: 0.119120, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001423, l3: 0.011381, l4: 0.019629, l5: 0.032738, l6: 0.064525\n","\n","[epoch: 16335/100000, batch:    12/    1, ite: 16335] train loss: 0.119152, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001160, l3: 0.011575, l4: 0.019940, l5: 0.033555, l6: 0.048373\n","\n","[epoch: 16336/100000, batch:    12/    1, ite: 16336] train loss: 0.119139, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001527, l3: 0.011129, l4: 0.019940, l5: 0.032823, l6: 0.062685\n","\n","[epoch: 16337/100000, batch:    12/    1, ite: 16337] train loss: 0.119166, tar: 0.000066 \n","l0: 0.000073, l1: 0.000069, l2: 0.001567, l3: 0.011354, l4: 0.020530, l5: 0.031710, l6: 0.053973\n","\n","[epoch: 16338/100000, batch:    12/    1, ite: 16338] train loss: 0.119166, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001481, l3: 0.011019, l4: 0.019736, l5: 0.030283, l6: 0.045709\n","\n","[epoch: 16339/100000, batch:    12/    1, ite: 16339] train loss: 0.119135, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001237, l3: 0.011654, l4: 0.020013, l5: 0.033293, l6: 0.065768\n","\n","[epoch: 16340/100000, batch:    12/    1, ite: 16340] train loss: 0.119173, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001545, l3: 0.011015, l4: 0.020366, l5: 0.034397, l6: 0.066217\n","\n","[epoch: 16341/100000, batch:    12/    1, ite: 16341] train loss: 0.119215, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001459, l3: 0.011549, l4: 0.019654, l5: 0.029534, l6: 0.051176\n","\n","[epoch: 16342/100000, batch:    12/    1, ite: 16342] train loss: 0.119199, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001122, l3: 0.011112, l4: 0.020766, l5: 0.032023, l6: 0.051477\n","\n","[epoch: 16343/100000, batch:    12/    1, ite: 16343] train loss: 0.119191, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001719, l3: 0.011299, l4: 0.020353, l5: 0.033428, l6: 0.067610\n","\n","[epoch: 16344/100000, batch:    12/    1, ite: 16344] train loss: 0.119236, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001434, l3: 0.011710, l4: 0.020253, l5: 0.034071, l6: 0.064177\n","\n","[epoch: 16345/100000, batch:    12/    1, ite: 16345] train loss: 0.119272, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001603, l3: 0.011359, l4: 0.020203, l5: 0.032949, l6: 0.049455\n","\n","[epoch: 16346/100000, batch:    12/    1, ite: 16346] train loss: 0.119262, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001112, l3: 0.011138, l4: 0.020012, l5: 0.029638, l6: 0.049568\n","\n","[epoch: 16347/100000, batch:    12/    1, ite: 16347] train loss: 0.119240, tar: 0.000066 \n","l0: 0.000062, l1: 0.000066, l2: 0.001163, l3: 0.011386, l4: 0.020010, l5: 0.031673, l6: 0.046123\n","\n","[epoch: 16348/100000, batch:    12/    1, ite: 16348] train loss: 0.119214, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001303, l3: 0.011512, l4: 0.020473, l5: 0.032915, l6: 0.050067\n","\n","[epoch: 16349/100000, batch:    12/    1, ite: 16349] train loss: 0.119206, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001309, l3: 0.010790, l4: 0.019176, l5: 0.030734, l6: 0.048067\n","\n","[epoch: 16350/100000, batch:    12/    1, ite: 16350] train loss: 0.119181, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001146, l3: 0.011595, l4: 0.020371, l5: 0.032983, l6: 0.049054\n","\n","[epoch: 16351/100000, batch:    12/    1, ite: 16351] train loss: 0.119170, tar: 0.000066 \n","l0: 0.000063, l1: 0.000068, l2: 0.001421, l3: 0.011252, l4: 0.019793, l5: 0.032015, l6: 0.053337\n","\n","[epoch: 16352/100000, batch:    12/    1, ite: 16352] train loss: 0.119166, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001455, l3: 0.011193, l4: 0.020280, l5: 0.033719, l6: 0.050096\n","\n","[epoch: 16353/100000, batch:    12/    1, ite: 16353] train loss: 0.119160, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001498, l3: 0.011032, l4: 0.020271, l5: 0.031502, l6: 0.051287\n","\n","[epoch: 16354/100000, batch:    12/    1, ite: 16354] train loss: 0.119150, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001332, l3: 0.011254, l4: 0.020232, l5: 0.034277, l6: 0.063111\n","\n","[epoch: 16355/100000, batch:    12/    1, ite: 16355] train loss: 0.119181, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001433, l3: 0.011544, l4: 0.020359, l5: 0.031461, l6: 0.047103\n","\n","[epoch: 16356/100000, batch:    12/    1, ite: 16356] train loss: 0.119161, tar: 0.000066 \n","l0: 0.000060, l1: 0.000059, l2: 0.001394, l3: 0.011166, l4: 0.020244, l5: 0.032844, l6: 0.051316\n","\n","[epoch: 16357/100000, batch:    12/    1, ite: 16357] train loss: 0.119155, tar: 0.000066 \n","l0: 0.000060, l1: 0.000059, l2: 0.001476, l3: 0.011101, l4: 0.020175, l5: 0.031086, l6: 0.050897\n","\n","[epoch: 16358/100000, batch:    12/    1, ite: 16358] train loss: 0.119143, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001359, l3: 0.011683, l4: 0.020347, l5: 0.035109, l6: 0.064867\n","\n","[epoch: 16359/100000, batch:    12/    1, ite: 16359] train loss: 0.119183, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001259, l3: 0.011824, l4: 0.021104, l5: 0.033025, l6: 0.063219\n","\n","[epoch: 16360/100000, batch:    12/    1, ite: 16360] train loss: 0.119215, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001555, l3: 0.011105, l4: 0.019654, l5: 0.031750, l6: 0.049766\n","\n","[epoch: 16361/100000, batch:    12/    1, ite: 16361] train loss: 0.119200, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001143, l3: 0.011429, l4: 0.021463, l5: 0.032106, l6: 0.060014\n","\n","[epoch: 16362/100000, batch:    12/    1, ite: 16362] train loss: 0.119220, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001133, l3: 0.011200, l4: 0.020388, l5: 0.031390, l6: 0.057778\n","\n","[epoch: 16363/100000, batch:    12/    1, ite: 16363] train loss: 0.119228, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001379, l3: 0.011283, l4: 0.020017, l5: 0.033768, l6: 0.064074\n","\n","[epoch: 16364/100000, batch:    12/    1, ite: 16364] train loss: 0.119259, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001238, l3: 0.011064, l4: 0.019518, l5: 0.032705, l6: 0.045818\n","\n","[epoch: 16365/100000, batch:    12/    1, ite: 16365] train loss: 0.119235, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001370, l3: 0.011158, l4: 0.020305, l5: 0.032243, l6: 0.055513\n","\n","[epoch: 16366/100000, batch:    12/    1, ite: 16366] train loss: 0.119239, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001558, l3: 0.011124, l4: 0.020147, l5: 0.029524, l6: 0.050797\n","\n","[epoch: 16367/100000, batch:    12/    1, ite: 16367] train loss: 0.119223, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001728, l3: 0.011731, l4: 0.020424, l5: 0.031904, l6: 0.057206\n","\n","[epoch: 16368/100000, batch:    12/    1, ite: 16368] train loss: 0.119233, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001250, l3: 0.011414, l4: 0.019418, l5: 0.031043, l6: 0.052344\n","\n","[epoch: 16369/100000, batch:    12/    1, ite: 16369] train loss: 0.119224, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001340, l3: 0.011506, l4: 0.019800, l5: 0.032949, l6: 0.063783\n","\n","[epoch: 16370/100000, batch:    12/    1, ite: 16370] train loss: 0.119251, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001168, l3: 0.011597, l4: 0.020800, l5: 0.032314, l6: 0.048468\n","\n","[epoch: 16371/100000, batch:    12/    1, ite: 16371] train loss: 0.119239, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001528, l3: 0.011137, l4: 0.020292, l5: 0.034434, l6: 0.067312\n","\n","[epoch: 16372/100000, batch:    12/    1, ite: 16372] train loss: 0.119280, tar: 0.000066 \n","l0: 0.000075, l1: 0.000075, l2: 0.001732, l3: 0.011202, l4: 0.020615, l5: 0.034277, l6: 0.066123\n","\n","[epoch: 16373/100000, batch:    12/    1, ite: 16373] train loss: 0.119320, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001212, l3: 0.012029, l4: 0.020524, l5: 0.032538, l6: 0.063057\n","\n","[epoch: 16374/100000, batch:    12/    1, ite: 16374] train loss: 0.119347, tar: 0.000066 \n","l0: 0.000064, l1: 0.000066, l2: 0.001606, l3: 0.011032, l4: 0.020128, l5: 0.033431, l6: 0.067260\n","\n","[epoch: 16375/100000, batch:    12/    1, ite: 16375] train loss: 0.119385, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001206, l3: 0.011929, l4: 0.021300, l5: 0.033518, l6: 0.063379\n","\n","[epoch: 16376/100000, batch:    12/    1, ite: 16376] train loss: 0.119417, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001508, l3: 0.011099, l4: 0.020601, l5: 0.030753, l6: 0.051110\n","\n","[epoch: 16377/100000, batch:    12/    1, ite: 16377] train loss: 0.119406, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001397, l3: 0.011339, l4: 0.020703, l5: 0.030649, l6: 0.050677\n","\n","[epoch: 16378/100000, batch:    12/    1, ite: 16378] train loss: 0.119394, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001262, l3: 0.011260, l4: 0.019545, l5: 0.032657, l6: 0.063502\n","\n","[epoch: 16379/100000, batch:    12/    1, ite: 16379] train loss: 0.119418, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001173, l3: 0.011784, l4: 0.020540, l5: 0.032496, l6: 0.062576\n","\n","[epoch: 16380/100000, batch:    12/    1, ite: 16380] train loss: 0.119443, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001453, l3: 0.011649, l4: 0.020170, l5: 0.031230, l6: 0.045793\n","\n","[epoch: 16381/100000, batch:    12/    1, ite: 16381] train loss: 0.119419, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001422, l3: 0.011299, l4: 0.020427, l5: 0.033494, l6: 0.061565\n","\n","[epoch: 16382/100000, batch:    12/    1, ite: 16382] train loss: 0.119442, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001199, l3: 0.011548, l4: 0.019558, l5: 0.032278, l6: 0.044627\n","\n","[epoch: 16383/100000, batch:    12/    1, ite: 16383] train loss: 0.119416, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001386, l3: 0.011711, l4: 0.020414, l5: 0.032571, l6: 0.056837\n","\n","[epoch: 16384/100000, batch:    12/    1, ite: 16384] train loss: 0.119425, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001458, l3: 0.011816, l4: 0.020095, l5: 0.031600, l6: 0.051009\n","\n","[epoch: 16385/100000, batch:    12/    1, ite: 16385] train loss: 0.119417, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001218, l3: 0.011567, l4: 0.019955, l5: 0.031272, l6: 0.051720\n","\n","[epoch: 16386/100000, batch:    12/    1, ite: 16386] train loss: 0.119407, tar: 0.000066 \n","l0: 0.000054, l1: 0.000058, l2: 0.001293, l3: 0.011148, l4: 0.019981, l5: 0.032816, l6: 0.046421\n","\n","[epoch: 16387/100000, batch:    12/    1, ite: 16387] train loss: 0.119388, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001158, l3: 0.011305, l4: 0.020617, l5: 0.032664, l6: 0.059037\n","\n","[epoch: 16388/100000, batch:    12/    1, ite: 16388] train loss: 0.119402, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001317, l3: 0.011117, l4: 0.020396, l5: 0.031051, l6: 0.051274\n","\n","[epoch: 16389/100000, batch:    12/    1, ite: 16389] train loss: 0.119391, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001519, l3: 0.011624, l4: 0.019489, l5: 0.031946, l6: 0.045553\n","\n","[epoch: 16390/100000, batch:    12/    1, ite: 16390] train loss: 0.119368, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001297, l3: 0.011343, l4: 0.020193, l5: 0.034020, l6: 0.063045\n","\n","[epoch: 16391/100000, batch:    12/    1, ite: 16391] train loss: 0.119395, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001156, l3: 0.011203, l4: 0.020162, l5: 0.031476, l6: 0.049972\n","\n","[epoch: 16392/100000, batch:    12/    1, ite: 16392] train loss: 0.119382, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001143, l3: 0.011242, l4: 0.019905, l5: 0.031982, l6: 0.052999\n","\n","[epoch: 16393/100000, batch:    12/    1, ite: 16393] train loss: 0.119377, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001512, l3: 0.011169, l4: 0.020184, l5: 0.033109, l6: 0.061973\n","\n","[epoch: 16394/100000, batch:    12/    1, ite: 16394] train loss: 0.119399, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001202, l3: 0.011675, l4: 0.020713, l5: 0.033901, l6: 0.061622\n","\n","[epoch: 16395/100000, batch:    12/    1, ite: 16395] train loss: 0.119424, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001449, l3: 0.011164, l4: 0.020099, l5: 0.030173, l6: 0.054168\n","\n","[epoch: 16396/100000, batch:    12/    1, ite: 16396] train loss: 0.119418, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001164, l3: 0.011646, l4: 0.019536, l5: 0.030567, l6: 0.053450\n","\n","[epoch: 16397/100000, batch:    12/    1, ite: 16397] train loss: 0.119411, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001161, l3: 0.011801, l4: 0.020204, l5: 0.033078, l6: 0.049072\n","\n","[epoch: 16398/100000, batch:    12/    1, ite: 16398] train loss: 0.119401, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001582, l3: 0.011215, l4: 0.020477, l5: 0.032019, l6: 0.057408\n","\n","[epoch: 16399/100000, batch:    12/    1, ite: 16399] train loss: 0.119409, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001393, l3: 0.011255, l4: 0.020008, l5: 0.030808, l6: 0.058344\n","\n","[epoch: 16400/100000, batch:    12/    1, ite: 16400] train loss: 0.119416, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001270, l3: 0.011707, l4: 0.020163, l5: 0.031611, l6: 0.046277\n","\n","[epoch: 16401/100000, batch:    12/    1, ite: 16401] train loss: 0.119395, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001198, l3: 0.011258, l4: 0.019906, l5: 0.032802, l6: 0.047125\n","\n","[epoch: 16402/100000, batch:    12/    1, ite: 16402] train loss: 0.119378, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001254, l3: 0.011672, l4: 0.019473, l5: 0.030923, l6: 0.054372\n","\n","[epoch: 16403/100000, batch:    12/    1, ite: 16403] train loss: 0.119374, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001636, l3: 0.011551, l4: 0.020674, l5: 0.033937, l6: 0.064065\n","\n","[epoch: 16404/100000, batch:    12/    1, ite: 16404] train loss: 0.119405, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001217, l3: 0.011838, l4: 0.020035, l5: 0.033665, l6: 0.047684\n","\n","[epoch: 16405/100000, batch:    12/    1, ite: 16405] train loss: 0.119393, tar: 0.000066 \n","l0: 0.000068, l1: 0.000072, l2: 0.001259, l3: 0.011566, l4: 0.019662, l5: 0.031666, l6: 0.051749\n","\n","[epoch: 16406/100000, batch:    12/    1, ite: 16406] train loss: 0.119385, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001597, l3: 0.011528, l4: 0.019317, l5: 0.032012, l6: 0.043709\n","\n","[epoch: 16407/100000, batch:    12/    1, ite: 16407] train loss: 0.119358, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001309, l3: 0.011080, l4: 0.020152, l5: 0.032054, l6: 0.047950\n","\n","[epoch: 16408/100000, batch:    12/    1, ite: 16408] train loss: 0.119341, tar: 0.000066 \n","l0: 0.000075, l1: 0.000075, l2: 0.001620, l3: 0.011345, l4: 0.020381, l5: 0.033349, l6: 0.048285\n","\n","[epoch: 16409/100000, batch:    12/    1, ite: 16409] train loss: 0.119331, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001382, l3: 0.011966, l4: 0.020935, l5: 0.032860, l6: 0.061668\n","\n","[epoch: 16410/100000, batch:    12/    1, ite: 16410] train loss: 0.119355, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001211, l3: 0.011452, l4: 0.020692, l5: 0.033001, l6: 0.057351\n","\n","[epoch: 16411/100000, batch:    12/    1, ite: 16411] train loss: 0.119365, tar: 0.000066 \n","l0: 0.000069, l1: 0.000067, l2: 0.001681, l3: 0.012322, l4: 0.020499, l5: 0.031353, l6: 0.048651\n","\n","[epoch: 16412/100000, batch:    12/    1, ite: 16412] train loss: 0.119354, tar: 0.000066 \n","l0: 0.000061, l1: 0.000062, l2: 0.001292, l3: 0.010995, l4: 0.019860, l5: 0.030154, l6: 0.044454\n","\n","[epoch: 16413/100000, batch:    12/    1, ite: 16413] train loss: 0.119324, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001217, l3: 0.011259, l4: 0.020525, l5: 0.032397, l6: 0.047606\n","\n","[epoch: 16414/100000, batch:    12/    1, ite: 16414] train loss: 0.119309, tar: 0.000066 \n","l0: 0.000068, l1: 0.000067, l2: 0.001123, l3: 0.011363, l4: 0.020479, l5: 0.032441, l6: 0.047590\n","\n","[epoch: 16415/100000, batch:    12/    1, ite: 16415] train loss: 0.119294, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001190, l3: 0.011828, l4: 0.020628, l5: 0.032723, l6: 0.058970\n","\n","[epoch: 16416/100000, batch:    12/    1, ite: 16416] train loss: 0.119309, tar: 0.000066 \n","l0: 0.000065, l1: 0.000064, l2: 0.001221, l3: 0.011670, l4: 0.020054, l5: 0.033919, l6: 0.049535\n","\n","[epoch: 16417/100000, batch:    12/    1, ite: 16417] train loss: 0.119302, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001723, l3: 0.011125, l4: 0.020363, l5: 0.034533, l6: 0.067687\n","\n","[epoch: 16418/100000, batch:    12/    1, ite: 16418] train loss: 0.119341, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001300, l3: 0.011019, l4: 0.020023, l5: 0.033067, l6: 0.066232\n","\n","[epoch: 16419/100000, batch:    12/    1, ite: 16419] train loss: 0.119371, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001767, l3: 0.011923, l4: 0.019693, l5: 0.032871, l6: 0.046622\n","\n","[epoch: 16420/100000, batch:    12/    1, ite: 16420] train loss: 0.119356, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001516, l3: 0.011005, l4: 0.020063, l5: 0.032924, l6: 0.066106\n","\n","[epoch: 16421/100000, batch:    12/    1, ite: 16421] train loss: 0.119385, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001359, l3: 0.011685, l4: 0.020988, l5: 0.031786, l6: 0.060200\n","\n","[epoch: 16422/100000, batch:    12/    1, ite: 16422] train loss: 0.119401, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001771, l3: 0.011940, l4: 0.020210, l5: 0.033864, l6: 0.048725\n","\n","[epoch: 16423/100000, batch:    12/    1, ite: 16423] train loss: 0.119395, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001559, l3: 0.011644, l4: 0.019321, l5: 0.030759, l6: 0.055052\n","\n","[epoch: 16424/100000, batch:    12/    1, ite: 16424] train loss: 0.119392, tar: 0.000066 \n","l0: 0.000069, l1: 0.000070, l2: 0.001341, l3: 0.011571, l4: 0.020289, l5: 0.031156, l6: 0.053171\n","\n","[epoch: 16425/100000, batch:    12/    1, ite: 16425] train loss: 0.119388, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001328, l3: 0.011105, l4: 0.019855, l5: 0.031779, l6: 0.052747\n","\n","[epoch: 16426/100000, batch:    12/    1, ite: 16426] train loss: 0.119383, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001276, l3: 0.011582, l4: 0.020368, l5: 0.034180, l6: 0.065424\n","\n","[epoch: 16427/100000, batch:    12/    1, ite: 16427] train loss: 0.119414, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001384, l3: 0.011796, l4: 0.019876, l5: 0.032226, l6: 0.053994\n","\n","[epoch: 16428/100000, batch:    12/    1, ite: 16428] train loss: 0.119414, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001412, l3: 0.011964, l4: 0.020947, l5: 0.031798, l6: 0.050253\n","\n","[epoch: 16429/100000, batch:    12/    1, ite: 16429] train loss: 0.119408, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001421, l3: 0.011103, l4: 0.020288, l5: 0.033518, l6: 0.067019\n","\n","[epoch: 16430/100000, batch:    12/    1, ite: 16430] train loss: 0.119440, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001165, l3: 0.011607, l4: 0.020544, l5: 0.032891, l6: 0.049990\n","\n","[epoch: 16431/100000, batch:    12/    1, ite: 16431] train loss: 0.119433, tar: 0.000066 \n","l0: 0.000072, l1: 0.000073, l2: 0.001312, l3: 0.012159, l4: 0.021224, l5: 0.033367, l6: 0.062973\n","\n","[epoch: 16432/100000, batch:    12/    1, ite: 16432] train loss: 0.119460, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001227, l3: 0.011330, l4: 0.020008, l5: 0.030481, l6: 0.044921\n","\n","[epoch: 16433/100000, batch:    12/    1, ite: 16433] train loss: 0.119434, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001177, l3: 0.011437, l4: 0.019985, l5: 0.030882, l6: 0.059114\n","\n","[epoch: 16434/100000, batch:    12/    1, ite: 16434] train loss: 0.119442, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001557, l3: 0.011341, l4: 0.020380, l5: 0.033686, l6: 0.049514\n","\n","[epoch: 16435/100000, batch:    12/    1, ite: 16435] train loss: 0.119435, tar: 0.000066 \n","l0: 0.000059, l1: 0.000060, l2: 0.001804, l3: 0.011494, l4: 0.020137, l5: 0.031207, l6: 0.059351\n","\n","[epoch: 16436/100000, batch:    12/    1, ite: 16436] train loss: 0.119446, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001591, l3: 0.011508, l4: 0.020252, l5: 0.030890, l6: 0.044421\n","\n","[epoch: 16437/100000, batch:    12/    1, ite: 16437] train loss: 0.119421, tar: 0.000066 \n","l0: 0.000060, l1: 0.000061, l2: 0.001470, l3: 0.011150, l4: 0.019776, l5: 0.032473, l6: 0.066831\n","\n","[epoch: 16438/100000, batch:    12/    1, ite: 16438] train loss: 0.119450, tar: 0.000066 \n","l0: 0.000064, l1: 0.000067, l2: 0.001567, l3: 0.011619, l4: 0.019862, l5: 0.033313, l6: 0.048560\n","\n","[epoch: 16439/100000, batch:    12/    1, ite: 16439] train loss: 0.119440, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001597, l3: 0.011270, l4: 0.019958, l5: 0.030589, l6: 0.051826\n","\n","[epoch: 16440/100000, batch:    12/    1, ite: 16440] train loss: 0.119431, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001390, l3: 0.011419, l4: 0.019869, l5: 0.030949, l6: 0.045600\n","\n","[epoch: 16441/100000, batch:    12/    1, ite: 16441] train loss: 0.119408, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001513, l3: 0.011754, l4: 0.019952, l5: 0.033088, l6: 0.064699\n","\n","[epoch: 16442/100000, batch:    12/    1, ite: 16442] train loss: 0.119434, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001267, l3: 0.011294, l4: 0.020135, l5: 0.033234, l6: 0.048788\n","\n","[epoch: 16443/100000, batch:    12/    1, ite: 16443] train loss: 0.119424, tar: 0.000066 \n","l0: 0.000056, l1: 0.000060, l2: 0.001781, l3: 0.011215, l4: 0.020127, l5: 0.033581, l6: 0.048687\n","\n","[epoch: 16444/100000, batch:    12/    1, ite: 16444] train loss: 0.119415, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001296, l3: 0.011323, l4: 0.019940, l5: 0.032815, l6: 0.046985\n","\n","[epoch: 16445/100000, batch:    12/    1, ite: 16445] train loss: 0.119400, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001482, l3: 0.011665, l4: 0.019937, l5: 0.030356, l6: 0.051993\n","\n","[epoch: 16446/100000, batch:    12/    1, ite: 16446] train loss: 0.119391, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001464, l3: 0.011518, l4: 0.020242, l5: 0.030708, l6: 0.052676\n","\n","[epoch: 16447/100000, batch:    12/    1, ite: 16447] train loss: 0.119385, tar: 0.000066 \n","l0: 0.000073, l1: 0.000074, l2: 0.001534, l3: 0.011281, l4: 0.020350, l5: 0.032580, l6: 0.048852\n","\n","[epoch: 16448/100000, batch:    12/    1, ite: 16448] train loss: 0.119375, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001121, l3: 0.011043, l4: 0.018841, l5: 0.030586, l6: 0.047622\n","\n","[epoch: 16449/100000, batch:    12/    1, ite: 16449] train loss: 0.119352, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001474, l3: 0.011021, l4: 0.019724, l5: 0.032335, l6: 0.043272\n","\n","[epoch: 16450/100000, batch:    12/    1, ite: 16450] train loss: 0.119327, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001744, l3: 0.011792, l4: 0.020326, l5: 0.031524, l6: 0.048004\n","\n","[epoch: 16451/100000, batch:    12/    1, ite: 16451] train loss: 0.119314, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001332, l3: 0.011113, l4: 0.020106, l5: 0.032028, l6: 0.047939\n","\n","[epoch: 16452/100000, batch:    12/    1, ite: 16452] train loss: 0.119299, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001667, l3: 0.011875, l4: 0.020514, l5: 0.032855, l6: 0.060903\n","\n","[epoch: 16453/100000, batch:    12/    1, ite: 16453] train loss: 0.119318, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001543, l3: 0.011530, l4: 0.019756, l5: 0.033494, l6: 0.048511\n","\n","[epoch: 16454/100000, batch:    12/    1, ite: 16454] train loss: 0.119309, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001829, l3: 0.011044, l4: 0.020121, l5: 0.034085, l6: 0.066072\n","\n","[epoch: 16455/100000, batch:    12/    1, ite: 16455] train loss: 0.119340, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001498, l3: 0.011635, l4: 0.019855, l5: 0.033073, l6: 0.047956\n","\n","[epoch: 16456/100000, batch:    12/    1, ite: 16456] train loss: 0.119328, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001497, l3: 0.011193, l4: 0.019728, l5: 0.028970, l6: 0.050635\n","\n","[epoch: 16457/100000, batch:    12/    1, ite: 16457] train loss: 0.119312, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001370, l3: 0.011686, l4: 0.020183, l5: 0.032259, l6: 0.061068\n","\n","[epoch: 16458/100000, batch:    12/    1, ite: 16458] train loss: 0.119329, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001289, l3: 0.011296, l4: 0.020098, l5: 0.033275, l6: 0.048878\n","\n","[epoch: 16459/100000, batch:    12/    1, ite: 16459] train loss: 0.119319, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001410, l3: 0.011679, l4: 0.020063, l5: 0.034006, l6: 0.063872\n","\n","[epoch: 16460/100000, batch:    12/    1, ite: 16460] train loss: 0.119345, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001474, l3: 0.011389, l4: 0.019827, l5: 0.030768, l6: 0.051360\n","\n","[epoch: 16461/100000, batch:    12/    1, ite: 16461] train loss: 0.119335, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001649, l3: 0.011162, l4: 0.020084, l5: 0.032259, l6: 0.048541\n","\n","[epoch: 16462/100000, batch:    12/    1, ite: 16462] train loss: 0.119323, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001189, l3: 0.011292, l4: 0.020203, l5: 0.033616, l6: 0.048384\n","\n","[epoch: 16463/100000, batch:    12/    1, ite: 16463] train loss: 0.119314, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001222, l3: 0.011294, l4: 0.020087, l5: 0.033604, l6: 0.062439\n","\n","[epoch: 16464/100000, batch:    12/    1, ite: 16464] train loss: 0.119334, tar: 0.000066 \n","l0: 0.000065, l1: 0.000069, l2: 0.001174, l3: 0.011404, l4: 0.020408, l5: 0.030426, l6: 0.049675\n","\n","[epoch: 16465/100000, batch:    12/    1, ite: 16465] train loss: 0.119321, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001507, l3: 0.011181, l4: 0.020324, l5: 0.030947, l6: 0.050562\n","\n","[epoch: 16466/100000, batch:    12/    1, ite: 16466] train loss: 0.119311, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001255, l3: 0.011279, l4: 0.019656, l5: 0.029578, l6: 0.051149\n","\n","[epoch: 16467/100000, batch:    12/    1, ite: 16467] train loss: 0.119297, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001464, l3: 0.011614, l4: 0.020365, l5: 0.033942, l6: 0.063995\n","\n","[epoch: 16468/100000, batch:    12/    1, ite: 16468] train loss: 0.119324, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001213, l3: 0.011661, l4: 0.020205, l5: 0.031032, l6: 0.049118\n","\n","[epoch: 16469/100000, batch:    12/    1, ite: 16469] train loss: 0.119311, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001145, l3: 0.011444, l4: 0.020056, l5: 0.033151, l6: 0.048848\n","\n","[epoch: 16470/100000, batch:    12/    1, ite: 16470] train loss: 0.119301, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001155, l3: 0.011598, l4: 0.019998, l5: 0.033594, l6: 0.048319\n","\n","[epoch: 16471/100000, batch:    12/    1, ite: 16471] train loss: 0.119292, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001309, l3: 0.011414, l4: 0.020807, l5: 0.031201, l6: 0.058832\n","\n","[epoch: 16472/100000, batch:    12/    1, ite: 16472] train loss: 0.119301, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001340, l3: 0.011394, l4: 0.020110, l5: 0.032697, l6: 0.063841\n","\n","[epoch: 16473/100000, batch:    12/    1, ite: 16473] train loss: 0.119322, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001387, l3: 0.011554, l4: 0.020531, l5: 0.034543, l6: 0.064788\n","\n","[epoch: 16474/100000, batch:    12/    1, ite: 16474] train loss: 0.119351, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001194, l3: 0.011180, l4: 0.019246, l5: 0.031766, l6: 0.043706\n","\n","[epoch: 16475/100000, batch:    12/    1, ite: 16475] train loss: 0.119326, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001208, l3: 0.011630, l4: 0.019982, l5: 0.034529, l6: 0.065236\n","\n","[epoch: 16476/100000, batch:    12/    1, ite: 16476] train loss: 0.119354, tar: 0.000066 \n","l0: 0.000064, l1: 0.000068, l2: 0.001242, l3: 0.011464, l4: 0.020529, l5: 0.034804, l6: 0.063958\n","\n","[epoch: 16477/100000, batch:    12/    1, ite: 16477] train loss: 0.119381, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001469, l3: 0.011855, l4: 0.020013, l5: 0.030382, l6: 0.051793\n","\n","[epoch: 16478/100000, batch:    12/    1, ite: 16478] train loss: 0.119373, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001161, l3: 0.011438, l4: 0.019561, l5: 0.032899, l6: 0.065397\n","\n","[epoch: 16479/100000, batch:    12/    1, ite: 16479] train loss: 0.119396, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001567, l3: 0.011796, l4: 0.020635, l5: 0.031804, l6: 0.057137\n","\n","[epoch: 16480/100000, batch:    12/    1, ite: 16480] train loss: 0.119404, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001163, l3: 0.011587, l4: 0.019950, l5: 0.031266, l6: 0.052010\n","\n","[epoch: 16481/100000, batch:    12/    1, ite: 16481] train loss: 0.119397, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001529, l3: 0.011721, l4: 0.020532, l5: 0.031621, l6: 0.053758\n","\n","[epoch: 16482/100000, batch:    12/    1, ite: 16482] train loss: 0.119397, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001401, l3: 0.011101, l4: 0.019802, l5: 0.030696, l6: 0.058375\n","\n","[epoch: 16483/100000, batch:    12/    1, ite: 16483] train loss: 0.119401, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001290, l3: 0.011237, l4: 0.020442, l5: 0.030900, l6: 0.050036\n","\n","[epoch: 16484/100000, batch:    12/    1, ite: 16484] train loss: 0.119390, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001350, l3: 0.011324, l4: 0.019819, l5: 0.030792, l6: 0.050909\n","\n","[epoch: 16485/100000, batch:    12/    1, ite: 16485] train loss: 0.119380, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001140, l3: 0.011185, l4: 0.019940, l5: 0.030850, l6: 0.043481\n","\n","[epoch: 16486/100000, batch:    12/    1, ite: 16486] train loss: 0.119354, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001397, l3: 0.011224, l4: 0.020026, l5: 0.029993, l6: 0.052218\n","\n","[epoch: 16487/100000, batch:    12/    1, ite: 16487] train loss: 0.119345, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001484, l3: 0.011747, l4: 0.020466, l5: 0.031775, l6: 0.049930\n","\n","[epoch: 16488/100000, batch:    12/    1, ite: 16488] train loss: 0.119337, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001285, l3: 0.011313, l4: 0.020173, l5: 0.033476, l6: 0.047767\n","\n","[epoch: 16489/100000, batch:    12/    1, ite: 16489] train loss: 0.119326, tar: 0.000066 \n","l0: 0.000065, l1: 0.000064, l2: 0.001190, l3: 0.011510, l4: 0.020325, l5: 0.035017, l6: 0.066088\n","\n","[epoch: 16490/100000, batch:    12/    1, ite: 16490] train loss: 0.119357, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001200, l3: 0.011730, l4: 0.020636, l5: 0.033627, l6: 0.059640\n","\n","[epoch: 16491/100000, batch:    12/    1, ite: 16491] train loss: 0.119372, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001308, l3: 0.011639, l4: 0.020037, l5: 0.031360, l6: 0.044821\n","\n","[epoch: 16492/100000, batch:    12/    1, ite: 16492] train loss: 0.119352, tar: 0.000066 \n","l0: 0.000060, l1: 0.000059, l2: 0.001122, l3: 0.010977, l4: 0.019945, l5: 0.030147, l6: 0.044205\n","\n","[epoch: 16493/100000, batch:    12/    1, ite: 16493] train loss: 0.119326, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001164, l3: 0.011614, l4: 0.020132, l5: 0.031437, l6: 0.046082\n","\n","[epoch: 16494/100000, batch:    12/    1, ite: 16494] train loss: 0.119308, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001566, l3: 0.011167, l4: 0.020093, l5: 0.031996, l6: 0.053911\n","\n","[epoch: 16495/100000, batch:    12/    1, ite: 16495] train loss: 0.119307, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001210, l3: 0.011402, l4: 0.020801, l5: 0.033079, l6: 0.048763\n","\n","[epoch: 16496/100000, batch:    12/    1, ite: 16496] train loss: 0.119299, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001231, l3: 0.011384, l4: 0.020241, l5: 0.033643, l6: 0.048253\n","\n","[epoch: 16497/100000, batch:    12/    1, ite: 16497] train loss: 0.119290, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001527, l3: 0.011667, l4: 0.020314, l5: 0.032017, l6: 0.058373\n","\n","[epoch: 16498/100000, batch:    12/    1, ite: 16498] train loss: 0.119300, tar: 0.000066 \n","l0: 0.000055, l1: 0.000058, l2: 0.001275, l3: 0.011489, l4: 0.019642, l5: 0.028956, l6: 0.050641\n","\n","[epoch: 16499/100000, batch:    12/    1, ite: 16499] train loss: 0.119285, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001155, l3: 0.011370, l4: 0.019917, l5: 0.030691, l6: 0.042793\n","\n","[epoch: 16500/100000, batch:    12/    1, ite: 16500] train loss: 0.119259, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001410, l3: 0.011759, l4: 0.019978, l5: 0.033706, l6: 0.049110\n","\n","[epoch: 16501/100000, batch:    12/    1, ite: 16501] train loss: 0.119253, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001156, l3: 0.011214, l4: 0.020131, l5: 0.031399, l6: 0.049975\n","\n","[epoch: 16502/100000, batch:    12/    1, ite: 16502] train loss: 0.119242, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001321, l3: 0.011418, l4: 0.020647, l5: 0.030406, l6: 0.048656\n","\n","[epoch: 16503/100000, batch:    12/    1, ite: 16503] train loss: 0.119229, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001337, l3: 0.011666, l4: 0.019675, l5: 0.029547, l6: 0.050998\n","\n","[epoch: 16504/100000, batch:    12/    1, ite: 16504] train loss: 0.119217, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001160, l3: 0.011274, l4: 0.018926, l5: 0.030257, l6: 0.046054\n","\n","[epoch: 16505/100000, batch:    12/    1, ite: 16505] train loss: 0.119195, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001487, l3: 0.010968, l4: 0.020081, l5: 0.033734, l6: 0.066065\n","\n","[epoch: 16506/100000, batch:    12/    1, ite: 16506] train loss: 0.119221, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001286, l3: 0.011349, l4: 0.020126, l5: 0.033608, l6: 0.048751\n","\n","[epoch: 16507/100000, batch:    12/    1, ite: 16507] train loss: 0.119213, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001130, l3: 0.011162, l4: 0.019968, l5: 0.030518, l6: 0.044881\n","\n","[epoch: 16508/100000, batch:    12/    1, ite: 16508] train loss: 0.119190, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001379, l3: 0.011670, l4: 0.019918, l5: 0.030392, l6: 0.051358\n","\n","[epoch: 16509/100000, batch:    12/    1, ite: 16509] train loss: 0.119182, tar: 0.000066 \n","l0: 0.000063, l1: 0.000062, l2: 0.001184, l3: 0.011485, l4: 0.019701, l5: 0.030083, l6: 0.050798\n","\n","[epoch: 16510/100000, batch:    12/    1, ite: 16510] train loss: 0.119171, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001320, l3: 0.011725, l4: 0.020625, l5: 0.032670, l6: 0.058911\n","\n","[epoch: 16511/100000, batch:    12/    1, ite: 16511] train loss: 0.119183, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001216, l3: 0.011155, l4: 0.020200, l5: 0.032159, l6: 0.047148\n","\n","[epoch: 16512/100000, batch:    12/    1, ite: 16512] train loss: 0.119169, tar: 0.000066 \n","l0: 0.000066, l1: 0.000065, l2: 0.001231, l3: 0.010664, l4: 0.019650, l5: 0.032326, l6: 0.064239\n","\n","[epoch: 16513/100000, batch:    12/    1, ite: 16513] train loss: 0.119186, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001183, l3: 0.011561, l4: 0.020094, l5: 0.033376, l6: 0.048416\n","\n","[epoch: 16514/100000, batch:    12/    1, ite: 16514] train loss: 0.119178, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001115, l3: 0.010930, l4: 0.019759, l5: 0.031901, l6: 0.043039\n","\n","[epoch: 16515/100000, batch:    12/    1, ite: 16515] train loss: 0.119154, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001480, l3: 0.011023, l4: 0.019652, l5: 0.032011, l6: 0.045796\n","\n","[epoch: 16516/100000, batch:    12/    1, ite: 16516] train loss: 0.119136, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001361, l3: 0.011197, l4: 0.020072, l5: 0.032354, l6: 0.047008\n","\n","[epoch: 16517/100000, batch:    12/    1, ite: 16517] train loss: 0.119123, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001109, l3: 0.011344, l4: 0.020558, l5: 0.032657, l6: 0.056686\n","\n","[epoch: 16518/100000, batch:    12/    1, ite: 16518] train loss: 0.119129, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001153, l3: 0.011756, l4: 0.020354, l5: 0.031518, l6: 0.053849\n","\n","[epoch: 16519/100000, batch:    12/    1, ite: 16519] train loss: 0.119129, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001150, l3: 0.011466, l4: 0.019948, l5: 0.031360, l6: 0.051957\n","\n","[epoch: 16520/100000, batch:    12/    1, ite: 16520] train loss: 0.119123, tar: 0.000066 \n","l0: 0.000063, l1: 0.000062, l2: 0.001200, l3: 0.011885, l4: 0.020561, l5: 0.033498, l6: 0.061952\n","\n","[epoch: 16521/100000, batch:    12/    1, ite: 16521] train loss: 0.119142, tar: 0.000066 \n","l0: 0.000075, l1: 0.000071, l2: 0.001556, l3: 0.011161, l4: 0.020199, l5: 0.034503, l6: 0.066029\n","\n","[epoch: 16522/100000, batch:    12/    1, ite: 16522] train loss: 0.119170, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001564, l3: 0.011270, l4: 0.019748, l5: 0.030976, l6: 0.047760\n","\n","[epoch: 16523/100000, batch:    12/    1, ite: 16523] train loss: 0.119155, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001856, l3: 0.010938, l4: 0.019892, l5: 0.033410, l6: 0.065638\n","\n","[epoch: 16524/100000, batch:    12/    1, ite: 16524] train loss: 0.119179, tar: 0.000066 \n","l0: 0.000064, l1: 0.000069, l2: 0.001295, l3: 0.011702, l4: 0.020179, l5: 0.034369, l6: 0.065077\n","\n","[epoch: 16525/100000, batch:    12/    1, ite: 16525] train loss: 0.119205, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001281, l3: 0.011149, l4: 0.019569, l5: 0.032391, l6: 0.052976\n","\n","[epoch: 16526/100000, batch:    12/    1, ite: 16526] train loss: 0.119202, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001241, l3: 0.011468, l4: 0.019928, l5: 0.031378, l6: 0.043882\n","\n","[epoch: 16527/100000, batch:    12/    1, ite: 16527] train loss: 0.119181, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001125, l3: 0.011510, l4: 0.019863, l5: 0.031640, l6: 0.061240\n","\n","[epoch: 16528/100000, batch:    12/    1, ite: 16528] train loss: 0.119193, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001186, l3: 0.011686, l4: 0.020619, l5: 0.033067, l6: 0.058786\n","\n","[epoch: 16529/100000, batch:    12/    1, ite: 16529] train loss: 0.119204, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001417, l3: 0.011681, l4: 0.020187, l5: 0.032787, l6: 0.054041\n","\n","[epoch: 16530/100000, batch:    12/    1, ite: 16530] train loss: 0.119206, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001408, l3: 0.011233, l4: 0.020124, l5: 0.030259, l6: 0.053893\n","\n","[epoch: 16531/100000, batch:    12/    1, ite: 16531] train loss: 0.119202, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001182, l3: 0.011395, l4: 0.019410, l5: 0.029609, l6: 0.047501\n","\n","[epoch: 16532/100000, batch:    12/    1, ite: 16532] train loss: 0.119184, tar: 0.000066 \n","l0: 0.000056, l1: 0.000055, l2: 0.001444, l3: 0.011181, l4: 0.019944, l5: 0.032219, l6: 0.053538\n","\n","[epoch: 16533/100000, batch:    12/    1, ite: 16533] train loss: 0.119182, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001482, l3: 0.011435, l4: 0.019528, l5: 0.032706, l6: 0.064327\n","\n","[epoch: 16534/100000, batch:    12/    1, ite: 16534] train loss: 0.119202, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001261, l3: 0.011250, l4: 0.020417, l5: 0.033923, l6: 0.062230\n","\n","[epoch: 16535/100000, batch:    12/    1, ite: 16535] train loss: 0.119220, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001245, l3: 0.011761, l4: 0.020306, l5: 0.034135, l6: 0.064884\n","\n","[epoch: 16536/100000, batch:    12/    1, ite: 16536] train loss: 0.119245, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001533, l3: 0.011575, l4: 0.019856, l5: 0.030242, l6: 0.051433\n","\n","[epoch: 16537/100000, batch:    12/    1, ite: 16537] train loss: 0.119237, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001204, l3: 0.011253, l4: 0.020051, l5: 0.032353, l6: 0.050456\n","\n","[epoch: 16538/100000, batch:    12/    1, ite: 16538] train loss: 0.119230, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001647, l3: 0.011764, l4: 0.019946, l5: 0.033491, l6: 0.048478\n","\n","[epoch: 16539/100000, batch:    12/    1, ite: 16539] train loss: 0.119223, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001182, l3: 0.011409, l4: 0.019311, l5: 0.031068, l6: 0.049368\n","\n","[epoch: 16540/100000, batch:    12/    1, ite: 16540] train loss: 0.119210, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001355, l3: 0.011807, l4: 0.020827, l5: 0.032818, l6: 0.061118\n","\n","[epoch: 16541/100000, batch:    12/    1, ite: 16541] train loss: 0.119227, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001317, l3: 0.011080, l4: 0.019972, l5: 0.032678, l6: 0.047213\n","\n","[epoch: 16542/100000, batch:    12/    1, ite: 16542] train loss: 0.119214, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001161, l3: 0.011113, l4: 0.019935, l5: 0.030451, l6: 0.044893\n","\n","[epoch: 16543/100000, batch:    12/    1, ite: 16543] train loss: 0.119193, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001157, l3: 0.011087, l4: 0.019142, l5: 0.030365, l6: 0.047977\n","\n","[epoch: 16544/100000, batch:    12/    1, ite: 16544] train loss: 0.119176, tar: 0.000066 \n","l0: 0.000064, l1: 0.000063, l2: 0.001303, l3: 0.011105, l4: 0.019889, l5: 0.032774, l6: 0.048009\n","\n","[epoch: 16545/100000, batch:    12/    1, ite: 16545] train loss: 0.119165, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001213, l3: 0.010942, l4: 0.019130, l5: 0.029110, l6: 0.048954\n","\n","[epoch: 16546/100000, batch:    12/    1, ite: 16546] train loss: 0.119147, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001471, l3: 0.011730, l4: 0.020566, l5: 0.032529, l6: 0.057234\n","\n","[epoch: 16547/100000, batch:    12/    1, ite: 16547] train loss: 0.119155, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001165, l3: 0.011098, l4: 0.019599, l5: 0.032108, l6: 0.044150\n","\n","[epoch: 16548/100000, batch:    12/    1, ite: 16548] train loss: 0.119135, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001745, l3: 0.011553, l4: 0.019803, l5: 0.033306, l6: 0.049339\n","\n","[epoch: 16549/100000, batch:    12/    1, ite: 16549] train loss: 0.119129, tar: 0.000066 \n","l0: 0.000055, l1: 0.000055, l2: 0.001479, l3: 0.011535, l4: 0.019975, l5: 0.033161, l6: 0.048898\n","\n","[epoch: 16550/100000, batch:    12/    1, ite: 16550] train loss: 0.119122, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001215, l3: 0.011358, l4: 0.020385, l5: 0.033395, l6: 0.048882\n","\n","[epoch: 16551/100000, batch:    12/    1, ite: 16551] train loss: 0.119115, tar: 0.000066 \n","l0: 0.000059, l1: 0.000058, l2: 0.001398, l3: 0.011293, l4: 0.020470, l5: 0.031923, l6: 0.054237\n","\n","[epoch: 16552/100000, batch:    12/    1, ite: 16552] train loss: 0.119116, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001526, l3: 0.011559, l4: 0.020432, l5: 0.032905, l6: 0.060451\n","\n","[epoch: 16553/100000, batch:    12/    1, ite: 16553] train loss: 0.119130, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001443, l3: 0.011566, l4: 0.019787, l5: 0.029768, l6: 0.052502\n","\n","[epoch: 16554/100000, batch:    12/    1, ite: 16554] train loss: 0.119123, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001683, l3: 0.011027, l4: 0.020006, l5: 0.033456, l6: 0.067249\n","\n","[epoch: 16555/100000, batch:    12/    1, ite: 16555] train loss: 0.119149, tar: 0.000066 \n","l0: 0.000054, l1: 0.000059, l2: 0.001417, l3: 0.011642, l4: 0.020646, l5: 0.031405, l6: 0.049186\n","\n","[epoch: 16556/100000, batch:    12/    1, ite: 16556] train loss: 0.119141, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001334, l3: 0.011687, l4: 0.020556, l5: 0.032093, l6: 0.048630\n","\n","[epoch: 16557/100000, batch:    12/    1, ite: 16557] train loss: 0.119132, tar: 0.000066 \n","l0: 0.000055, l1: 0.000055, l2: 0.001450, l3: 0.011193, l4: 0.020321, l5: 0.033235, l6: 0.052982\n","\n","[epoch: 16558/100000, batch:    12/    1, ite: 16558] train loss: 0.119132, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001426, l3: 0.011366, l4: 0.019264, l5: 0.030257, l6: 0.047220\n","\n","[epoch: 16559/100000, batch:    12/    1, ite: 16559] train loss: 0.119115, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001351, l3: 0.011447, l4: 0.019236, l5: 0.031315, l6: 0.055102\n","\n","[epoch: 16560/100000, batch:    12/    1, ite: 16560] train loss: 0.119115, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001236, l3: 0.011621, l4: 0.019995, l5: 0.034098, l6: 0.066379\n","\n","[epoch: 16561/100000, batch:    12/    1, ite: 16561] train loss: 0.119140, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001190, l3: 0.011311, l4: 0.019582, l5: 0.031041, l6: 0.044062\n","\n","[epoch: 16562/100000, batch:    12/    1, ite: 16562] train loss: 0.119119, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001231, l3: 0.011397, l4: 0.019572, l5: 0.028972, l6: 0.049832\n","\n","[epoch: 16563/100000, batch:    12/    1, ite: 16563] train loss: 0.119105, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001543, l3: 0.011459, l4: 0.019670, l5: 0.032240, l6: 0.046597\n","\n","[epoch: 16564/100000, batch:    12/    1, ite: 16564] train loss: 0.119092, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001244, l3: 0.011387, l4: 0.019864, l5: 0.031079, l6: 0.044289\n","\n","[epoch: 16565/100000, batch:    12/    1, ite: 16565] train loss: 0.119072, tar: 0.000066 \n","l0: 0.000059, l1: 0.000067, l2: 0.001209, l3: 0.011217, l4: 0.020645, l5: 0.031692, l6: 0.051027\n","\n","[epoch: 16566/100000, batch:    12/    1, ite: 16566] train loss: 0.119066, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001203, l3: 0.011698, l4: 0.019995, l5: 0.031699, l6: 0.052476\n","\n","[epoch: 16567/100000, batch:    12/    1, ite: 16567] train loss: 0.119063, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001610, l3: 0.011370, l4: 0.020096, l5: 0.032961, l6: 0.046246\n","\n","[epoch: 16568/100000, batch:    12/    1, ite: 16568] train loss: 0.119051, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001587, l3: 0.011635, l4: 0.020650, l5: 0.033264, l6: 0.059980\n","\n","[epoch: 16569/100000, batch:    12/    1, ite: 16569] train loss: 0.119066, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001578, l3: 0.011619, l4: 0.020178, l5: 0.031979, l6: 0.061700\n","\n","[epoch: 16570/100000, batch:    12/    1, ite: 16570] train loss: 0.119080, tar: 0.000066 \n","l0: 0.000053, l1: 0.000057, l2: 0.001291, l3: 0.011219, l4: 0.019983, l5: 0.032798, l6: 0.046304\n","\n","[epoch: 16571/100000, batch:    12/    1, ite: 16571] train loss: 0.119067, tar: 0.000066 \n","l0: 0.000054, l1: 0.000058, l2: 0.001294, l3: 0.011501, l4: 0.019788, l5: 0.032524, l6: 0.046214\n","\n","[epoch: 16572/100000, batch:    12/    1, ite: 16572] train loss: 0.119054, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001216, l3: 0.011434, l4: 0.020862, l5: 0.031455, l6: 0.060573\n","\n","[epoch: 16573/100000, batch:    12/    1, ite: 16573] train loss: 0.119065, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001383, l3: 0.011713, l4: 0.019948, l5: 0.030113, l6: 0.052580\n","\n","[epoch: 16574/100000, batch:    12/    1, ite: 16574] train loss: 0.119060, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001161, l3: 0.011134, l4: 0.020521, l5: 0.033460, l6: 0.049223\n","\n","[epoch: 16575/100000, batch:    12/    1, ite: 16575] train loss: 0.119054, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001639, l3: 0.011113, l4: 0.019675, l5: 0.030117, l6: 0.051625\n","\n","[epoch: 16576/100000, batch:    12/    1, ite: 16576] train loss: 0.119046, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001269, l3: 0.011617, l4: 0.019439, l5: 0.030342, l6: 0.048242\n","\n","[epoch: 16577/100000, batch:    12/    1, ite: 16577] train loss: 0.119032, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001384, l3: 0.011287, l4: 0.019855, l5: 0.030398, l6: 0.043929\n","\n","[epoch: 16578/100000, batch:    12/    1, ite: 16578] train loss: 0.119011, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001312, l3: 0.011158, l4: 0.019814, l5: 0.030247, l6: 0.046326\n","\n","[epoch: 16579/100000, batch:    12/    1, ite: 16579] train loss: 0.118993, tar: 0.000066 \n","l0: 0.000059, l1: 0.000061, l2: 0.001536, l3: 0.011000, l4: 0.020141, l5: 0.033449, l6: 0.067608\n","\n","[epoch: 16580/100000, batch:    12/    1, ite: 16580] train loss: 0.119019, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001617, l3: 0.010957, l4: 0.019376, l5: 0.031201, l6: 0.049218\n","\n","[epoch: 16581/100000, batch:    12/    1, ite: 16581] train loss: 0.119008, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001550, l3: 0.011150, l4: 0.020023, l5: 0.032590, l6: 0.046026\n","\n","[epoch: 16582/100000, batch:    12/    1, ite: 16582] train loss: 0.118995, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001556, l3: 0.011518, l4: 0.019266, l5: 0.032328, l6: 0.042876\n","\n","[epoch: 16583/100000, batch:    12/    1, ite: 16583] train loss: 0.118975, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001167, l3: 0.011244, l4: 0.018999, l5: 0.030704, l6: 0.054070\n","\n","[epoch: 16584/100000, batch:    12/    1, ite: 16584] train loss: 0.118971, tar: 0.000066 \n","l0: 0.000099, l1: 0.000098, l2: 0.001733, l3: 0.011362, l4: 0.018950, l5: 0.030193, l6: 0.047167\n","\n","[epoch: 16585/100000, batch:    12/    1, ite: 16585] train loss: 0.118955, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001521, l3: 0.011078, l4: 0.019777, l5: 0.032602, l6: 0.045325\n","\n","[epoch: 16586/100000, batch:    12/    1, ite: 16586] train loss: 0.118940, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001515, l3: 0.011157, l4: 0.020733, l5: 0.034942, l6: 0.066989\n","\n","[epoch: 16587/100000, batch:    12/    1, ite: 16587] train loss: 0.118969, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001327, l3: 0.011138, l4: 0.019819, l5: 0.031911, l6: 0.044280\n","\n","[epoch: 16588/100000, batch:    12/    1, ite: 16588] train loss: 0.118951, tar: 0.000066 \n","l0: 0.000076, l1: 0.000078, l2: 0.001556, l3: 0.011223, l4: 0.020139, l5: 0.032893, l6: 0.049734\n","\n","[epoch: 16589/100000, batch:    12/    1, ite: 16589] train loss: 0.118945, tar: 0.000066 \n","l0: 0.000187, l1: 0.000183, l2: 0.001684, l3: 0.011521, l4: 0.019330, l5: 0.030154, l6: 0.048504\n","\n","[epoch: 16590/100000, batch:    12/    1, ite: 16590] train loss: 0.118933, tar: 0.000066 \n","l0: 0.000059, l1: 0.000060, l2: 0.001280, l3: 0.011065, l4: 0.020242, l5: 0.031925, l6: 0.051423\n","\n","[epoch: 16591/100000, batch:    12/    1, ite: 16591] train loss: 0.118928, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001119, l3: 0.011145, l4: 0.020408, l5: 0.033525, l6: 0.050300\n","\n","[epoch: 16592/100000, batch:    12/    1, ite: 16592] train loss: 0.118924, tar: 0.000066 \n","l0: 0.000070, l1: 0.000069, l2: 0.001274, l3: 0.010837, l4: 0.019887, l5: 0.032520, l6: 0.066362\n","\n","[epoch: 16593/100000, batch:    12/    1, ite: 16593] train loss: 0.118945, tar: 0.000066 \n","l0: 0.000068, l1: 0.000070, l2: 0.001109, l3: 0.011105, l4: 0.019998, l5: 0.031780, l6: 0.053248\n","\n","[epoch: 16594/100000, batch:    12/    1, ite: 16594] train loss: 0.118942, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001553, l3: 0.011723, l4: 0.020576, l5: 0.031818, l6: 0.057242\n","\n","[epoch: 16595/100000, batch:    12/    1, ite: 16595] train loss: 0.118949, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001171, l3: 0.011313, l4: 0.019914, l5: 0.030929, l6: 0.059790\n","\n","[epoch: 16596/100000, batch:    12/    1, ite: 16596] train loss: 0.118956, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001683, l3: 0.011538, l4: 0.020150, l5: 0.034442, l6: 0.064195\n","\n","[epoch: 16597/100000, batch:    12/    1, ite: 16597] train loss: 0.118978, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001525, l3: 0.011129, l4: 0.019812, l5: 0.031520, l6: 0.045038\n","\n","[epoch: 16598/100000, batch:    12/    1, ite: 16598] train loss: 0.118962, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001423, l3: 0.011129, l4: 0.019963, l5: 0.032873, l6: 0.046970\n","\n","[epoch: 16599/100000, batch:    12/    1, ite: 16599] train loss: 0.118951, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001554, l3: 0.011506, l4: 0.020057, l5: 0.033755, l6: 0.049633\n","\n","[epoch: 16600/100000, batch:    12/    1, ite: 16600] train loss: 0.118947, tar: 0.000066 \n","l0: 0.000055, l1: 0.000059, l2: 0.001316, l3: 0.011033, l4: 0.019815, l5: 0.029615, l6: 0.047525\n","\n","[epoch: 16601/100000, batch:    12/    1, ite: 16601] train loss: 0.118931, tar: 0.000066 \n","l0: 0.000068, l1: 0.000070, l2: 0.001259, l3: 0.011674, l4: 0.020284, l5: 0.032932, l6: 0.049492\n","\n","[epoch: 16602/100000, batch:    12/    1, ite: 16602] train loss: 0.118926, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001219, l3: 0.011358, l4: 0.020540, l5: 0.032789, l6: 0.057856\n","\n","[epoch: 16603/100000, batch:    12/    1, ite: 16603] train loss: 0.118934, tar: 0.000066 \n","l0: 0.000064, l1: 0.000068, l2: 0.001225, l3: 0.011386, l4: 0.019746, l5: 0.031858, l6: 0.052784\n","\n","[epoch: 16604/100000, batch:    12/    1, ite: 16604] train loss: 0.118931, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001233, l3: 0.011637, l4: 0.020012, l5: 0.033941, l6: 0.048548\n","\n","[epoch: 16605/100000, batch:    12/    1, ite: 16605] train loss: 0.118925, tar: 0.000066 \n","l0: 0.000064, l1: 0.000068, l2: 0.001181, l3: 0.011450, l4: 0.020404, l5: 0.032150, l6: 0.047482\n","\n","[epoch: 16606/100000, batch:    12/    1, ite: 16606] train loss: 0.118915, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001246, l3: 0.011486, l4: 0.020055, l5: 0.034566, l6: 0.062988\n","\n","[epoch: 16607/100000, batch:    12/    1, ite: 16607] train loss: 0.118934, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001453, l3: 0.011204, l4: 0.019993, l5: 0.032410, l6: 0.049155\n","\n","[epoch: 16608/100000, batch:    12/    1, ite: 16608] train loss: 0.118927, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001143, l3: 0.011266, l4: 0.020291, l5: 0.030945, l6: 0.055592\n","\n","[epoch: 16609/100000, batch:    12/    1, ite: 16609] train loss: 0.118928, tar: 0.000066 \n","l0: 0.000063, l1: 0.000062, l2: 0.001233, l3: 0.011077, l4: 0.019712, l5: 0.032413, l6: 0.045155\n","\n","[epoch: 16610/100000, batch:    12/    1, ite: 16610] train loss: 0.118913, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001519, l3: 0.011092, l4: 0.019932, l5: 0.030844, l6: 0.059032\n","\n","[epoch: 16611/100000, batch:    12/    1, ite: 16611] train loss: 0.118918, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001537, l3: 0.011618, l4: 0.020104, l5: 0.031191, l6: 0.051415\n","\n","[epoch: 16612/100000, batch:    12/    1, ite: 16612] train loss: 0.118914, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001298, l3: 0.011726, l4: 0.020794, l5: 0.032481, l6: 0.048235\n","\n","[epoch: 16613/100000, batch:    12/    1, ite: 16613] train loss: 0.118907, tar: 0.000066 \n","l0: 0.000067, l1: 0.000066, l2: 0.001507, l3: 0.011169, l4: 0.020329, l5: 0.032586, l6: 0.051975\n","\n","[epoch: 16614/100000, batch:    12/    1, ite: 16614] train loss: 0.118905, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001506, l3: 0.011081, l4: 0.020058, l5: 0.031166, l6: 0.052745\n","\n","[epoch: 16615/100000, batch:    12/    1, ite: 16615] train loss: 0.118901, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001229, l3: 0.011608, l4: 0.019703, l5: 0.033625, l6: 0.048825\n","\n","[epoch: 16616/100000, batch:    12/    1, ite: 16616] train loss: 0.118895, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001285, l3: 0.011248, l4: 0.020070, l5: 0.034361, l6: 0.062903\n","\n","[epoch: 16617/100000, batch:    12/    1, ite: 16617] train loss: 0.118913, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001289, l3: 0.011379, l4: 0.019953, l5: 0.033011, l6: 0.065011\n","\n","[epoch: 16618/100000, batch:    12/    1, ite: 16618] train loss: 0.118932, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001185, l3: 0.011067, l4: 0.019957, l5: 0.033366, l6: 0.049009\n","\n","[epoch: 16619/100000, batch:    12/    1, ite: 16619] train loss: 0.118925, tar: 0.000066 \n","l0: 0.000061, l1: 0.000062, l2: 0.001488, l3: 0.011022, l4: 0.020124, l5: 0.032619, l6: 0.067423\n","\n","[epoch: 16620/100000, batch:    12/    1, ite: 16620] train loss: 0.118948, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001156, l3: 0.011357, l4: 0.019862, l5: 0.030081, l6: 0.052173\n","\n","[epoch: 16621/100000, batch:    12/    1, ite: 16621] train loss: 0.118941, tar: 0.000066 \n","l0: 0.000060, l1: 0.000061, l2: 0.001444, l3: 0.010996, l4: 0.020065, l5: 0.032560, l6: 0.067231\n","\n","[epoch: 16622/100000, batch:    12/    1, ite: 16622] train loss: 0.118963, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001508, l3: 0.011662, l4: 0.020067, l5: 0.033759, l6: 0.048974\n","\n","[epoch: 16623/100000, batch:    12/    1, ite: 16623] train loss: 0.118958, tar: 0.000066 \n","l0: 0.000073, l1: 0.000077, l2: 0.001493, l3: 0.011641, l4: 0.020060, l5: 0.033814, l6: 0.065620\n","\n","[epoch: 16624/100000, batch:    12/    1, ite: 16624] train loss: 0.118980, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001452, l3: 0.011771, l4: 0.019984, l5: 0.031878, l6: 0.051156\n","\n","[epoch: 16625/100000, batch:    12/    1, ite: 16625] train loss: 0.118976, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001290, l3: 0.011699, l4: 0.019827, l5: 0.031472, l6: 0.051975\n","\n","[epoch: 16626/100000, batch:    12/    1, ite: 16626] train loss: 0.118972, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001601, l3: 0.011602, l4: 0.020327, l5: 0.034226, l6: 0.064037\n","\n","[epoch: 16627/100000, batch:    12/    1, ite: 16627] train loss: 0.118993, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001556, l3: 0.011553, l4: 0.020497, l5: 0.033339, l6: 0.049777\n","\n","[epoch: 16628/100000, batch:    12/    1, ite: 16628] train loss: 0.118989, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001474, l3: 0.011105, l4: 0.019984, l5: 0.031283, l6: 0.053744\n","\n","[epoch: 16629/100000, batch:    12/    1, ite: 16629] train loss: 0.118987, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001107, l3: 0.011289, l4: 0.019214, l5: 0.030027, l6: 0.046145\n","\n","[epoch: 16630/100000, batch:    12/    1, ite: 16630] train loss: 0.118970, tar: 0.000066 \n","l0: 0.000074, l1: 0.000075, l2: 0.001385, l3: 0.011225, l4: 0.019935, l5: 0.030219, l6: 0.052102\n","\n","[epoch: 16631/100000, batch:    12/    1, ite: 16631] train loss: 0.118963, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001237, l3: 0.011269, l4: 0.019464, l5: 0.031164, l6: 0.049028\n","\n","[epoch: 16632/100000, batch:    12/    1, ite: 16632] train loss: 0.118953, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001349, l3: 0.011543, l4: 0.019696, l5: 0.032506, l6: 0.052604\n","\n","[epoch: 16633/100000, batch:    12/    1, ite: 16633] train loss: 0.118951, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001623, l3: 0.011766, l4: 0.020135, l5: 0.030980, l6: 0.052682\n","\n","[epoch: 16634/100000, batch:    12/    1, ite: 16634] train loss: 0.118948, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001109, l3: 0.010961, l4: 0.019216, l5: 0.031500, l6: 0.042269\n","\n","[epoch: 16635/100000, batch:    12/    1, ite: 16635] train loss: 0.118927, tar: 0.000066 \n","l0: 0.000058, l1: 0.000059, l2: 0.001346, l3: 0.011399, l4: 0.019398, l5: 0.029819, l6: 0.051920\n","\n","[epoch: 16636/100000, batch:    12/    1, ite: 16636] train loss: 0.118919, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001370, l3: 0.010940, l4: 0.019510, l5: 0.031469, l6: 0.042688\n","\n","[epoch: 16637/100000, batch:    12/    1, ite: 16637] train loss: 0.118899, tar: 0.000066 \n","l0: 0.000053, l1: 0.000053, l2: 0.001269, l3: 0.010986, l4: 0.019555, l5: 0.030654, l6: 0.056235\n","\n","[epoch: 16638/100000, batch:    12/    1, ite: 16638] train loss: 0.118899, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001336, l3: 0.011807, l4: 0.020565, l5: 0.032617, l6: 0.057619\n","\n","[epoch: 16639/100000, batch:    12/    1, ite: 16639] train loss: 0.118907, tar: 0.000066 \n","l0: 0.000057, l1: 0.000057, l2: 0.001194, l3: 0.011253, l4: 0.019644, l5: 0.030879, l6: 0.048977\n","\n","[epoch: 16640/100000, batch:    12/    1, ite: 16640] train loss: 0.118896, tar: 0.000066 \n","l0: 0.000064, l1: 0.000063, l2: 0.001263, l3: 0.011075, l4: 0.019914, l5: 0.030239, l6: 0.046996\n","\n","[epoch: 16641/100000, batch:    12/    1, ite: 16641] train loss: 0.118882, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001728, l3: 0.011722, l4: 0.020290, l5: 0.034557, l6: 0.066033\n","\n","[epoch: 16642/100000, batch:    12/    1, ite: 16642] train loss: 0.118906, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001205, l3: 0.011543, l4: 0.019918, l5: 0.031386, l6: 0.045759\n","\n","[epoch: 16643/100000, batch:    12/    1, ite: 16643] train loss: 0.118892, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001209, l3: 0.011743, l4: 0.019745, l5: 0.030123, l6: 0.057814\n","\n","[epoch: 16644/100000, batch:    12/    1, ite: 16644] train loss: 0.118895, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001657, l3: 0.011242, l4: 0.020542, l5: 0.034580, l6: 0.062525\n","\n","[epoch: 16645/100000, batch:    12/    1, ite: 16645] train loss: 0.118913, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001623, l3: 0.011260, l4: 0.020182, l5: 0.030520, l6: 0.059171\n","\n","[epoch: 16646/100000, batch:    12/    1, ite: 16646] train loss: 0.118919, tar: 0.000066 \n","l0: 0.000065, l1: 0.000060, l2: 0.001606, l3: 0.011172, l4: 0.019662, l5: 0.032148, l6: 0.062868\n","\n","[epoch: 16647/100000, batch:    12/    1, ite: 16647] train loss: 0.118933, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001206, l3: 0.011580, l4: 0.020015, l5: 0.034522, l6: 0.065192\n","\n","[epoch: 16648/100000, batch:    12/    1, ite: 16648] train loss: 0.118954, tar: 0.000066 \n","l0: 0.000073, l1: 0.000074, l2: 0.001311, l3: 0.010802, l4: 0.020234, l5: 0.033881, l6: 0.066718\n","\n","[epoch: 16649/100000, batch:    12/    1, ite: 16649] train loss: 0.118976, tar: 0.000066 \n","l0: 0.000053, l1: 0.000053, l2: 0.001339, l3: 0.011546, l4: 0.020359, l5: 0.031568, l6: 0.048159\n","\n","[epoch: 16650/100000, batch:    12/    1, ite: 16650] train loss: 0.118967, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001343, l3: 0.011058, l4: 0.019846, l5: 0.030667, l6: 0.058410\n","\n","[epoch: 16651/100000, batch:    12/    1, ite: 16651] train loss: 0.118970, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001431, l3: 0.011580, l4: 0.020698, l5: 0.032889, l6: 0.058071\n","\n","[epoch: 16652/100000, batch:    12/    1, ite: 16652] train loss: 0.118979, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001735, l3: 0.011057, l4: 0.020439, l5: 0.034226, l6: 0.066535\n","\n","[epoch: 16653/100000, batch:    12/    1, ite: 16653] train loss: 0.119003, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001620, l3: 0.011053, l4: 0.019894, l5: 0.030717, l6: 0.057701\n","\n","[epoch: 16654/100000, batch:    12/    1, ite: 16654] train loss: 0.119006, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001304, l3: 0.011202, l4: 0.020375, l5: 0.033668, l6: 0.048701\n","\n","[epoch: 16655/100000, batch:    12/    1, ite: 16655] train loss: 0.119000, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001325, l3: 0.010887, l4: 0.019461, l5: 0.031897, l6: 0.042716\n","\n","[epoch: 16656/100000, batch:    12/    1, ite: 16656] train loss: 0.118981, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001348, l3: 0.011690, l4: 0.019412, l5: 0.030046, l6: 0.047835\n","\n","[epoch: 16657/100000, batch:    12/    1, ite: 16657] train loss: 0.118968, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001321, l3: 0.011389, l4: 0.020100, l5: 0.030641, l6: 0.054307\n","\n","[epoch: 16658/100000, batch:    12/    1, ite: 16658] train loss: 0.118966, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001502, l3: 0.011479, l4: 0.019904, l5: 0.031785, l6: 0.046688\n","\n","[epoch: 16659/100000, batch:    12/    1, ite: 16659] train loss: 0.118955, tar: 0.000066 \n","l0: 0.000054, l1: 0.000055, l2: 0.001281, l3: 0.011113, l4: 0.020294, l5: 0.033771, l6: 0.049345\n","\n","[epoch: 16660/100000, batch:    12/    1, ite: 16660] train loss: 0.118951, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001497, l3: 0.011417, l4: 0.019828, l5: 0.030868, l6: 0.050523\n","\n","[epoch: 16661/100000, batch:    12/    1, ite: 16661] train loss: 0.118943, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001453, l3: 0.011497, l4: 0.019853, l5: 0.033067, l6: 0.047134\n","\n","[epoch: 16662/100000, batch:    12/    1, ite: 16662] train loss: 0.118935, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001237, l3: 0.011246, l4: 0.019839, l5: 0.031711, l6: 0.052800\n","\n","[epoch: 16663/100000, batch:    12/    1, ite: 16663] train loss: 0.118932, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001499, l3: 0.010881, l4: 0.019729, l5: 0.029990, l6: 0.045149\n","\n","[epoch: 16664/100000, batch:    12/    1, ite: 16664] train loss: 0.118914, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001394, l3: 0.011610, l4: 0.020049, l5: 0.030587, l6: 0.050959\n","\n","[epoch: 16665/100000, batch:    12/    1, ite: 16665] train loss: 0.118908, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001267, l3: 0.011010, l4: 0.019957, l5: 0.030380, l6: 0.054664\n","\n","[epoch: 16666/100000, batch:    12/    1, ite: 16666] train loss: 0.118906, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001545, l3: 0.011708, l4: 0.019854, l5: 0.030013, l6: 0.051369\n","\n","[epoch: 16667/100000, batch:    12/    1, ite: 16667] train loss: 0.118899, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001500, l3: 0.011746, l4: 0.020341, l5: 0.031716, l6: 0.048364\n","\n","[epoch: 16668/100000, batch:    12/    1, ite: 16668] train loss: 0.118892, tar: 0.000066 \n","l0: 0.000065, l1: 0.000064, l2: 0.001157, l3: 0.011787, l4: 0.020392, l5: 0.033174, l6: 0.058726\n","\n","[epoch: 16669/100000, batch:    12/    1, ite: 16669] train loss: 0.118901, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001375, l3: 0.010922, l4: 0.020299, l5: 0.033927, l6: 0.067319\n","\n","[epoch: 16670/100000, batch:    12/    1, ite: 16670] train loss: 0.118924, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001142, l3: 0.011310, l4: 0.019481, l5: 0.029271, l6: 0.048071\n","\n","[epoch: 16671/100000, batch:    12/    1, ite: 16671] train loss: 0.118910, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001559, l3: 0.011531, l4: 0.019933, l5: 0.033726, l6: 0.064005\n","\n","[epoch: 16672/100000, batch:    12/    1, ite: 16672] train loss: 0.118928, tar: 0.000066 \n","l0: 0.000074, l1: 0.000074, l2: 0.001387, l3: 0.010948, l4: 0.020147, l5: 0.033909, l6: 0.066748\n","\n","[epoch: 16673/100000, batch:    12/    1, ite: 16673] train loss: 0.118949, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001539, l3: 0.011105, l4: 0.019963, l5: 0.032148, l6: 0.046823\n","\n","[epoch: 16674/100000, batch:    12/    1, ite: 16674] train loss: 0.118938, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001319, l3: 0.010899, l4: 0.019358, l5: 0.029473, l6: 0.049647\n","\n","[epoch: 16675/100000, batch:    12/    1, ite: 16675] train loss: 0.118926, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001301, l3: 0.011575, l4: 0.019519, l5: 0.030419, l6: 0.051897\n","\n","[epoch: 16676/100000, batch:    12/    1, ite: 16676] train loss: 0.118920, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001220, l3: 0.011276, l4: 0.019863, l5: 0.030041, l6: 0.054156\n","\n","[epoch: 16677/100000, batch:    12/    1, ite: 16677] train loss: 0.118917, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001301, l3: 0.011810, l4: 0.020821, l5: 0.032737, l6: 0.061136\n","\n","[epoch: 16678/100000, batch:    12/    1, ite: 16678] train loss: 0.118930, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001230, l3: 0.011408, l4: 0.020599, l5: 0.032786, l6: 0.057532\n","\n","[epoch: 16679/100000, batch:    12/    1, ite: 16679] train loss: 0.118937, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001137, l3: 0.010938, l4: 0.019849, l5: 0.030124, l6: 0.044618\n","\n","[epoch: 16680/100000, batch:    12/    1, ite: 16680] train loss: 0.118919, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001271, l3: 0.011624, l4: 0.020180, l5: 0.032122, l6: 0.049818\n","\n","[epoch: 16681/100000, batch:    12/    1, ite: 16681] train loss: 0.118914, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001140, l3: 0.011181, l4: 0.020158, l5: 0.030255, l6: 0.048646\n","\n","[epoch: 16682/100000, batch:    12/    1, ite: 16682] train loss: 0.118903, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001140, l3: 0.011527, l4: 0.020006, l5: 0.033676, l6: 0.049113\n","\n","[epoch: 16683/100000, batch:    12/    1, ite: 16683] train loss: 0.118898, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001504, l3: 0.011160, l4: 0.020329, l5: 0.030362, l6: 0.048731\n","\n","[epoch: 16684/100000, batch:    12/    1, ite: 16684] train loss: 0.118888, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001657, l3: 0.011542, l4: 0.020358, l5: 0.034729, l6: 0.064991\n","\n","[epoch: 16685/100000, batch:    12/    1, ite: 16685] train loss: 0.118909, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001318, l3: 0.011474, l4: 0.019604, l5: 0.033448, l6: 0.048606\n","\n","[epoch: 16686/100000, batch:    12/    1, ite: 16686] train loss: 0.118903, tar: 0.000066 \n","l0: 0.000074, l1: 0.000075, l2: 0.001535, l3: 0.010944, l4: 0.020222, l5: 0.034226, l6: 0.067496\n","\n","[epoch: 16687/100000, batch:    12/    1, ite: 16687] train loss: 0.118926, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001468, l3: 0.011775, l4: 0.019854, l5: 0.031278, l6: 0.052073\n","\n","[epoch: 16688/100000, batch:    12/    1, ite: 16688] train loss: 0.118923, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001505, l3: 0.011112, l4: 0.019955, l5: 0.031956, l6: 0.053121\n","\n","[epoch: 16689/100000, batch:    12/    1, ite: 16689] train loss: 0.118921, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001480, l3: 0.011516, l4: 0.020483, l5: 0.031917, l6: 0.049876\n","\n","[epoch: 16690/100000, batch:    12/    1, ite: 16690] train loss: 0.118916, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001441, l3: 0.011132, l4: 0.020183, l5: 0.031867, l6: 0.052408\n","\n","[epoch: 16691/100000, batch:    12/    1, ite: 16691] train loss: 0.118913, tar: 0.000066 \n","l0: 0.000073, l1: 0.000075, l2: 0.001489, l3: 0.011169, l4: 0.020823, l5: 0.034951, l6: 0.067055\n","\n","[epoch: 16692/100000, batch:    12/    1, ite: 16692] train loss: 0.118937, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001532, l3: 0.011694, l4: 0.020367, l5: 0.031019, l6: 0.047722\n","\n","[epoch: 16693/100000, batch:    12/    1, ite: 16693] train loss: 0.118928, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001486, l3: 0.011110, l4: 0.019937, l5: 0.032916, l6: 0.047970\n","\n","[epoch: 16694/100000, batch:    12/    1, ite: 16694] train loss: 0.118920, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001213, l3: 0.011493, l4: 0.019675, l5: 0.029369, l6: 0.050113\n","\n","[epoch: 16695/100000, batch:    12/    1, ite: 16695] train loss: 0.118910, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001553, l3: 0.011068, l4: 0.019941, l5: 0.032716, l6: 0.063305\n","\n","[epoch: 16696/100000, batch:    12/    1, ite: 16696] train loss: 0.118924, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001366, l3: 0.011418, l4: 0.019930, l5: 0.030564, l6: 0.050515\n","\n","[epoch: 16697/100000, batch:    12/    1, ite: 16697] train loss: 0.118917, tar: 0.000066 \n","l0: 0.000077, l1: 0.000077, l2: 0.001659, l3: 0.011257, l4: 0.020578, l5: 0.034752, l6: 0.066870\n","\n","[epoch: 16698/100000, batch:    12/    1, ite: 16698] train loss: 0.118941, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001284, l3: 0.010854, l4: 0.019341, l5: 0.031302, l6: 0.047579\n","\n","[epoch: 16699/100000, batch:    12/    1, ite: 16699] train loss: 0.118929, tar: 0.000066 \n","l0: 0.000069, l1: 0.000069, l2: 0.001695, l3: 0.011768, l4: 0.020283, l5: 0.034369, l6: 0.065801\n","\n","[epoch: 16700/100000, batch:    12/    1, ite: 16700] train loss: 0.118950, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001448, l3: 0.011552, l4: 0.020350, l5: 0.031788, l6: 0.059309\n","\n","[epoch: 16701/100000, batch:    12/    1, ite: 16701] train loss: 0.118958, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001303, l3: 0.011000, l4: 0.020536, l5: 0.034053, l6: 0.067194\n","\n","[epoch: 16702/100000, batch:    12/    1, ite: 16702] train loss: 0.118980, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001335, l3: 0.011461, l4: 0.019822, l5: 0.030877, l6: 0.052511\n","\n","[epoch: 16703/100000, batch:    12/    1, ite: 16703] train loss: 0.118976, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001462, l3: 0.011850, l4: 0.020331, l5: 0.032485, l6: 0.056948\n","\n","[epoch: 16704/100000, batch:    12/    1, ite: 16704] train loss: 0.118982, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001445, l3: 0.011133, l4: 0.020376, l5: 0.032212, l6: 0.057763\n","\n","[epoch: 16705/100000, batch:    12/    1, ite: 16705] train loss: 0.118988, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001416, l3: 0.011291, l4: 0.020432, l5: 0.031010, l6: 0.053698\n","\n","[epoch: 16706/100000, batch:    12/    1, ite: 16706] train loss: 0.118986, tar: 0.000066 \n","l0: 0.000064, l1: 0.000068, l2: 0.001338, l3: 0.011261, l4: 0.020406, l5: 0.033853, l6: 0.062379\n","\n","[epoch: 16707/100000, batch:    12/    1, ite: 16707] train loss: 0.119001, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001307, l3: 0.011565, l4: 0.019621, l5: 0.030570, l6: 0.051941\n","\n","[epoch: 16708/100000, batch:    12/    1, ite: 16708] train loss: 0.118996, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001253, l3: 0.011295, l4: 0.020661, l5: 0.031641, l6: 0.055294\n","\n","[epoch: 16709/100000, batch:    12/    1, ite: 16709] train loss: 0.118997, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001302, l3: 0.011618, l4: 0.020074, l5: 0.032758, l6: 0.049335\n","\n","[epoch: 16710/100000, batch:    12/    1, ite: 16710] train loss: 0.118992, tar: 0.000066 \n","l0: 0.000069, l1: 0.000070, l2: 0.001291, l3: 0.011422, l4: 0.019200, l5: 0.031803, l6: 0.042959\n","\n","[epoch: 16711/100000, batch:    12/    1, ite: 16711] train loss: 0.118975, tar: 0.000066 \n","l0: 0.000068, l1: 0.000069, l2: 0.001287, l3: 0.011041, l4: 0.020213, l5: 0.033033, l6: 0.066615\n","\n","[epoch: 16712/100000, batch:    12/    1, ite: 16712] train loss: 0.118994, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001164, l3: 0.011502, l4: 0.020581, l5: 0.032518, l6: 0.049490\n","\n","[epoch: 16713/100000, batch:    12/    1, ite: 16713] train loss: 0.118989, tar: 0.000066 \n","l0: 0.000061, l1: 0.000062, l2: 0.001062, l3: 0.011203, l4: 0.018956, l5: 0.030633, l6: 0.054013\n","\n","[epoch: 16714/100000, batch:    12/    1, ite: 16714] train loss: 0.118984, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001392, l3: 0.011235, l4: 0.020400, l5: 0.030496, l6: 0.059268\n","\n","[epoch: 16715/100000, batch:    12/    1, ite: 16715] train loss: 0.118990, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001247, l3: 0.010861, l4: 0.019420, l5: 0.031123, l6: 0.042851\n","\n","[epoch: 16716/100000, batch:    12/    1, ite: 16716] train loss: 0.118971, tar: 0.000066 \n","l0: 0.000073, l1: 0.000068, l2: 0.001526, l3: 0.011081, l4: 0.019590, l5: 0.031119, l6: 0.047747\n","\n","[epoch: 16717/100000, batch:    12/    1, ite: 16717] train loss: 0.118960, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001286, l3: 0.011659, l4: 0.020326, l5: 0.031711, l6: 0.057704\n","\n","[epoch: 16718/100000, batch:    12/    1, ite: 16718] train loss: 0.118966, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001272, l3: 0.011067, l4: 0.020071, l5: 0.030028, l6: 0.048536\n","\n","[epoch: 16719/100000, batch:    12/    1, ite: 16719] train loss: 0.118955, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001312, l3: 0.011015, l4: 0.019544, l5: 0.031209, l6: 0.042989\n","\n","[epoch: 16720/100000, batch:    12/    1, ite: 16720] train loss: 0.118937, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001188, l3: 0.011097, l4: 0.019951, l5: 0.030492, l6: 0.046114\n","\n","[epoch: 16721/100000, batch:    12/    1, ite: 16721] train loss: 0.118923, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001223, l3: 0.011096, l4: 0.020017, l5: 0.030542, l6: 0.046309\n","\n","[epoch: 16722/100000, batch:    12/    1, ite: 16722] train loss: 0.118910, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001330, l3: 0.011500, l4: 0.019995, l5: 0.030239, l6: 0.050957\n","\n","[epoch: 16723/100000, batch:    12/    1, ite: 16723] train loss: 0.118903, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001141, l3: 0.010941, l4: 0.019359, l5: 0.030100, l6: 0.048976\n","\n","[epoch: 16724/100000, batch:    12/    1, ite: 16724] train loss: 0.118892, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001498, l3: 0.011114, l4: 0.020276, l5: 0.030888, l6: 0.051495\n","\n","[epoch: 16725/100000, batch:    12/    1, ite: 16725] train loss: 0.118887, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001314, l3: 0.011672, l4: 0.020209, l5: 0.031892, l6: 0.058902\n","\n","[epoch: 16726/100000, batch:    12/    1, ite: 16726] train loss: 0.118894, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001114, l3: 0.011336, l4: 0.020504, l5: 0.031097, l6: 0.055363\n","\n","[epoch: 16727/100000, batch:    12/    1, ite: 16727] train loss: 0.118895, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001467, l3: 0.011603, l4: 0.020680, l5: 0.033195, l6: 0.059912\n","\n","[epoch: 16728/100000, batch:    12/    1, ite: 16728] train loss: 0.118906, tar: 0.000066 \n","l0: 0.000068, l1: 0.000067, l2: 0.001168, l3: 0.011448, l4: 0.020371, l5: 0.031278, l6: 0.050591\n","\n","[epoch: 16729/100000, batch:    12/    1, ite: 16729] train loss: 0.118901, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001190, l3: 0.011237, l4: 0.019984, l5: 0.033486, l6: 0.063458\n","\n","[epoch: 16730/100000, batch:    12/    1, ite: 16730] train loss: 0.118916, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001102, l3: 0.011623, l4: 0.020289, l5: 0.033086, l6: 0.049917\n","\n","[epoch: 16731/100000, batch:    12/    1, ite: 16731] train loss: 0.118912, tar: 0.000066 \n","l0: 0.000072, l1: 0.000068, l2: 0.001554, l3: 0.011275, l4: 0.020254, l5: 0.032340, l6: 0.056018\n","\n","[epoch: 16732/100000, batch:    12/    1, ite: 16732] train loss: 0.118915, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001081, l3: 0.011673, l4: 0.020493, l5: 0.032564, l6: 0.058836\n","\n","[epoch: 16733/100000, batch:    12/    1, ite: 16733] train loss: 0.118923, tar: 0.000066 \n","l0: 0.000061, l1: 0.000062, l2: 0.001095, l3: 0.011120, l4: 0.019260, l5: 0.031612, l6: 0.043256\n","\n","[epoch: 16734/100000, batch:    12/    1, ite: 16734] train loss: 0.118906, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001300, l3: 0.011331, l4: 0.019312, l5: 0.031203, l6: 0.050697\n","\n","[epoch: 16735/100000, batch:    12/    1, ite: 16735] train loss: 0.118900, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001456, l3: 0.010948, l4: 0.019503, l5: 0.029350, l6: 0.050117\n","\n","[epoch: 16736/100000, batch:    12/    1, ite: 16736] train loss: 0.118890, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001117, l3: 0.010994, l4: 0.020009, l5: 0.030445, l6: 0.046752\n","\n","[epoch: 16737/100000, batch:    12/    1, ite: 16737] train loss: 0.118877, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001514, l3: 0.011203, l4: 0.020265, l5: 0.033090, l6: 0.060828\n","\n","[epoch: 16738/100000, batch:    12/    1, ite: 16738] train loss: 0.118888, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001234, l3: 0.011360, l4: 0.019982, l5: 0.032938, l6: 0.064461\n","\n","[epoch: 16739/100000, batch:    12/    1, ite: 16739] train loss: 0.118903, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001289, l3: 0.011265, l4: 0.019878, l5: 0.031834, l6: 0.045601\n","\n","[epoch: 16740/100000, batch:    12/    1, ite: 16740] train loss: 0.118891, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001099, l3: 0.011661, l4: 0.020362, l5: 0.031746, l6: 0.049305\n","\n","[epoch: 16741/100000, batch:    12/    1, ite: 16741] train loss: 0.118885, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001120, l3: 0.011587, l4: 0.020144, l5: 0.031171, l6: 0.049587\n","\n","[epoch: 16742/100000, batch:    12/    1, ite: 16742] train loss: 0.118878, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001326, l3: 0.011659, l4: 0.019777, l5: 0.029922, l6: 0.056151\n","\n","[epoch: 16743/100000, batch:    12/    1, ite: 16743] train loss: 0.118878, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001638, l3: 0.011717, l4: 0.020282, l5: 0.034560, l6: 0.065776\n","\n","[epoch: 16744/100000, batch:    12/    1, ite: 16744] train loss: 0.118899, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001267, l3: 0.011278, l4: 0.019175, l5: 0.032041, l6: 0.043891\n","\n","[epoch: 16745/100000, batch:    12/    1, ite: 16745] train loss: 0.118884, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001215, l3: 0.011118, l4: 0.019597, l5: 0.029034, l6: 0.050261\n","\n","[epoch: 16746/100000, batch:    12/    1, ite: 16746] train loss: 0.118874, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001086, l3: 0.011430, l4: 0.019558, l5: 0.031117, l6: 0.043891\n","\n","[epoch: 16747/100000, batch:    12/    1, ite: 16747] train loss: 0.118858, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001410, l3: 0.011563, l4: 0.020040, l5: 0.031847, l6: 0.054521\n","\n","[epoch: 16748/100000, batch:    12/    1, ite: 16748] train loss: 0.118859, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001528, l3: 0.010966, l4: 0.019024, l5: 0.032345, l6: 0.064535\n","\n","[epoch: 16749/100000, batch:    12/    1, ite: 16749] train loss: 0.118872, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001266, l3: 0.010713, l4: 0.019664, l5: 0.033111, l6: 0.064498\n","\n","[epoch: 16750/100000, batch:    12/    1, ite: 16750] train loss: 0.118886, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001475, l3: 0.011183, l4: 0.020351, l5: 0.031728, l6: 0.053932\n","\n","[epoch: 16751/100000, batch:    12/    1, ite: 16751] train loss: 0.118886, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001472, l3: 0.011103, l4: 0.020233, l5: 0.033033, l6: 0.049292\n","\n","[epoch: 16752/100000, batch:    12/    1, ite: 16752] train loss: 0.118881, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001230, l3: 0.011388, l4: 0.019427, l5: 0.031189, l6: 0.049804\n","\n","[epoch: 16753/100000, batch:    12/    1, ite: 16753] train loss: 0.118873, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001196, l3: 0.011608, l4: 0.019760, l5: 0.032922, l6: 0.046824\n","\n","[epoch: 16754/100000, batch:    12/    1, ite: 16754] train loss: 0.118865, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001294, l3: 0.011600, l4: 0.019550, l5: 0.029026, l6: 0.049413\n","\n","[epoch: 16755/100000, batch:    12/    1, ite: 16755] train loss: 0.118854, tar: 0.000066 \n","l0: 0.000057, l1: 0.000057, l2: 0.001084, l3: 0.010951, l4: 0.018901, l5: 0.030544, l6: 0.047487\n","\n","[epoch: 16756/100000, batch:    12/    1, ite: 16756] train loss: 0.118841, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001134, l3: 0.011267, l4: 0.019952, l5: 0.032529, l6: 0.052489\n","\n","[epoch: 16757/100000, batch:    12/    1, ite: 16757] train loss: 0.118840, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001092, l3: 0.011394, l4: 0.020032, l5: 0.033208, l6: 0.049193\n","\n","[epoch: 16758/100000, batch:    12/    1, ite: 16758] train loss: 0.118835, tar: 0.000066 \n","l0: 0.000056, l1: 0.000061, l2: 0.001535, l3: 0.011066, l4: 0.020424, l5: 0.034729, l6: 0.066468\n","\n","[epoch: 16759/100000, batch:    12/    1, ite: 16759] train loss: 0.118855, tar: 0.000066 \n","l0: 0.000071, l1: 0.000070, l2: 0.001424, l3: 0.010881, l4: 0.019828, l5: 0.029715, l6: 0.047399\n","\n","[epoch: 16760/100000, batch:    12/    1, ite: 16760] train loss: 0.118843, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001540, l3: 0.011084, l4: 0.020148, l5: 0.032570, l6: 0.047636\n","\n","[epoch: 16761/100000, batch:    12/    1, ite: 16761] train loss: 0.118835, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001263, l3: 0.011521, l4: 0.019700, l5: 0.029665, l6: 0.051219\n","\n","[epoch: 16762/100000, batch:    12/    1, ite: 16762] train loss: 0.118828, tar: 0.000066 \n","l0: 0.000068, l1: 0.000063, l2: 0.001436, l3: 0.011211, l4: 0.020032, l5: 0.030881, l6: 0.048735\n","\n","[epoch: 16763/100000, batch:    12/    1, ite: 16763] train loss: 0.118820, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001171, l3: 0.011509, l4: 0.020007, l5: 0.031719, l6: 0.052020\n","\n","[epoch: 16764/100000, batch:    12/    1, ite: 16764] train loss: 0.118817, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001362, l3: 0.011113, l4: 0.020229, l5: 0.031832, l6: 0.051250\n","\n","[epoch: 16765/100000, batch:    12/    1, ite: 16765] train loss: 0.118813, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001240, l3: 0.011364, l4: 0.019408, l5: 0.029879, l6: 0.051174\n","\n","[epoch: 16766/100000, batch:    12/    1, ite: 16766] train loss: 0.118806, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001550, l3: 0.011793, l4: 0.020962, l5: 0.032520, l6: 0.061424\n","\n","[epoch: 16767/100000, batch:    12/    1, ite: 16767] train loss: 0.118818, tar: 0.000066 \n","l0: 0.000063, l1: 0.000067, l2: 0.001100, l3: 0.011379, l4: 0.020314, l5: 0.031382, l6: 0.050538\n","\n","[epoch: 16768/100000, batch:    12/    1, ite: 16768] train loss: 0.118813, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001130, l3: 0.011294, l4: 0.020120, l5: 0.033564, l6: 0.048771\n","\n","[epoch: 16769/100000, batch:    12/    1, ite: 16769] train loss: 0.118808, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001143, l3: 0.011324, l4: 0.020851, l5: 0.031932, l6: 0.059910\n","\n","[epoch: 16770/100000, batch:    12/    1, ite: 16770] train loss: 0.118816, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001199, l3: 0.010738, l4: 0.019779, l5: 0.032486, l6: 0.066119\n","\n","[epoch: 16771/100000, batch:    12/    1, ite: 16771] train loss: 0.118831, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001372, l3: 0.010951, l4: 0.019121, l5: 0.031454, l6: 0.049012\n","\n","[epoch: 16772/100000, batch:    12/    1, ite: 16772] train loss: 0.118823, tar: 0.000066 \n","l0: 0.000064, l1: 0.000060, l2: 0.001420, l3: 0.011059, l4: 0.019827, l5: 0.032166, l6: 0.046612\n","\n","[epoch: 16773/100000, batch:    12/    1, ite: 16773] train loss: 0.118813, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001058, l3: 0.011300, l4: 0.020355, l5: 0.033533, l6: 0.049061\n","\n","[epoch: 16774/100000, batch:    12/    1, ite: 16774] train loss: 0.118808, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001223, l3: 0.011181, l4: 0.019986, l5: 0.032685, l6: 0.064119\n","\n","[epoch: 16775/100000, batch:    12/    1, ite: 16775] train loss: 0.118822, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001436, l3: 0.011225, l4: 0.020190, l5: 0.030282, l6: 0.054011\n","\n","[epoch: 16776/100000, batch:    12/    1, ite: 16776] train loss: 0.118820, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001076, l3: 0.011468, l4: 0.019505, l5: 0.030187, l6: 0.052997\n","\n","[epoch: 16777/100000, batch:    12/    1, ite: 16777] train loss: 0.118816, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001486, l3: 0.011260, l4: 0.020430, l5: 0.033950, l6: 0.060857\n","\n","[epoch: 16778/100000, batch:    12/    1, ite: 16778] train loss: 0.118827, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001173, l3: 0.011219, l4: 0.019634, l5: 0.029147, l6: 0.050246\n","\n","[epoch: 16779/100000, batch:    12/    1, ite: 16779] train loss: 0.118818, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001281, l3: 0.011476, l4: 0.020181, l5: 0.032867, l6: 0.050286\n","\n","[epoch: 16780/100000, batch:    12/    1, ite: 16780] train loss: 0.118815, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001076, l3: 0.011243, l4: 0.020033, l5: 0.032421, l6: 0.052709\n","\n","[epoch: 16781/100000, batch:    12/    1, ite: 16781] train loss: 0.118813, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001226, l3: 0.011152, l4: 0.019907, l5: 0.032846, l6: 0.064270\n","\n","[epoch: 16782/100000, batch:    12/    1, ite: 16782] train loss: 0.118827, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001413, l3: 0.011458, l4: 0.019586, l5: 0.032649, l6: 0.064031\n","\n","[epoch: 16783/100000, batch:    12/    1, ite: 16783] train loss: 0.118840, tar: 0.000066 \n","l0: 0.000056, l1: 0.000056, l2: 0.001278, l3: 0.011237, l4: 0.019868, l5: 0.031939, l6: 0.045995\n","\n","[epoch: 16784/100000, batch:    12/    1, ite: 16784] train loss: 0.118830, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001240, l3: 0.011128, l4: 0.019781, l5: 0.029771, l6: 0.047684\n","\n","[epoch: 16785/100000, batch:    12/    1, ite: 16785] train loss: 0.118818, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001413, l3: 0.011079, l4: 0.020105, l5: 0.032024, l6: 0.053880\n","\n","[epoch: 16786/100000, batch:    12/    1, ite: 16786] train loss: 0.118818, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001222, l3: 0.011303, l4: 0.020217, l5: 0.030635, l6: 0.046995\n","\n","[epoch: 16787/100000, batch:    12/    1, ite: 16787] train loss: 0.118807, tar: 0.000066 \n","l0: 0.000062, l1: 0.000067, l2: 0.001412, l3: 0.011475, l4: 0.019531, l5: 0.032647, l6: 0.046021\n","\n","[epoch: 16788/100000, batch:    12/    1, ite: 16788] train loss: 0.118798, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001149, l3: 0.011595, l4: 0.020067, l5: 0.030516, l6: 0.052436\n","\n","[epoch: 16789/100000, batch:    12/    1, ite: 16789] train loss: 0.118794, tar: 0.000066 \n","l0: 0.000053, l1: 0.000058, l2: 0.001270, l3: 0.011397, l4: 0.019222, l5: 0.032464, l6: 0.044146\n","\n","[epoch: 16790/100000, batch:    12/    1, ite: 16790] train loss: 0.118781, tar: 0.000066 \n","l0: 0.000073, l1: 0.000072, l2: 0.001332, l3: 0.011642, l4: 0.019770, l5: 0.033083, l6: 0.046830\n","\n","[epoch: 16791/100000, batch:    12/    1, ite: 16791] train loss: 0.118773, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001349, l3: 0.011819, l4: 0.020345, l5: 0.034159, l6: 0.063697\n","\n","[epoch: 16792/100000, batch:    12/    1, ite: 16792] train loss: 0.118789, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001174, l3: 0.011392, l4: 0.020208, l5: 0.031340, l6: 0.057680\n","\n","[epoch: 16793/100000, batch:    12/    1, ite: 16793] train loss: 0.118793, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001133, l3: 0.011319, l4: 0.020610, l5: 0.033241, l6: 0.049374\n","\n","[epoch: 16794/100000, batch:    12/    1, ite: 16794] train loss: 0.118790, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001408, l3: 0.011679, l4: 0.019820, l5: 0.032352, l6: 0.053183\n","\n","[epoch: 16795/100000, batch:    12/    1, ite: 16795] train loss: 0.118789, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001092, l3: 0.011257, l4: 0.020257, l5: 0.032166, l6: 0.047826\n","\n","[epoch: 16796/100000, batch:    12/    1, ite: 16796] train loss: 0.118782, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001188, l3: 0.011546, l4: 0.019335, l5: 0.030658, l6: 0.056843\n","\n","[epoch: 16797/100000, batch:    12/    1, ite: 16797] train loss: 0.118783, tar: 0.000066 \n","l0: 0.000053, l1: 0.000054, l2: 0.001275, l3: 0.011124, l4: 0.019994, l5: 0.029486, l6: 0.048670\n","\n","[epoch: 16798/100000, batch:    12/    1, ite: 16798] train loss: 0.118773, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001177, l3: 0.011243, l4: 0.020110, l5: 0.033680, l6: 0.062546\n","\n","[epoch: 16799/100000, batch:    12/    1, ite: 16799] train loss: 0.118785, tar: 0.000066 \n","l0: 0.000053, l1: 0.000057, l2: 0.001234, l3: 0.011568, l4: 0.019893, l5: 0.033339, l6: 0.048533\n","\n","[epoch: 16800/100000, batch:    12/    1, ite: 16800] train loss: 0.118780, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001139, l3: 0.011248, l4: 0.020395, l5: 0.033433, l6: 0.048426\n","\n","[epoch: 16801/100000, batch:    12/    1, ite: 16801] train loss: 0.118775, tar: 0.000066 \n","l0: 0.000069, l1: 0.000070, l2: 0.001523, l3: 0.011773, l4: 0.020762, l5: 0.031799, l6: 0.049255\n","\n","[epoch: 16802/100000, batch:    12/    1, ite: 16802] train loss: 0.118771, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001333, l3: 0.011612, l4: 0.019909, l5: 0.031125, l6: 0.059940\n","\n","[epoch: 16803/100000, batch:    12/    1, ite: 16803] train loss: 0.118777, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001124, l3: 0.011193, l4: 0.020190, l5: 0.032538, l6: 0.047376\n","\n","[epoch: 16804/100000, batch:    12/    1, ite: 16804] train loss: 0.118770, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001486, l3: 0.011612, l4: 0.020496, l5: 0.032782, l6: 0.048579\n","\n","[epoch: 16805/100000, batch:    12/    1, ite: 16805] train loss: 0.118765, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001180, l3: 0.011557, l4: 0.020012, l5: 0.033469, l6: 0.048087\n","\n","[epoch: 16806/100000, batch:    12/    1, ite: 16806] train loss: 0.118760, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001150, l3: 0.011721, l4: 0.020405, l5: 0.032615, l6: 0.059137\n","\n","[epoch: 16807/100000, batch:    12/    1, ite: 16807] train loss: 0.118768, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001096, l3: 0.011228, l4: 0.020409, l5: 0.033534, l6: 0.048534\n","\n","[epoch: 16808/100000, batch:    12/    1, ite: 16808] train loss: 0.118763, tar: 0.000066 \n","l0: 0.000048, l1: 0.000052, l2: 0.001235, l3: 0.011321, l4: 0.018878, l5: 0.030809, l6: 0.052371\n","\n","[epoch: 16809/100000, batch:    12/    1, ite: 16809] train loss: 0.118758, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001115, l3: 0.011607, l4: 0.020217, l5: 0.030501, l6: 0.051829\n","\n","[epoch: 16810/100000, batch:    12/    1, ite: 16810] train loss: 0.118754, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001405, l3: 0.011226, l4: 0.019921, l5: 0.032393, l6: 0.049412\n","\n","[epoch: 16811/100000, batch:    12/    1, ite: 16811] train loss: 0.118748, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001532, l3: 0.011501, l4: 0.020465, l5: 0.032082, l6: 0.049894\n","\n","[epoch: 16812/100000, batch:    12/    1, ite: 16812] train loss: 0.118745, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001116, l3: 0.011416, l4: 0.019337, l5: 0.030211, l6: 0.046427\n","\n","[epoch: 16813/100000, batch:    12/    1, ite: 16813] train loss: 0.118732, tar: 0.000066 \n","l0: 0.000073, l1: 0.000072, l2: 0.001169, l3: 0.011619, l4: 0.020747, l5: 0.032049, l6: 0.059671\n","\n","[epoch: 16814/100000, batch:    12/    1, ite: 16814] train loss: 0.118740, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001498, l3: 0.011514, l4: 0.020584, l5: 0.031894, l6: 0.049912\n","\n","[epoch: 16815/100000, batch:    12/    1, ite: 16815] train loss: 0.118736, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001208, l3: 0.011497, l4: 0.020008, l5: 0.033356, l6: 0.048227\n","\n","[epoch: 16816/100000, batch:    12/    1, ite: 16816] train loss: 0.118731, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001302, l3: 0.011908, l4: 0.020321, l5: 0.031883, l6: 0.061980\n","\n","[epoch: 16817/100000, batch:    12/    1, ite: 16817] train loss: 0.118742, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001374, l3: 0.011354, l4: 0.020353, l5: 0.034526, l6: 0.062431\n","\n","[epoch: 16818/100000, batch:    12/    1, ite: 16818] train loss: 0.118756, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001523, l3: 0.011039, l4: 0.019774, l5: 0.032548, l6: 0.045738\n","\n","[epoch: 16819/100000, batch:    12/    1, ite: 16819] train loss: 0.118746, tar: 0.000066 \n","l0: 0.000069, l1: 0.000071, l2: 0.001170, l3: 0.011579, l4: 0.019806, l5: 0.033245, l6: 0.065463\n","\n","[epoch: 16820/100000, batch:    12/    1, ite: 16820] train loss: 0.118762, tar: 0.000066 \n","l0: 0.000070, l1: 0.000075, l2: 0.001653, l3: 0.011107, l4: 0.020159, l5: 0.033611, l6: 0.066335\n","\n","[epoch: 16821/100000, batch:    12/    1, ite: 16821] train loss: 0.118779, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001156, l3: 0.011427, l4: 0.019275, l5: 0.030139, l6: 0.046835\n","\n","[epoch: 16822/100000, batch:    12/    1, ite: 16822] train loss: 0.118767, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001450, l3: 0.011576, l4: 0.019751, l5: 0.031157, l6: 0.061050\n","\n","[epoch: 16823/100000, batch:    12/    1, ite: 16823] train loss: 0.118775, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001374, l3: 0.011545, l4: 0.020109, l5: 0.033388, l6: 0.051137\n","\n","[epoch: 16824/100000, batch:    12/    1, ite: 16824] train loss: 0.118773, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001436, l3: 0.011683, l4: 0.020328, l5: 0.031018, l6: 0.047218\n","\n","[epoch: 16825/100000, batch:    12/    1, ite: 16825] train loss: 0.118765, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001193, l3: 0.011694, l4: 0.020321, l5: 0.033026, l6: 0.058934\n","\n","[epoch: 16826/100000, batch:    12/    1, ite: 16826] train loss: 0.118773, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001152, l3: 0.011304, l4: 0.020595, l5: 0.033708, l6: 0.049324\n","\n","[epoch: 16827/100000, batch:    12/    1, ite: 16827] train loss: 0.118770, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001440, l3: 0.011116, l4: 0.019683, l5: 0.030118, l6: 0.051560\n","\n","[epoch: 16828/100000, batch:    12/    1, ite: 16828] train loss: 0.118764, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001476, l3: 0.011100, l4: 0.020011, l5: 0.030685, l6: 0.048807\n","\n","[epoch: 16829/100000, batch:    12/    1, ite: 16829] train loss: 0.118756, tar: 0.000066 \n","l0: 0.000070, l1: 0.000069, l2: 0.001514, l3: 0.011137, l4: 0.019869, l5: 0.032547, l6: 0.067287\n","\n","[epoch: 16830/100000, batch:    12/    1, ite: 16830] train loss: 0.118773, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001104, l3: 0.011091, l4: 0.019938, l5: 0.031126, l6: 0.059277\n","\n","[epoch: 16831/100000, batch:    12/    1, ite: 16831] train loss: 0.118777, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001131, l3: 0.011473, l4: 0.019498, l5: 0.030671, l6: 0.057650\n","\n","[epoch: 16832/100000, batch:    12/    1, ite: 16832] train loss: 0.118779, tar: 0.000066 \n","l0: 0.000073, l1: 0.000073, l2: 0.001457, l3: 0.011624, l4: 0.020058, l5: 0.034318, l6: 0.066272\n","\n","[epoch: 16833/100000, batch:    12/    1, ite: 16833] train loss: 0.118798, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001305, l3: 0.011537, l4: 0.019595, l5: 0.029309, l6: 0.050738\n","\n","[epoch: 16834/100000, batch:    12/    1, ite: 16834] train loss: 0.118790, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001479, l3: 0.011653, l4: 0.020682, l5: 0.032336, l6: 0.057254\n","\n","[epoch: 16835/100000, batch:    12/    1, ite: 16835] train loss: 0.118796, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001113, l3: 0.011629, l4: 0.019622, l5: 0.033121, l6: 0.046372\n","\n","[epoch: 16836/100000, batch:    12/    1, ite: 16836] train loss: 0.118788, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001142, l3: 0.011480, l4: 0.019895, l5: 0.031584, l6: 0.061294\n","\n","[epoch: 16837/100000, batch:    12/    1, ite: 16837] train loss: 0.118796, tar: 0.000066 \n","l0: 0.000069, l1: 0.000070, l2: 0.001278, l3: 0.011003, l4: 0.020200, l5: 0.034383, l6: 0.066954\n","\n","[epoch: 16838/100000, batch:    12/    1, ite: 16838] train loss: 0.118814, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001393, l3: 0.011195, l4: 0.019230, l5: 0.032425, l6: 0.063341\n","\n","[epoch: 16839/100000, batch:    12/    1, ite: 16839] train loss: 0.118824, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001218, l3: 0.011733, l4: 0.020494, l5: 0.034949, l6: 0.066747\n","\n","[epoch: 16840/100000, batch:    12/    1, ite: 16840] train loss: 0.118844, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001419, l3: 0.011512, l4: 0.020140, l5: 0.031761, l6: 0.049946\n","\n","[epoch: 16841/100000, batch:    12/    1, ite: 16841] train loss: 0.118839, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001298, l3: 0.011741, l4: 0.020578, l5: 0.033533, l6: 0.059317\n","\n","[epoch: 16842/100000, batch:    12/    1, ite: 16842] train loss: 0.118849, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001155, l3: 0.011177, l4: 0.019827, l5: 0.031939, l6: 0.052537\n","\n","[epoch: 16843/100000, batch:    12/    1, ite: 16843] train loss: 0.118846, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001157, l3: 0.011064, l4: 0.019432, l5: 0.029867, l6: 0.049725\n","\n","[epoch: 16844/100000, batch:    12/    1, ite: 16844] train loss: 0.118837, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001166, l3: 0.011347, l4: 0.020841, l5: 0.031196, l6: 0.060348\n","\n","[epoch: 16845/100000, batch:    12/    1, ite: 16845] train loss: 0.118845, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001315, l3: 0.011637, l4: 0.020196, l5: 0.032653, l6: 0.054917\n","\n","[epoch: 16846/100000, batch:    12/    1, ite: 16846] train loss: 0.118847, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001479, l3: 0.011067, l4: 0.020019, l5: 0.030015, l6: 0.048562\n","\n","[epoch: 16847/100000, batch:    12/    1, ite: 16847] train loss: 0.118838, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001297, l3: 0.010974, l4: 0.019629, l5: 0.032178, l6: 0.046353\n","\n","[epoch: 16848/100000, batch:    12/    1, ite: 16848] train loss: 0.118828, tar: 0.000066 \n","l0: 0.000063, l1: 0.000062, l2: 0.001159, l3: 0.011175, l4: 0.019187, l5: 0.031724, l6: 0.043838\n","\n","[epoch: 16849/100000, batch:    12/    1, ite: 16849] train loss: 0.118815, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001149, l3: 0.011652, l4: 0.020537, l5: 0.031187, l6: 0.049823\n","\n","[epoch: 16850/100000, batch:    12/    1, ite: 16850] train loss: 0.118810, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001271, l3: 0.010911, l4: 0.020289, l5: 0.034434, l6: 0.067521\n","\n","[epoch: 16851/100000, batch:    12/    1, ite: 16851] train loss: 0.118828, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001131, l3: 0.011185, l4: 0.019879, l5: 0.033458, l6: 0.049065\n","\n","[epoch: 16852/100000, batch:    12/    1, ite: 16852] train loss: 0.118823, tar: 0.000066 \n","l0: 0.000065, l1: 0.000066, l2: 0.001485, l3: 0.011517, l4: 0.019295, l5: 0.032007, l6: 0.042883\n","\n","[epoch: 16853/100000, batch:    12/    1, ite: 16853] train loss: 0.118810, tar: 0.000066 \n","l0: 0.000054, l1: 0.000055, l2: 0.001384, l3: 0.011574, l4: 0.019852, l5: 0.033514, l6: 0.048586\n","\n","[epoch: 16854/100000, batch:    12/    1, ite: 16854] train loss: 0.118805, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001436, l3: 0.011426, l4: 0.019378, l5: 0.030149, l6: 0.048417\n","\n","[epoch: 16855/100000, batch:    12/    1, ite: 16855] train loss: 0.118796, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001157, l3: 0.011341, l4: 0.019484, l5: 0.031650, l6: 0.047411\n","\n","[epoch: 16856/100000, batch:    12/    1, ite: 16856] train loss: 0.118787, tar: 0.000066 \n","l0: 0.000076, l1: 0.000077, l2: 0.001373, l3: 0.011036, l4: 0.019849, l5: 0.029916, l6: 0.052125\n","\n","[epoch: 16857/100000, batch:    12/    1, ite: 16857] train loss: 0.118782, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001171, l3: 0.011392, l4: 0.020334, l5: 0.031262, l6: 0.060202\n","\n","[epoch: 16858/100000, batch:    12/    1, ite: 16858] train loss: 0.118789, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001209, l3: 0.011088, l4: 0.019650, l5: 0.033280, l6: 0.063058\n","\n","[epoch: 16859/100000, batch:    12/    1, ite: 16859] train loss: 0.118800, tar: 0.000066 \n","l0: 0.000059, l1: 0.000063, l2: 0.001124, l3: 0.011193, l4: 0.020141, l5: 0.033122, l6: 0.048698\n","\n","[epoch: 16860/100000, batch:    12/    1, ite: 16860] train loss: 0.118795, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001486, l3: 0.010956, l4: 0.019393, l5: 0.031389, l6: 0.049325\n","\n","[epoch: 16861/100000, batch:    12/    1, ite: 16861] train loss: 0.118788, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001117, l3: 0.011438, l4: 0.019562, l5: 0.031005, l6: 0.043171\n","\n","[epoch: 16862/100000, batch:    12/    1, ite: 16862] train loss: 0.118774, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001547, l3: 0.010982, l4: 0.019737, l5: 0.033510, l6: 0.067215\n","\n","[epoch: 16863/100000, batch:    12/    1, ite: 16863] train loss: 0.118790, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001477, l3: 0.011603, l4: 0.019754, l5: 0.032998, l6: 0.047827\n","\n","[epoch: 16864/100000, batch:    12/    1, ite: 16864] train loss: 0.118784, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001426, l3: 0.011285, l4: 0.019831, l5: 0.030216, l6: 0.043441\n","\n","[epoch: 16865/100000, batch:    12/    1, ite: 16865] train loss: 0.118770, tar: 0.000066 \n","l0: 0.000068, l1: 0.000068, l2: 0.001325, l3: 0.011446, l4: 0.020246, l5: 0.034155, l6: 0.062174\n","\n","[epoch: 16866/100000, batch:    12/    1, ite: 16866] train loss: 0.118782, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001271, l3: 0.011262, l4: 0.020470, l5: 0.031964, l6: 0.054388\n","\n","[epoch: 16867/100000, batch:    12/    1, ite: 16867] train loss: 0.118783, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001133, l3: 0.011435, l4: 0.019820, l5: 0.031402, l6: 0.051462\n","\n","[epoch: 16868/100000, batch:    12/    1, ite: 16868] train loss: 0.118779, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001086, l3: 0.011380, l4: 0.019596, l5: 0.030978, l6: 0.048822\n","\n","[epoch: 16869/100000, batch:    12/    1, ite: 16869] train loss: 0.118771, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001459, l3: 0.011765, l4: 0.020733, l5: 0.032294, l6: 0.049021\n","\n","[epoch: 16870/100000, batch:    12/    1, ite: 16870] train loss: 0.118768, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001172, l3: 0.011603, l4: 0.020577, l5: 0.032421, l6: 0.059051\n","\n","[epoch: 16871/100000, batch:    12/    1, ite: 16871] train loss: 0.118775, tar: 0.000066 \n","l0: 0.000055, l1: 0.000055, l2: 0.001259, l3: 0.011074, l4: 0.020287, l5: 0.031786, l6: 0.051181\n","\n","[epoch: 16872/100000, batch:    12/    1, ite: 16872] train loss: 0.118771, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001220, l3: 0.011469, l4: 0.020300, l5: 0.034813, l6: 0.066229\n","\n","[epoch: 16873/100000, batch:    12/    1, ite: 16873] train loss: 0.118789, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001226, l3: 0.011485, l4: 0.020337, l5: 0.031672, l6: 0.053180\n","\n","[epoch: 16874/100000, batch:    12/    1, ite: 16874] train loss: 0.118788, tar: 0.000066 \n","l0: 0.000071, l1: 0.000070, l2: 0.001288, l3: 0.011469, l4: 0.019541, l5: 0.031820, l6: 0.053137\n","\n","[epoch: 16875/100000, batch:    12/    1, ite: 16875] train loss: 0.118786, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001195, l3: 0.011560, l4: 0.020057, l5: 0.033352, l6: 0.048611\n","\n","[epoch: 16876/100000, batch:    12/    1, ite: 16876] train loss: 0.118782, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001255, l3: 0.011832, l4: 0.020805, l5: 0.032716, l6: 0.061231\n","\n","[epoch: 16877/100000, batch:    12/    1, ite: 16877] train loss: 0.118792, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001138, l3: 0.011637, l4: 0.020384, l5: 0.031679, l6: 0.049375\n","\n","[epoch: 16878/100000, batch:    12/    1, ite: 16878] train loss: 0.118787, tar: 0.000066 \n","l0: 0.000055, l1: 0.000055, l2: 0.001309, l3: 0.011514, l4: 0.020462, l5: 0.030785, l6: 0.048943\n","\n","[epoch: 16879/100000, batch:    12/    1, ite: 16879] train loss: 0.118781, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001138, l3: 0.011279, l4: 0.019707, l5: 0.030700, l6: 0.043071\n","\n","[epoch: 16880/100000, batch:    12/    1, ite: 16880] train loss: 0.118766, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001450, l3: 0.011134, l4: 0.020117, l5: 0.032982, l6: 0.048061\n","\n","[epoch: 16881/100000, batch:    12/    1, ite: 16881] train loss: 0.118761, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001151, l3: 0.011647, l4: 0.020451, l5: 0.031176, l6: 0.049646\n","\n","[epoch: 16882/100000, batch:    12/    1, ite: 16882] train loss: 0.118756, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001389, l3: 0.011153, l4: 0.020192, l5: 0.031008, l6: 0.050783\n","\n","[epoch: 16883/100000, batch:    12/    1, ite: 16883] train loss: 0.118751, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001162, l3: 0.011581, l4: 0.020113, l5: 0.034083, l6: 0.065540\n","\n","[epoch: 16884/100000, batch:    12/    1, ite: 16884] train loss: 0.118767, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001123, l3: 0.011352, l4: 0.019578, l5: 0.030841, l6: 0.044077\n","\n","[epoch: 16885/100000, batch:    12/    1, ite: 16885] train loss: 0.118753, tar: 0.000066 \n","l0: 0.000065, l1: 0.000065, l2: 0.001120, l3: 0.011568, l4: 0.020340, l5: 0.031210, l6: 0.048102\n","\n","[epoch: 16886/100000, batch:    12/    1, ite: 16886] train loss: 0.118746, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001480, l3: 0.011248, l4: 0.020099, l5: 0.030382, l6: 0.052554\n","\n","[epoch: 16887/100000, batch:    12/    1, ite: 16887] train loss: 0.118743, tar: 0.000066 \n","l0: 0.000057, l1: 0.000057, l2: 0.001078, l3: 0.010942, l4: 0.018859, l5: 0.030476, l6: 0.047573\n","\n","[epoch: 16888/100000, batch:    12/    1, ite: 16888] train loss: 0.118732, tar: 0.000066 \n","l0: 0.000072, l1: 0.000071, l2: 0.001298, l3: 0.011705, l4: 0.020585, l5: 0.032671, l6: 0.049464\n","\n","[epoch: 16889/100000, batch:    12/    1, ite: 16889] train loss: 0.118729, tar: 0.000066 \n","l0: 0.000057, l1: 0.000057, l2: 0.001125, l3: 0.010976, l4: 0.019217, l5: 0.032443, l6: 0.062634\n","\n","[epoch: 16890/100000, batch:    12/    1, ite: 16890] train loss: 0.118738, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001176, l3: 0.011302, l4: 0.020281, l5: 0.034662, l6: 0.063407\n","\n","[epoch: 16891/100000, batch:    12/    1, ite: 16891] train loss: 0.118751, tar: 0.000066 \n","l0: 0.000061, l1: 0.000061, l2: 0.001119, l3: 0.011293, l4: 0.019387, l5: 0.030187, l6: 0.046683\n","\n","[epoch: 16892/100000, batch:    12/    1, ite: 16892] train loss: 0.118740, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001297, l3: 0.011547, l4: 0.020118, l5: 0.031039, l6: 0.045433\n","\n","[epoch: 16893/100000, batch:    12/    1, ite: 16893] train loss: 0.118730, tar: 0.000066 \n","l0: 0.000055, l1: 0.000055, l2: 0.001380, l3: 0.010903, l4: 0.019502, l5: 0.032515, l6: 0.063978\n","\n","[epoch: 16894/100000, batch:    12/    1, ite: 16894] train loss: 0.118741, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001121, l3: 0.011513, l4: 0.020410, l5: 0.033003, l6: 0.049479\n","\n","[epoch: 16895/100000, batch:    12/    1, ite: 16895] train loss: 0.118737, tar: 0.000066 \n","l0: 0.000065, l1: 0.000060, l2: 0.001467, l3: 0.011313, l4: 0.020525, l5: 0.031668, l6: 0.054189\n","\n","[epoch: 16896/100000, batch:    12/    1, ite: 16896] train loss: 0.118738, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001065, l3: 0.011338, l4: 0.019542, l5: 0.030886, l6: 0.043536\n","\n","[epoch: 16897/100000, batch:    12/    1, ite: 16897] train loss: 0.118724, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001219, l3: 0.011161, l4: 0.020437, l5: 0.034697, l6: 0.063928\n","\n","[epoch: 16898/100000, batch:    12/    1, ite: 16898] train loss: 0.118739, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001262, l3: 0.011144, l4: 0.020090, l5: 0.030112, l6: 0.053915\n","\n","[epoch: 16899/100000, batch:    12/    1, ite: 16899] train loss: 0.118736, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001085, l3: 0.011607, l4: 0.020431, l5: 0.033271, l6: 0.048881\n","\n","[epoch: 16900/100000, batch:    12/    1, ite: 16900] train loss: 0.118733, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001383, l3: 0.011549, l4: 0.020384, l5: 0.032496, l6: 0.057218\n","\n","[epoch: 16901/100000, batch:    12/    1, ite: 16901] train loss: 0.118737, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001097, l3: 0.011719, l4: 0.019977, l5: 0.033661, l6: 0.047498\n","\n","[epoch: 16902/100000, batch:    12/    1, ite: 16902] train loss: 0.118732, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001086, l3: 0.011614, l4: 0.019914, l5: 0.029897, l6: 0.050706\n","\n","[epoch: 16903/100000, batch:    12/    1, ite: 16903] train loss: 0.118726, tar: 0.000066 \n","l0: 0.000076, l1: 0.000080, l2: 0.001249, l3: 0.010986, l4: 0.020189, l5: 0.033497, l6: 0.066579\n","\n","[epoch: 16904/100000, batch:    12/    1, ite: 16904] train loss: 0.118742, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001254, l3: 0.011762, l4: 0.020358, l5: 0.031792, l6: 0.061866\n","\n","[epoch: 16905/100000, batch:    12/    1, ite: 16905] train loss: 0.118751, tar: 0.000066 \n","l0: 0.000070, l1: 0.000071, l2: 0.001180, l3: 0.011188, l4: 0.020095, l5: 0.033436, l6: 0.062582\n","\n","[epoch: 16906/100000, batch:    12/    1, ite: 16906] train loss: 0.118762, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001182, l3: 0.011249, l4: 0.020059, l5: 0.033158, l6: 0.048861\n","\n","[epoch: 16907/100000, batch:    12/    1, ite: 16907] train loss: 0.118757, tar: 0.000066 \n","l0: 0.000071, l1: 0.000072, l2: 0.001307, l3: 0.011287, l4: 0.020503, l5: 0.031834, l6: 0.054262\n","\n","[epoch: 16908/100000, batch:    12/    1, ite: 16908] train loss: 0.118758, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001071, l3: 0.011468, l4: 0.020162, l5: 0.032433, l6: 0.050747\n","\n","[epoch: 16909/100000, batch:    12/    1, ite: 16909] train loss: 0.118755, tar: 0.000066 \n","l0: 0.000060, l1: 0.000060, l2: 0.001431, l3: 0.011058, l4: 0.019635, l5: 0.031805, l6: 0.043985\n","\n","[epoch: 16910/100000, batch:    12/    1, ite: 16910] train loss: 0.118743, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001463, l3: 0.011789, l4: 0.020448, l5: 0.032569, l6: 0.057900\n","\n","[epoch: 16911/100000, batch:    12/    1, ite: 16911] train loss: 0.118749, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001125, l3: 0.011096, l4: 0.019389, l5: 0.029006, l6: 0.050062\n","\n","[epoch: 16912/100000, batch:    12/    1, ite: 16912] train loss: 0.118741, tar: 0.000066 \n","l0: 0.000070, l1: 0.000070, l2: 0.001282, l3: 0.011636, l4: 0.019897, l5: 0.031736, l6: 0.052985\n","\n","[epoch: 16913/100000, batch:    12/    1, ite: 16913] train loss: 0.118739, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001136, l3: 0.011289, l4: 0.020093, l5: 0.033364, l6: 0.047636\n","\n","[epoch: 16914/100000, batch:    12/    1, ite: 16914] train loss: 0.118734, tar: 0.000066 \n","l0: 0.000072, l1: 0.000072, l2: 0.001322, l3: 0.011502, l4: 0.020212, l5: 0.031717, l6: 0.053465\n","\n","[epoch: 16915/100000, batch:    12/    1, ite: 16915] train loss: 0.118733, tar: 0.000066 \n","l0: 0.000076, l1: 0.000078, l2: 0.001295, l3: 0.011056, l4: 0.019797, l5: 0.030140, l6: 0.046217\n","\n","[epoch: 16916/100000, batch:    12/    1, ite: 16916] train loss: 0.118722, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001112, l3: 0.011125, l4: 0.020415, l5: 0.031778, l6: 0.050327\n","\n","[epoch: 16917/100000, batch:    12/    1, ite: 16917] train loss: 0.118718, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001308, l3: 0.011036, l4: 0.019811, l5: 0.030773, l6: 0.058386\n","\n","[epoch: 16918/100000, batch:    12/    1, ite: 16918] train loss: 0.118721, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001199, l3: 0.011177, l4: 0.020260, l5: 0.034344, l6: 0.062981\n","\n","[epoch: 16919/100000, batch:    12/    1, ite: 16919] train loss: 0.118734, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001340, l3: 0.011406, l4: 0.020608, l5: 0.034222, l6: 0.063897\n","\n","[epoch: 16920/100000, batch:    12/    1, ite: 16920] train loss: 0.118748, tar: 0.000066 \n","l0: 0.000072, l1: 0.000074, l2: 0.001309, l3: 0.011546, l4: 0.019665, l5: 0.032975, l6: 0.046794\n","\n","[epoch: 16921/100000, batch:    12/    1, ite: 16921] train loss: 0.118741, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001134, l3: 0.011181, l4: 0.020392, l5: 0.033250, l6: 0.049325\n","\n","[epoch: 16922/100000, batch:    12/    1, ite: 16922] train loss: 0.118737, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001275, l3: 0.011038, l4: 0.020073, l5: 0.031573, l6: 0.051075\n","\n","[epoch: 16923/100000, batch:    12/    1, ite: 16923] train loss: 0.118733, tar: 0.000066 \n","l0: 0.000059, l1: 0.000063, l2: 0.001488, l3: 0.011654, l4: 0.019717, l5: 0.033521, l6: 0.048789\n","\n","[epoch: 16924/100000, batch:    12/    1, ite: 16924] train loss: 0.118730, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001107, l3: 0.011433, l4: 0.020425, l5: 0.031978, l6: 0.050425\n","\n","[epoch: 16925/100000, batch:    12/    1, ite: 16925] train loss: 0.118726, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001083, l3: 0.011270, l4: 0.020025, l5: 0.033648, l6: 0.049178\n","\n","[epoch: 16926/100000, batch:    12/    1, ite: 16926] train loss: 0.118722, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001064, l3: 0.010867, l4: 0.019828, l5: 0.031846, l6: 0.043406\n","\n","[epoch: 16927/100000, batch:    12/    1, ite: 16927] train loss: 0.118710, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001444, l3: 0.010973, l4: 0.019926, l5: 0.031946, l6: 0.045073\n","\n","[epoch: 16928/100000, batch:    12/    1, ite: 16928] train loss: 0.118700, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001100, l3: 0.011181, l4: 0.019830, l5: 0.031718, l6: 0.052444\n","\n","[epoch: 16929/100000, batch:    12/    1, ite: 16929] train loss: 0.118697, tar: 0.000066 \n","l0: 0.000055, l1: 0.000059, l2: 0.001250, l3: 0.011117, l4: 0.020066, l5: 0.033216, l6: 0.049263\n","\n","[epoch: 16930/100000, batch:    12/    1, ite: 16930] train loss: 0.118694, tar: 0.000066 \n","l0: 0.000054, l1: 0.000054, l2: 0.001246, l3: 0.011048, l4: 0.019560, l5: 0.030483, l6: 0.049378\n","\n","[epoch: 16931/100000, batch:    12/    1, ite: 16931] train loss: 0.118686, tar: 0.000066 \n","l0: 0.000052, l1: 0.000053, l2: 0.001196, l3: 0.010766, l4: 0.019138, l5: 0.030694, l6: 0.048104\n","\n","[epoch: 16932/100000, batch:    12/    1, ite: 16932] train loss: 0.118677, tar: 0.000066 \n","l0: 0.000067, l1: 0.000068, l2: 0.001450, l3: 0.011483, l4: 0.020071, l5: 0.031246, l6: 0.045264\n","\n","[epoch: 16933/100000, batch:    12/    1, ite: 16933] train loss: 0.118667, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001454, l3: 0.011795, l4: 0.019766, l5: 0.033756, l6: 0.048852\n","\n","[epoch: 16934/100000, batch:    12/    1, ite: 16934] train loss: 0.118664, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001171, l3: 0.011531, l4: 0.020221, l5: 0.032425, l6: 0.053688\n","\n","[epoch: 16935/100000, batch:    12/    1, ite: 16935] train loss: 0.118665, tar: 0.000066 \n","l0: 0.000057, l1: 0.000057, l2: 0.001270, l3: 0.010877, l4: 0.019168, l5: 0.030599, l6: 0.049078\n","\n","[epoch: 16936/100000, batch:    12/    1, ite: 16936] train loss: 0.118656, tar: 0.000066 \n","l0: 0.000060, l1: 0.000062, l2: 0.001477, l3: 0.010955, l4: 0.019235, l5: 0.032083, l6: 0.064168\n","\n","[epoch: 16937/100000, batch:    12/    1, ite: 16937] train loss: 0.118666, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001186, l3: 0.011797, l4: 0.020665, l5: 0.032813, l6: 0.063155\n","\n","[epoch: 16938/100000, batch:    12/    1, ite: 16938] train loss: 0.118678, tar: 0.000066 \n","l0: 0.000063, l1: 0.000064, l2: 0.001165, l3: 0.011291, l4: 0.019874, l5: 0.033192, l6: 0.063515\n","\n","[epoch: 16939/100000, batch:    12/    1, ite: 16939] train loss: 0.118689, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001079, l3: 0.011506, l4: 0.020233, l5: 0.032610, l6: 0.049660\n","\n","[epoch: 16940/100000, batch:    12/    1, ite: 16940] train loss: 0.118686, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001076, l3: 0.011231, l4: 0.019272, l5: 0.032469, l6: 0.063857\n","\n","[epoch: 16941/100000, batch:    12/    1, ite: 16941] train loss: 0.118696, tar: 0.000066 \n","l0: 0.000066, l1: 0.000066, l2: 0.001491, l3: 0.011503, l4: 0.019413, l5: 0.032278, l6: 0.043532\n","\n","[epoch: 16942/100000, batch:    12/    1, ite: 16942] train loss: 0.118685, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001451, l3: 0.011123, l4: 0.020076, l5: 0.030539, l6: 0.054887\n","\n","[epoch: 16943/100000, batch:    12/    1, ite: 16943] train loss: 0.118684, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001483, l3: 0.011116, l4: 0.020056, l5: 0.032317, l6: 0.048525\n","\n","[epoch: 16944/100000, batch:    12/    1, ite: 16944] train loss: 0.118679, tar: 0.000066 \n","l0: 0.000067, l1: 0.000063, l2: 0.001581, l3: 0.011172, l4: 0.020402, l5: 0.034373, l6: 0.066130\n","\n","[epoch: 16945/100000, batch:    12/    1, ite: 16945] train loss: 0.118695, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001143, l3: 0.011544, l4: 0.019941, l5: 0.031273, l6: 0.051450\n","\n","[epoch: 16946/100000, batch:    12/    1, ite: 16946] train loss: 0.118691, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001303, l3: 0.011328, l4: 0.020628, l5: 0.030379, l6: 0.048397\n","\n","[epoch: 16947/100000, batch:    12/    1, ite: 16947] train loss: 0.118685, tar: 0.000066 \n","l0: 0.000058, l1: 0.000058, l2: 0.001048, l3: 0.010988, l4: 0.019306, l5: 0.031467, l6: 0.042538\n","\n","[epoch: 16948/100000, batch:    12/    1, ite: 16948] train loss: 0.118671, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001121, l3: 0.011702, l4: 0.020282, l5: 0.032175, l6: 0.049445\n","\n","[epoch: 16949/100000, batch:    12/    1, ite: 16949] train loss: 0.118667, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001188, l3: 0.011493, l4: 0.019933, l5: 0.031575, l6: 0.061688\n","\n","[epoch: 16950/100000, batch:    12/    1, ite: 16950] train loss: 0.118674, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001596, l3: 0.011240, l4: 0.020105, l5: 0.032895, l6: 0.063987\n","\n","[epoch: 16951/100000, batch:    12/    1, ite: 16951] train loss: 0.118686, tar: 0.000065 \n","l0: 0.000089, l1: 0.000084, l2: 0.001679, l3: 0.010984, l4: 0.020140, l5: 0.034339, l6: 0.066382\n","\n","[epoch: 16952/100000, batch:    12/    1, ite: 16952] train loss: 0.118702, tar: 0.000066 \n","l0: 0.000066, l1: 0.000067, l2: 0.001096, l3: 0.011601, l4: 0.020231, l5: 0.030174, l6: 0.051265\n","\n","[epoch: 16953/100000, batch:    12/    1, ite: 16953] train loss: 0.118698, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001107, l3: 0.011134, l4: 0.019818, l5: 0.033359, l6: 0.048744\n","\n","[epoch: 16954/100000, batch:    12/    1, ite: 16954] train loss: 0.118693, tar: 0.000066 \n","l0: 0.000064, l1: 0.000063, l2: 0.001117, l3: 0.011271, l4: 0.020357, l5: 0.031892, l6: 0.050455\n","\n","[epoch: 16955/100000, batch:    12/    1, ite: 16955] train loss: 0.118689, tar: 0.000066 \n","l0: 0.000071, l1: 0.000071, l2: 0.001328, l3: 0.011052, l4: 0.020731, l5: 0.034053, l6: 0.067202\n","\n","[epoch: 16956/100000, batch:    12/    1, ite: 16956] train loss: 0.118706, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001229, l3: 0.011267, l4: 0.020403, l5: 0.033723, l6: 0.062392\n","\n","[epoch: 16957/100000, batch:    12/    1, ite: 16957] train loss: 0.118717, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001191, l3: 0.011232, l4: 0.020594, l5: 0.031813, l6: 0.050185\n","\n","[epoch: 16958/100000, batch:    12/    1, ite: 16958] train loss: 0.118713, tar: 0.000066 \n","l0: 0.000054, l1: 0.000055, l2: 0.001408, l3: 0.011535, l4: 0.019663, l5: 0.031046, l6: 0.051776\n","\n","[epoch: 16959/100000, batch:    12/    1, ite: 16959] train loss: 0.118710, tar: 0.000066 \n","l0: 0.000064, l1: 0.000065, l2: 0.001524, l3: 0.010988, l4: 0.020000, l5: 0.033426, l6: 0.067243\n","\n","[epoch: 16960/100000, batch:    12/    1, ite: 16960] train loss: 0.118725, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001473, l3: 0.011579, l4: 0.020201, l5: 0.031443, l6: 0.061169\n","\n","[epoch: 16961/100000, batch:    12/    1, ite: 16961] train loss: 0.118732, tar: 0.000066 \n","l0: 0.000057, l1: 0.000058, l2: 0.001404, l3: 0.011300, l4: 0.019899, l5: 0.031743, l6: 0.044232\n","\n","[epoch: 16962/100000, batch:    12/    1, ite: 16962] train loss: 0.118722, tar: 0.000065 \n","l0: 0.000056, l1: 0.000056, l2: 0.001443, l3: 0.011098, l4: 0.020124, l5: 0.033259, l6: 0.065750\n","\n","[epoch: 16963/100000, batch:    12/    1, ite: 16963] train loss: 0.118736, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001206, l3: 0.011871, l4: 0.020635, l5: 0.033476, l6: 0.062124\n","\n","[epoch: 16964/100000, batch:    12/    1, ite: 16964] train loss: 0.118747, tar: 0.000065 \n","l0: 0.000070, l1: 0.000069, l2: 0.001737, l3: 0.011188, l4: 0.019925, l5: 0.030438, l6: 0.047790\n","\n","[epoch: 16965/100000, batch:    12/    1, ite: 16965] train loss: 0.118739, tar: 0.000065 \n","l0: 0.000082, l1: 0.000083, l2: 0.001596, l3: 0.010844, l4: 0.019910, l5: 0.033375, l6: 0.065991\n","\n","[epoch: 16966/100000, batch:    12/    1, ite: 16966] train loss: 0.118752, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001192, l3: 0.011364, l4: 0.019925, l5: 0.031013, l6: 0.048993\n","\n","[epoch: 16967/100000, batch:    12/    1, ite: 16967] train loss: 0.118746, tar: 0.000066 \n","l0: 0.000062, l1: 0.000062, l2: 0.001121, l3: 0.011336, l4: 0.019977, l5: 0.033193, l6: 0.048758\n","\n","[epoch: 16968/100000, batch:    12/    1, ite: 16968] train loss: 0.118742, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001228, l3: 0.011313, l4: 0.019531, l5: 0.029891, l6: 0.049773\n","\n","[epoch: 16969/100000, batch:    12/    1, ite: 16969] train loss: 0.118735, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001531, l3: 0.011622, l4: 0.020520, l5: 0.033335, l6: 0.049775\n","\n","[epoch: 16970/100000, batch:    12/    1, ite: 16970] train loss: 0.118733, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001209, l3: 0.011358, l4: 0.019371, l5: 0.032420, l6: 0.044603\n","\n","[epoch: 16971/100000, batch:    12/    1, ite: 16971] train loss: 0.118723, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001454, l3: 0.011107, l4: 0.020276, l5: 0.029969, l6: 0.053973\n","\n","[epoch: 16972/100000, batch:    12/    1, ite: 16972] train loss: 0.118721, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001353, l3: 0.011260, l4: 0.020060, l5: 0.030160, l6: 0.048052\n","\n","[epoch: 16973/100000, batch:    12/    1, ite: 16973] train loss: 0.118713, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001478, l3: 0.011255, l4: 0.020277, l5: 0.033207, l6: 0.048880\n","\n","[epoch: 16974/100000, batch:    12/    1, ite: 16974] train loss: 0.118709, tar: 0.000065 \n","l0: 0.000071, l1: 0.000071, l2: 0.001416, l3: 0.011147, l4: 0.020030, l5: 0.032326, l6: 0.049290\n","\n","[epoch: 16975/100000, batch:    12/    1, ite: 16975] train loss: 0.118705, tar: 0.000065 \n","l0: 0.000075, l1: 0.000075, l2: 0.001357, l3: 0.011143, l4: 0.020363, l5: 0.030303, l6: 0.059058\n","\n","[epoch: 16976/100000, batch:    12/    1, ite: 16976] train loss: 0.118709, tar: 0.000066 \n","l0: 0.000062, l1: 0.000063, l2: 0.001179, l3: 0.011760, l4: 0.020518, l5: 0.033470, l6: 0.062088\n","\n","[epoch: 16977/100000, batch:    12/    1, ite: 16977] train loss: 0.118719, tar: 0.000066 \n","l0: 0.000063, l1: 0.000063, l2: 0.001267, l3: 0.011802, l4: 0.019776, l5: 0.030205, l6: 0.057688\n","\n","[epoch: 16978/100000, batch:    12/    1, ite: 16978] train loss: 0.118722, tar: 0.000066 \n","l0: 0.000069, l1: 0.000071, l2: 0.001165, l3: 0.011453, l4: 0.019982, l5: 0.033697, l6: 0.049121\n","\n","[epoch: 16979/100000, batch:    12/    1, ite: 16979] train loss: 0.118718, tar: 0.000066 \n","l0: 0.000064, l1: 0.000064, l2: 0.001192, l3: 0.011410, l4: 0.019893, l5: 0.030089, l6: 0.050318\n","\n","[epoch: 16980/100000, batch:    12/    1, ite: 16980] train loss: 0.118713, tar: 0.000066 \n","l0: 0.000067, l1: 0.000067, l2: 0.001152, l3: 0.011042, l4: 0.019657, l5: 0.028958, l6: 0.050883\n","\n","[epoch: 16981/100000, batch:    12/    1, ite: 16981] train loss: 0.118706, tar: 0.000066 \n","l0: 0.000059, l1: 0.000059, l2: 0.001431, l3: 0.011353, l4: 0.020476, l5: 0.031845, l6: 0.055097\n","\n","[epoch: 16982/100000, batch:    12/    1, ite: 16982] train loss: 0.118707, tar: 0.000065 \n","l0: 0.000058, l1: 0.000058, l2: 0.001443, l3: 0.011276, l4: 0.019732, l5: 0.031713, l6: 0.052714\n","\n","[epoch: 16983/100000, batch:    12/    1, ite: 16983] train loss: 0.118705, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001434, l3: 0.011496, l4: 0.019520, l5: 0.032264, l6: 0.044719\n","\n","[epoch: 16984/100000, batch:    12/    1, ite: 16984] train loss: 0.118696, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001582, l3: 0.011349, l4: 0.020329, l5: 0.033284, l6: 0.048286\n","\n","[epoch: 16985/100000, batch:    12/    1, ite: 16985] train loss: 0.118692, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001642, l3: 0.011525, l4: 0.019937, l5: 0.030767, l6: 0.045733\n","\n","[epoch: 16986/100000, batch:    12/    1, ite: 16986] train loss: 0.118683, tar: 0.000065 \n","l0: 0.000065, l1: 0.000065, l2: 0.001246, l3: 0.011185, l4: 0.020458, l5: 0.031137, l6: 0.050359\n","\n","[epoch: 16987/100000, batch:    12/    1, ite: 16987] train loss: 0.118679, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001362, l3: 0.011267, l4: 0.020160, l5: 0.030851, l6: 0.053163\n","\n","[epoch: 16988/100000, batch:    12/    1, ite: 16988] train loss: 0.118677, tar: 0.000065 \n","l0: 0.000061, l1: 0.000061, l2: 0.001661, l3: 0.011346, l4: 0.020220, l5: 0.032914, l6: 0.062304\n","\n","[epoch: 16989/100000, batch:    12/    1, ite: 16989] train loss: 0.118687, tar: 0.000065 \n","l0: 0.000058, l1: 0.000058, l2: 0.001394, l3: 0.011659, l4: 0.020610, l5: 0.031639, l6: 0.057044\n","\n","[epoch: 16990/100000, batch:    12/    1, ite: 16990] train loss: 0.118691, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001249, l3: 0.011143, l4: 0.019816, l5: 0.032706, l6: 0.048073\n","\n","[epoch: 16991/100000, batch:    12/    1, ite: 16991] train loss: 0.118686, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001437, l3: 0.011403, l4: 0.019718, l5: 0.029436, l6: 0.051494\n","\n","[epoch: 16992/100000, batch:    12/    1, ite: 16992] train loss: 0.118680, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001582, l3: 0.011309, l4: 0.020243, l5: 0.030461, l6: 0.052568\n","\n","[epoch: 16993/100000, batch:    12/    1, ite: 16993] train loss: 0.118678, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001323, l3: 0.011077, l4: 0.019719, l5: 0.030823, l6: 0.048107\n","\n","[epoch: 16994/100000, batch:    12/    1, ite: 16994] train loss: 0.118670, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001368, l3: 0.011455, l4: 0.019156, l5: 0.031737, l6: 0.042430\n","\n","[epoch: 16995/100000, batch:    12/    1, ite: 16995] train loss: 0.118658, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001555, l3: 0.011879, l4: 0.020708, l5: 0.032655, l6: 0.057211\n","\n","[epoch: 16996/100000, batch:    12/    1, ite: 16996] train loss: 0.118663, tar: 0.000065 \n","l0: 0.000059, l1: 0.000059, l2: 0.001480, l3: 0.011456, l4: 0.019098, l5: 0.030203, l6: 0.046817\n","\n","[epoch: 16997/100000, batch:    12/    1, ite: 16997] train loss: 0.118654, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001219, l3: 0.011131, l4: 0.019250, l5: 0.031785, l6: 0.043404\n","\n","[epoch: 16998/100000, batch:    12/    1, ite: 16998] train loss: 0.118642, tar: 0.000065 \n","l0: 0.000073, l1: 0.000074, l2: 0.001297, l3: 0.010882, l4: 0.019845, l5: 0.032367, l6: 0.066486\n","\n","[epoch: 16999/100000, batch:    12/    1, ite: 16999] train loss: 0.118655, tar: 0.000065 \n","l0: 0.000058, l1: 0.000059, l2: 0.001194, l3: 0.011489, l4: 0.019317, l5: 0.030556, l6: 0.055477\n","\n","[epoch: 17000/100000, batch:    12/    1, ite: 17000] train loss: 0.118654, tar: 0.000065 \n","l0: 0.000073, l1: 0.000073, l2: 0.001390, l3: 0.010990, l4: 0.019875, l5: 0.029384, l6: 0.051205\n","\n","[epoch: 17001/100000, batch:    12/    1, ite: 17001] train loss: 0.118648, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001156, l3: 0.011263, l4: 0.019822, l5: 0.029979, l6: 0.054504\n","\n","[epoch: 17002/100000, batch:    12/    1, ite: 17002] train loss: 0.118647, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001238, l3: 0.011601, l4: 0.019862, l5: 0.031922, l6: 0.053649\n","\n","[epoch: 17003/100000, batch:    12/    1, ite: 17003] train loss: 0.118646, tar: 0.000065 \n","l0: 0.000065, l1: 0.000065, l2: 0.001243, l3: 0.011382, l4: 0.020135, l5: 0.033600, l6: 0.062475\n","\n","[epoch: 17004/100000, batch:    12/    1, ite: 17004] train loss: 0.118657, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001154, l3: 0.011637, l4: 0.020529, l5: 0.033105, l6: 0.049167\n","\n","[epoch: 17005/100000, batch:    12/    1, ite: 17005] train loss: 0.118654, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001443, l3: 0.011758, l4: 0.020006, l5: 0.033169, l6: 0.065318\n","\n","[epoch: 17006/100000, batch:    12/    1, ite: 17006] train loss: 0.118667, tar: 0.000065 \n","l0: 0.000072, l1: 0.000073, l2: 0.001800, l3: 0.011224, l4: 0.019898, l5: 0.034054, l6: 0.067980\n","\n","[epoch: 17007/100000, batch:    12/    1, ite: 17007] train loss: 0.118683, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001149, l3: 0.011630, l4: 0.019686, l5: 0.029625, l6: 0.052466\n","\n","[epoch: 17008/100000, batch:    12/    1, ite: 17008] train loss: 0.118679, tar: 0.000065 \n","l0: 0.000074, l1: 0.000074, l2: 0.001465, l3: 0.010913, l4: 0.020192, l5: 0.034149, l6: 0.065947\n","\n","[epoch: 17009/100000, batch:    12/    1, ite: 17009] train loss: 0.118693, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001471, l3: 0.011069, l4: 0.019784, l5: 0.032003, l6: 0.046627\n","\n","[epoch: 17010/100000, batch:    12/    1, ite: 17010] train loss: 0.118686, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001150, l3: 0.011662, l4: 0.020220, l5: 0.033838, l6: 0.048162\n","\n","[epoch: 17011/100000, batch:    12/    1, ite: 17011] train loss: 0.118682, tar: 0.000065 \n","l0: 0.000059, l1: 0.000059, l2: 0.001511, l3: 0.011187, l4: 0.019616, l5: 0.032541, l6: 0.044908\n","\n","[epoch: 17012/100000, batch:    12/    1, ite: 17012] train loss: 0.118674, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001238, l3: 0.011714, l4: 0.020584, l5: 0.032657, l6: 0.058820\n","\n","[epoch: 17013/100000, batch:    12/    1, ite: 17013] train loss: 0.118680, tar: 0.000065 \n","l0: 0.000066, l1: 0.000067, l2: 0.001516, l3: 0.011563, l4: 0.019876, l5: 0.032218, l6: 0.052101\n","\n","[epoch: 17014/100000, batch:    12/    1, ite: 17014] train loss: 0.118679, tar: 0.000065 \n","l0: 0.000071, l1: 0.000071, l2: 0.001310, l3: 0.011276, l4: 0.020291, l5: 0.032301, l6: 0.056753\n","\n","[epoch: 17015/100000, batch:    12/    1, ite: 17015] train loss: 0.118682, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001158, l3: 0.011376, l4: 0.020165, l5: 0.031537, l6: 0.045360\n","\n","[epoch: 17016/100000, batch:    12/    1, ite: 17016] train loss: 0.118673, tar: 0.000065 \n","l0: 0.000058, l1: 0.000058, l2: 0.001369, l3: 0.011270, l4: 0.020341, l5: 0.030512, l6: 0.059372\n","\n","[epoch: 17017/100000, batch:    12/    1, ite: 17017] train loss: 0.118677, tar: 0.000065 \n","l0: 0.000065, l1: 0.000066, l2: 0.001283, l3: 0.010802, l4: 0.019368, l5: 0.031161, l6: 0.043038\n","\n","[epoch: 17018/100000, batch:    12/    1, ite: 17018] train loss: 0.118665, tar: 0.000065 \n","l0: 0.000064, l1: 0.000064, l2: 0.001250, l3: 0.011228, l4: 0.019582, l5: 0.031239, l6: 0.052181\n","\n","[epoch: 17019/100000, batch:    12/    1, ite: 17019] train loss: 0.118662, tar: 0.000065 \n","l0: 0.000067, l1: 0.000068, l2: 0.001546, l3: 0.011225, l4: 0.020352, l5: 0.033200, l6: 0.060675\n","\n","[epoch: 17020/100000, batch:    12/    1, ite: 17020] train loss: 0.118670, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001218, l3: 0.011792, l4: 0.020531, l5: 0.032528, l6: 0.058834\n","\n","[epoch: 17021/100000, batch:    12/    1, ite: 17021] train loss: 0.118676, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001174, l3: 0.011008, l4: 0.020021, l5: 0.031578, l6: 0.045396\n","\n","[epoch: 17022/100000, batch:    12/    1, ite: 17022] train loss: 0.118667, tar: 0.000065 \n","l0: 0.000060, l1: 0.000060, l2: 0.001502, l3: 0.011153, l4: 0.019620, l5: 0.032547, l6: 0.066252\n","\n","[epoch: 17023/100000, batch:    12/    1, ite: 17023] train loss: 0.118679, tar: 0.000065 \n","l0: 0.000067, l1: 0.000068, l2: 0.001205, l3: 0.011823, l4: 0.020471, l5: 0.033549, l6: 0.062058\n","\n","[epoch: 17024/100000, batch:    12/    1, ite: 17024] train loss: 0.118690, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001224, l3: 0.011561, l4: 0.019452, l5: 0.032385, l6: 0.044963\n","\n","[epoch: 17025/100000, batch:    12/    1, ite: 17025] train loss: 0.118681, tar: 0.000065 \n","l0: 0.000064, l1: 0.000064, l2: 0.001639, l3: 0.011075, l4: 0.019667, l5: 0.032031, l6: 0.061942\n","\n","[epoch: 17026/100000, batch:    12/    1, ite: 17026] train loss: 0.118688, tar: 0.000065 \n","l0: 0.000048, l1: 0.000052, l2: 0.001282, l3: 0.011343, l4: 0.019890, l5: 0.030390, l6: 0.043991\n","\n","[epoch: 17027/100000, batch:    12/    1, ite: 17027] train loss: 0.118677, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001138, l3: 0.011145, l4: 0.019586, l5: 0.028877, l6: 0.049870\n","\n","[epoch: 17028/100000, batch:    12/    1, ite: 17028] train loss: 0.118669, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001123, l3: 0.011632, l4: 0.019654, l5: 0.033647, l6: 0.048614\n","\n","[epoch: 17029/100000, batch:    12/    1, ite: 17029] train loss: 0.118666, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001296, l3: 0.011081, l4: 0.019935, l5: 0.029471, l6: 0.049432\n","\n","[epoch: 17030/100000, batch:    12/    1, ite: 17030] train loss: 0.118658, tar: 0.000065 \n","l0: 0.000064, l1: 0.000064, l2: 0.001534, l3: 0.011286, l4: 0.020241, l5: 0.034311, l6: 0.061599\n","\n","[epoch: 17031/100000, batch:    12/    1, ite: 17031] train loss: 0.118669, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001151, l3: 0.011245, l4: 0.019640, l5: 0.029536, l6: 0.051367\n","\n","[epoch: 17032/100000, batch:    12/    1, ite: 17032] train loss: 0.118663, tar: 0.000065 \n","l0: 0.000067, l1: 0.000066, l2: 0.001184, l3: 0.011490, l4: 0.020238, l5: 0.032385, l6: 0.053664\n","\n","[epoch: 17033/100000, batch:    12/    1, ite: 17033] train loss: 0.118664, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001101, l3: 0.011554, l4: 0.019663, l5: 0.029661, l6: 0.052459\n","\n","[epoch: 17034/100000, batch:    12/    1, ite: 17034] train loss: 0.118660, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001126, l3: 0.011113, l4: 0.019588, l5: 0.030763, l6: 0.053380\n","\n","[epoch: 17035/100000, batch:    12/    1, ite: 17035] train loss: 0.118657, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001562, l3: 0.011558, l4: 0.020054, l5: 0.033807, l6: 0.065686\n","\n","[epoch: 17036/100000, batch:    12/    1, ite: 17036] train loss: 0.118671, tar: 0.000065 \n","l0: 0.000071, l1: 0.000071, l2: 0.001304, l3: 0.011657, l4: 0.020378, l5: 0.033283, l6: 0.050104\n","\n","[epoch: 17037/100000, batch:    12/    1, ite: 17037] train loss: 0.118669, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001178, l3: 0.011302, l4: 0.020779, l5: 0.032877, l6: 0.048341\n","\n","[epoch: 17038/100000, batch:    12/    1, ite: 17038] train loss: 0.118665, tar: 0.000065 \n","l0: 0.000063, l1: 0.000064, l2: 0.001575, l3: 0.011176, l4: 0.020100, l5: 0.030065, l6: 0.053885\n","\n","[epoch: 17039/100000, batch:    12/    1, ite: 17039] train loss: 0.118664, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001260, l3: 0.011421, l4: 0.020087, l5: 0.030611, l6: 0.054182\n","\n","[epoch: 17040/100000, batch:    12/    1, ite: 17040] train loss: 0.118663, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001152, l3: 0.011360, l4: 0.019766, l5: 0.030399, l6: 0.046454\n","\n","[epoch: 17041/100000, batch:    12/    1, ite: 17041] train loss: 0.118654, tar: 0.000065 \n","l0: 0.000071, l1: 0.000067, l2: 0.001530, l3: 0.011099, l4: 0.020169, l5: 0.031457, l6: 0.051183\n","\n","[epoch: 17042/100000, batch:    12/    1, ite: 17042] train loss: 0.118651, tar: 0.000065 \n","l0: 0.000061, l1: 0.000061, l2: 0.001532, l3: 0.011310, l4: 0.020287, l5: 0.033724, l6: 0.060455\n","\n","[epoch: 17043/100000, batch:    12/    1, ite: 17043] train loss: 0.118659, tar: 0.000065 \n","l0: 0.000060, l1: 0.000059, l2: 0.001474, l3: 0.011690, l4: 0.020123, l5: 0.030981, l6: 0.055805\n","\n","[epoch: 17044/100000, batch:    12/    1, ite: 17044] train loss: 0.118661, tar: 0.000065 \n","l0: 0.000068, l1: 0.000069, l2: 0.001540, l3: 0.011553, l4: 0.020212, l5: 0.031771, l6: 0.059498\n","\n","[epoch: 17045/100000, batch:    12/    1, ite: 17045] train loss: 0.118666, tar: 0.000065 \n","l0: 0.000068, l1: 0.000068, l2: 0.001530, l3: 0.011276, l4: 0.020393, l5: 0.034155, l6: 0.063266\n","\n","[epoch: 17046/100000, batch:    12/    1, ite: 17046] train loss: 0.118678, tar: 0.000065 \n","l0: 0.000066, l1: 0.000067, l2: 0.001346, l3: 0.011199, l4: 0.018870, l5: 0.031560, l6: 0.042557\n","\n","[epoch: 17047/100000, batch:    12/    1, ite: 17047] train loss: 0.118665, tar: 0.000065 \n","l0: 0.000066, l1: 0.000066, l2: 0.001210, l3: 0.011194, l4: 0.019717, l5: 0.030492, l6: 0.045371\n","\n","[epoch: 17048/100000, batch:    12/    1, ite: 17048] train loss: 0.118655, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001719, l3: 0.011660, l4: 0.020421, l5: 0.034711, l6: 0.065551\n","\n","[epoch: 17049/100000, batch:    12/    1, ite: 17049] train loss: 0.118670, tar: 0.000065 \n","l0: 0.000070, l1: 0.000071, l2: 0.001198, l3: 0.011545, l4: 0.020281, l5: 0.034132, l6: 0.065429\n","\n","[epoch: 17050/100000, batch:    12/    1, ite: 17050] train loss: 0.118684, tar: 0.000065 \n","l0: 0.000066, l1: 0.000066, l2: 0.001121, l3: 0.011062, l4: 0.019657, l5: 0.030233, l6: 0.044973\n","\n","[epoch: 17051/100000, batch:    12/    1, ite: 17051] train loss: 0.118673, tar: 0.000065 \n","l0: 0.000060, l1: 0.000061, l2: 0.001072, l3: 0.010975, l4: 0.019977, l5: 0.030997, l6: 0.057524\n","\n","[epoch: 17052/100000, batch:    12/    1, ite: 17052] train loss: 0.118675, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001183, l3: 0.011721, l4: 0.020156, l5: 0.033777, l6: 0.048257\n","\n","[epoch: 17053/100000, batch:    12/    1, ite: 17053] train loss: 0.118671, tar: 0.000065 \n","l0: 0.000064, l1: 0.000064, l2: 0.001137, l3: 0.011476, l4: 0.020514, l5: 0.032317, l6: 0.049685\n","\n","[epoch: 17054/100000, batch:    12/    1, ite: 17054] train loss: 0.118668, tar: 0.000065 \n","l0: 0.000066, l1: 0.000067, l2: 0.001544, l3: 0.011560, l4: 0.019616, l5: 0.029596, l6: 0.051071\n","\n","[epoch: 17055/100000, batch:    12/    1, ite: 17055] train loss: 0.118663, tar: 0.000065 \n","l0: 0.000068, l1: 0.000069, l2: 0.001171, l3: 0.011496, l4: 0.020238, l5: 0.034053, l6: 0.065320\n","\n","[epoch: 17056/100000, batch:    12/    1, ite: 17056] train loss: 0.118676, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001107, l3: 0.011242, l4: 0.020094, l5: 0.033458, l6: 0.048783\n","\n","[epoch: 17057/100000, batch:    12/    1, ite: 17057] train loss: 0.118672, tar: 0.000065 \n","l0: 0.000059, l1: 0.000059, l2: 0.001435, l3: 0.011086, l4: 0.020117, l5: 0.030124, l6: 0.054016\n","\n","[epoch: 17058/100000, batch:    12/    1, ite: 17058] train loss: 0.118671, tar: 0.000065 \n","l0: 0.000053, l1: 0.000054, l2: 0.001289, l3: 0.011418, l4: 0.020335, l5: 0.031367, l6: 0.047427\n","\n","[epoch: 17059/100000, batch:    12/    1, ite: 17059] train loss: 0.118664, tar: 0.000065 \n","l0: 0.000066, l1: 0.000066, l2: 0.001246, l3: 0.011163, l4: 0.019497, l5: 0.032658, l6: 0.064186\n","\n","[epoch: 17060/100000, batch:    12/    1, ite: 17060] train loss: 0.118674, tar: 0.000065 \n","l0: 0.000059, l1: 0.000059, l2: 0.001411, l3: 0.011083, l4: 0.019669, l5: 0.029957, l6: 0.051364\n","\n","[epoch: 17061/100000, batch:    12/    1, ite: 17061] train loss: 0.118669, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001461, l3: 0.011248, l4: 0.020452, l5: 0.033924, l6: 0.061352\n","\n","[epoch: 17062/100000, batch:    12/    1, ite: 17062] train loss: 0.118679, tar: 0.000065 \n","l0: 0.000057, l1: 0.000057, l2: 0.001177, l3: 0.011180, l4: 0.018887, l5: 0.030094, l6: 0.046124\n","\n","[epoch: 17063/100000, batch:    12/    1, ite: 17063] train loss: 0.118668, tar: 0.000065 \n","l0: 0.000067, l1: 0.000068, l2: 0.001444, l3: 0.011710, l4: 0.020738, l5: 0.031632, l6: 0.048534\n","\n","[epoch: 17064/100000, batch:    12/    1, ite: 17064] train loss: 0.118664, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001406, l3: 0.011713, l4: 0.019601, l5: 0.030154, l6: 0.055994\n","\n","[epoch: 17065/100000, batch:    12/    1, ite: 17065] train loss: 0.118664, tar: 0.000065 \n","l0: 0.000067, l1: 0.000071, l2: 0.001367, l3: 0.011615, l4: 0.019928, l5: 0.030358, l6: 0.052092\n","\n","[epoch: 17066/100000, batch:    12/    1, ite: 17066] train loss: 0.118661, tar: 0.000065 \n","l0: 0.000066, l1: 0.000067, l2: 0.001099, l3: 0.011388, l4: 0.019339, l5: 0.030579, l6: 0.055569\n","\n","[epoch: 17067/100000, batch:    12/    1, ite: 17067] train loss: 0.118661, tar: 0.000065 \n","l0: 0.000063, l1: 0.000064, l2: 0.001143, l3: 0.011347, l4: 0.019408, l5: 0.029223, l6: 0.048093\n","\n","[epoch: 17068/100000, batch:    12/    1, ite: 17068] train loss: 0.118652, tar: 0.000065 \n","l0: 0.000057, l1: 0.000057, l2: 0.001444, l3: 0.010841, l4: 0.019167, l5: 0.030780, l6: 0.048247\n","\n","[epoch: 17069/100000, batch:    12/    1, ite: 17069] train loss: 0.118645, tar: 0.000065 \n","l0: 0.000070, l1: 0.000070, l2: 0.001284, l3: 0.011721, l4: 0.020543, l5: 0.032619, l6: 0.049297\n","\n","[epoch: 17070/100000, batch:    12/    1, ite: 17070] train loss: 0.118642, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001097, l3: 0.011425, l4: 0.019589, l5: 0.028985, l6: 0.049793\n","\n","[epoch: 17071/100000, batch:    12/    1, ite: 17071] train loss: 0.118635, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001123, l3: 0.011755, l4: 0.019984, l5: 0.030370, l6: 0.057715\n","\n","[epoch: 17072/100000, batch:    12/    1, ite: 17072] train loss: 0.118637, tar: 0.000065 \n","l0: 0.000071, l1: 0.000071, l2: 0.001378, l3: 0.011137, l4: 0.019735, l5: 0.032125, l6: 0.062515\n","\n","[epoch: 17073/100000, batch:    12/    1, ite: 17073] train loss: 0.118645, tar: 0.000065 \n","l0: 0.000055, l1: 0.000055, l2: 0.001262, l3: 0.011181, l4: 0.019862, l5: 0.031821, l6: 0.045643\n","\n","[epoch: 17074/100000, batch:    12/    1, ite: 17074] train loss: 0.118636, tar: 0.000065 \n","l0: 0.000058, l1: 0.000059, l2: 0.001439, l3: 0.010958, l4: 0.019792, l5: 0.033916, l6: 0.068294\n","\n","[epoch: 17075/100000, batch:    12/    1, ite: 17075] train loss: 0.118651, tar: 0.000065 \n","l0: 0.000069, l1: 0.000068, l2: 0.001155, l3: 0.011600, l4: 0.019713, l5: 0.030496, l6: 0.057354\n","\n","[epoch: 17076/100000, batch:    12/    1, ite: 17076] train loss: 0.118653, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001113, l3: 0.011349, l4: 0.019532, l5: 0.028869, l6: 0.049725\n","\n","[epoch: 17077/100000, batch:    12/    1, ite: 17077] train loss: 0.118646, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001324, l3: 0.011146, l4: 0.020382, l5: 0.030908, l6: 0.053925\n","\n","[epoch: 17078/100000, batch:    12/    1, ite: 17078] train loss: 0.118645, tar: 0.000065 \n","l0: 0.000058, l1: 0.000058, l2: 0.001122, l3: 0.011330, l4: 0.019548, l5: 0.032636, l6: 0.064162\n","\n","[epoch: 17079/100000, batch:    12/    1, ite: 17079] train loss: 0.118654, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001426, l3: 0.011092, l4: 0.020036, l5: 0.033320, l6: 0.049586\n","\n","[epoch: 17080/100000, batch:    12/    1, ite: 17080] train loss: 0.118651, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001417, l3: 0.011694, l4: 0.020142, l5: 0.033194, l6: 0.048396\n","\n","[epoch: 17081/100000, batch:    12/    1, ite: 17081] train loss: 0.118648, tar: 0.000065 \n","l0: 0.000067, l1: 0.000067, l2: 0.001441, l3: 0.010975, l4: 0.019578, l5: 0.032019, l6: 0.043818\n","\n","[epoch: 17082/100000, batch:    12/    1, ite: 17082] train loss: 0.118638, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001101, l3: 0.011524, l4: 0.020091, l5: 0.030447, l6: 0.052512\n","\n","[epoch: 17083/100000, batch:    12/    1, ite: 17083] train loss: 0.118636, tar: 0.000065 \n","l0: 0.000062, l1: 0.000063, l2: 0.001119, l3: 0.011324, l4: 0.020190, l5: 0.030969, l6: 0.054214\n","\n","[epoch: 17084/100000, batch:    12/    1, ite: 17084] train loss: 0.118635, tar: 0.000065 \n","l0: 0.000072, l1: 0.000073, l2: 0.001547, l3: 0.011062, l4: 0.020548, l5: 0.034277, l6: 0.066143\n","\n","[epoch: 17085/100000, batch:    12/    1, ite: 17085] train loss: 0.118649, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001335, l3: 0.011506, l4: 0.019541, l5: 0.029276, l6: 0.050867\n","\n","[epoch: 17086/100000, batch:    12/    1, ite: 17086] train loss: 0.118643, tar: 0.000065 \n","l0: 0.000063, l1: 0.000064, l2: 0.001414, l3: 0.011444, l4: 0.020450, l5: 0.031890, l6: 0.049935\n","\n","[epoch: 17087/100000, batch:    12/    1, ite: 17087] train loss: 0.118640, tar: 0.000065 \n","l0: 0.000063, l1: 0.000064, l2: 0.001405, l3: 0.011091, l4: 0.020589, l5: 0.030643, l6: 0.050772\n","\n","[epoch: 17088/100000, batch:    12/    1, ite: 17088] train loss: 0.118637, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001121, l3: 0.011613, l4: 0.020110, l5: 0.033798, l6: 0.048224\n","\n","[epoch: 17089/100000, batch:    12/    1, ite: 17089] train loss: 0.118633, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001242, l3: 0.011607, l4: 0.019795, l5: 0.031394, l6: 0.052026\n","\n","[epoch: 17090/100000, batch:    12/    1, ite: 17090] train loss: 0.118631, tar: 0.000065 \n","l0: 0.000066, l1: 0.000066, l2: 0.001115, l3: 0.011265, l4: 0.019505, l5: 0.030990, l6: 0.048991\n","\n","[epoch: 17091/100000, batch:    12/    1, ite: 17091] train loss: 0.118625, tar: 0.000065 \n","l0: 0.000055, l1: 0.000059, l2: 0.001302, l3: 0.011200, l4: 0.020216, l5: 0.034235, l6: 0.063129\n","\n","[epoch: 17092/100000, batch:    12/    1, ite: 17092] train loss: 0.118635, tar: 0.000065 \n","l0: 0.000066, l1: 0.000066, l2: 0.001112, l3: 0.010978, l4: 0.020068, l5: 0.030905, l6: 0.056211\n","\n","[epoch: 17093/100000, batch:    12/    1, ite: 17093] train loss: 0.118636, tar: 0.000065 \n","l0: 0.000061, l1: 0.000062, l2: 0.001488, l3: 0.011250, l4: 0.019850, l5: 0.030214, l6: 0.043407\n","\n","[epoch: 17094/100000, batch:    12/    1, ite: 17094] train loss: 0.118625, tar: 0.000065 \n","l0: 0.000070, l1: 0.000070, l2: 0.001307, l3: 0.011208, l4: 0.019863, l5: 0.030670, l6: 0.055325\n","\n","[epoch: 17095/100000, batch:    12/    1, ite: 17095] train loss: 0.118625, tar: 0.000065 \n","l0: 0.000067, l1: 0.000066, l2: 0.001128, l3: 0.011139, l4: 0.019930, l5: 0.030789, l6: 0.051994\n","\n","[epoch: 17096/100000, batch:    12/    1, ite: 17096] train loss: 0.118622, tar: 0.000065 \n","l0: 0.000054, l1: 0.000054, l2: 0.001242, l3: 0.011469, l4: 0.019660, l5: 0.033385, l6: 0.048651\n","\n","[epoch: 17097/100000, batch:    12/    1, ite: 17097] train loss: 0.118618, tar: 0.000065 \n","l0: 0.000053, l1: 0.000053, l2: 0.001245, l3: 0.011429, l4: 0.019221, l5: 0.030718, l6: 0.055090\n","\n","[epoch: 17098/100000, batch:    12/    1, ite: 17098] train loss: 0.118617, tar: 0.000065 \n","l0: 0.000072, l1: 0.000072, l2: 0.001607, l3: 0.011017, l4: 0.020156, l5: 0.034328, l6: 0.066171\n","\n","[epoch: 17099/100000, batch:    12/    1, ite: 17099] train loss: 0.118631, tar: 0.000065 \n","l0: 0.000061, l1: 0.000061, l2: 0.001173, l3: 0.010975, l4: 0.019301, l5: 0.032264, l6: 0.062418\n","\n","[epoch: 17100/100000, batch:    12/    1, ite: 17100] train loss: 0.118637, tar: 0.000065 \n","l0: 0.000071, l1: 0.000071, l2: 0.001436, l3: 0.011819, l4: 0.020482, l5: 0.034908, l6: 0.065537\n","\n","[epoch: 17101/100000, batch:    12/    1, ite: 17101] train loss: 0.118652, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001146, l3: 0.011291, l4: 0.020182, l5: 0.032134, l6: 0.046534\n","\n","[epoch: 17102/100000, batch:    12/    1, ite: 17102] train loss: 0.118645, tar: 0.000065 \n","l0: 0.000065, l1: 0.000065, l2: 0.001445, l3: 0.010904, l4: 0.019330, l5: 0.031409, l6: 0.049459\n","\n","[epoch: 17103/100000, batch:    12/    1, ite: 17103] train loss: 0.118640, tar: 0.000065 \n","l0: 0.000063, l1: 0.000063, l2: 0.001090, l3: 0.011228, l4: 0.019603, l5: 0.029525, l6: 0.051345\n","\n","[epoch: 17104/100000, batch:    12/    1, ite: 17104] train loss: 0.118635, tar: 0.000065 \n","l0: 0.000062, l1: 0.000062, l2: 0.001102, l3: 0.011720, l4: 0.019681, l5: 0.032338, l6: 0.054711\n","\n","[epoch: 17105/100000, batch:    12/    1, ite: 17105] train loss: 0.118636, tar: 0.000065 \n","l0: 0.000053, l1: 0.000053, l2: 0.001276, l3: 0.011772, l4: 0.020299, l5: 0.031795, l6: 0.061437\n","\n","[epoch: 17106/100000, batch:    12/    1, ite: 17106] train loss: 0.118643, tar: 0.000065 \n","l0: 0.000070, l1: 0.000070, l2: 0.001318, l3: 0.011617, l4: 0.019914, l5: 0.033336, l6: 0.048558\n","\n","[epoch: 17107/100000, batch:    12/    1, ite: 17107] train loss: 0.118639, tar: 0.000065 \n"]}]},{"cell_type":"markdown","source":["now to apply rembg, try another code (run_rembg_with_ft_u2net.ipynb)"],"metadata":{"id":"NhnbIdikkLAQ"}}]}